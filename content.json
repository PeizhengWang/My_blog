{"pages":[{"title":"about","text":"待完工..","link":"/about/index.html"},{"title":"分类","text":"","link":"/categories/index.html"},{"title":"投资随记","text":"本文以日记的形式，记录我人在股市、学习投资的一些感想。 人在股市，慢就是快。 2021/03/22 安全边际 经过市场最近一个月的深度回调，让我深刻认识到了买入时安全边际的重要性。诚然，有着机构和媒体的推波助澜，找到一支股价已经在高位的优质股非常容易。但显然此时的买入是不具有安全边际的，虽然追高买入也还可能会有持续不断收益(比如2020年的白酒、新能源)，但谁也无法准确预测到它能涨到多高，但一旦遭遇市场风格的变化，回调往往是突然地、剧烈的、没有反弹的。那么如果你的成本没有安全边际的话，你大概率是从浮盈迅速变为浮亏的，此时割又不舍得割，不割又一路下跌，价格见底却又因为都被套住而没钱补仓的尴尬局面。 因此，投资思路应该是管住手，如果没在行情刚刚启动的时候进入，那么不管涨的多疯，都不要去博傻，等优质股被错杀到一个合理的位置时，再逐步建仓，分批买入。 每次买入都要有充足的理由 证券分析员或许有着极高的说服能力，看了他的分析，似乎就觉得某家公司大有可为、未来可期。然而，如果让你经过思考后逐条列出在当前价位买入某某公司的原因时，或许你又会感觉写出的原因不是那么可靠。 今日优质股记录 贵州茅台：股王，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率16.47%。价格1989.99元，动态PE为55.42，PE百分位92.44%，极度高估。 海天味业：调味品龙头，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率27.21%。价格151.66元，动态PE为80.63，PE百分位90.78%，极度高估。 当前持股 长电科技：半导体封装测试龙头，受益于半导体国产替代大趋势，叠加芯片需求大增，产能满载，业绩预增。成本40.20，当前价格34.08，浮亏15.23%。市销率为2.09，百分位66.84%，估值偏高。 2021/03/25 分散投资 不要把鸡蛋放在一个篮子里，也最好不要把篮子放在一个桌子上，不然一波股灾桌子塌了。分散投资有很多好处，其一是在基本不影响收益的情况下减小收益曲线的波动，其二是增加投资组合的抗风险能力，避免由于踩雷财务造假导致整个账户崩盘。 耐心 在市场热度高时，好公司的估值都很高。很难找到一个可以接受的安全边际买入建仓，此时需要的就是耐心，从极度高估等到高估，从高估等到合理估值，可能能等到低估，也可能等不到低估。由于我们无法预测股票的短期走势，在公司基本面不变的情况下，我们可以选择分批进场的方式，在估值跌至50分位以下时考虑开始建仓，跌至40分位加仓，跌至30分位再加仓，跌至20分位时重仓，删app。 2021/4/27简单复盘一下近两天的窒息操作。耐不住长线价投的我，自以为发现了很好的短线机会，进场被吊锤。 一周前传出华为自动驾驶的路试视频，北汽蓝谷应声3连板，随后就是三天的回调，以为自动驾驶会是一波像医美和碳中和的大行情，在26日水下低吸北汽蓝谷，当日最高涨幅7个点，但收盘只是微红。今天低开后就不断下杀，我在亏8个点时破位清仓。随后同板块的常熟气饰走出一波反包行情，封板前的一瞬间成功上车。但无奈很快被砸开，当日共亏13个点。 不得不说，我这两天的这种短线行为简直就是赌博一般的行为，虽然刺激，但无疑是在浪费时间和钱财，还好大部分仓位都在长电，短线只是小仓位玩一玩，不然就欲哭无泪了。 短线中的Alpha和Beta y= Alpha+x*Betax: 代表市场收益Beta: 反映的是弹性，是投资组合对市场的敏感度Alpha: 由投资者自身水平导致的超额收益 这是我经常思考的一个问题，短线操作到底存不存在Alpha。目前我的思考是存在，但发现Alpha的人会将其保密。若不存在，股市上不可能出现8年10万倍的传说。而将其公开出去又会导致其因其广为人知的属性而变成Beta。","link":"/draft/Investment%20notes.html"},{"title":"(转)果壳中的经典统计","text":"钟昊均 摘要&emsp;&emsp;一直想写一个有关经典统计的note，结合凝聚态场论学习中的一些经验，趁此机会写点我自己对经典统计的理解。 统计方法的引入&emsp;&emsp;在经典力学的框架下，我们可以通过构造系统的拉格朗日量或哈密顿量来求出描述系统演化的拉格朗日方程或者正则方程，进而求解系统的各项性质。这种从最多的自由度出发精确求解系统相轨道的方法在数学上是困难的，从微分方程理论的角度来说，系统自由度的增加会极大地增加求解方程的困难，对于某些相互作用还会导致系统不可积，而且一旦方程是非线性的，这个系统就很有可能是极端初值依赖的，这使得对系统进行长时间的预测变为不可能，这些性质决定了运动方程在刻画多体系统上的不完备性。 &emsp;&emsp;但是，对于多体系统，我们真的有必要精确求解这个系统吗？统计方法告诉我们，如果我们将尺度放到整个多体系统或者某个整体的多体子系统的层次并且给定一个简单的假设，我们就能从巨大的自由度中得到很多非平凡的结论，这就是经典统计方法带来的优势。微正则系综处理的是不满足KAM定理的保守不可积系统，对于不可积系统是不存在力学解析解的，而不满足KAM定理的要求会带来相轨道的遍历性（满足KAM定理的系统即使存在微扰也会保KAM环面的拓扑，哈密顿相流是必然不遍历的，因此从力学出发是不可能导出统计方法的），对于能量约束下形成的相空间上的球面来说，哈密顿相流的遍历性启发我们用系综平均来取代瞬时的体系参量以描述统计系统，同时如果给定一个更强的等概率原理（其实可以理解成相空间中态上的随机行走，这一点是比遍历性要求更高的），我们就能够给出微正则系综的数学描述，但是接下来就牵涉到系综平均和时间平均是否等价的问题。 &emsp;&emsp;KAM定理的要求是在哈密顿量的可积部分上加上一项微扰项，微扰失效的情况下才会给系统引入遍历性，因此遍历性是统计方法的基本假设，在遍历性假设下时间平均才等于系综平均（哈密顿相流在能量球面上的随机行走轨迹能完全填充球面），但是实际上我们处理的很多系统的相互作用能不能打破KAM环面的拓扑是存疑的，在很多非线性系统中存在的吸引子也能够引发遍历性破缺。在我们的统计物理中，环境的噪声就被当成遍历性假设成立的解释之一，毕竟对于初值敏感的系统，噪声带来的微扰是影响巨大的。当然对于经典系统，能量很自然地假设是连续变化的，这个也是经典统计的一大基本假设。 经典统计的局限性&emsp;&emsp;经典统计的局限性在很多微观体系中其实已经浮现，比如在固体比热和多原子分子气体乃至黑体辐射的分析中就已经一窥踪影。其原因无外乎三点：1、能级的不连续性被极小的宏观平均能级差隐藏；2、自旋乃至更高的自由度没有被考虑；3、能量表象下简并度被忽略。 &emsp;&emsp;在我看来，第三点是经典统计最容易引入的修正，因为这个简并度在从微正则系综到正则系综的映射中就已经隐含了。微正则系综作为孤立系，在相空间中的自由度是最大的，需要考虑孤立系中全部子系统的轨迹，但是一旦我们考虑一个温度恒定的热源，通过积分积去环境带来的自由度，这样就完成了从微正则系综到正则系综的跃变。在这个过程中，很明显，被隐藏的自由度会带来更高的对称性，以至于在任意表象下都会引入非平庸的分布函数，但是目前我们刻意地只保留了指数的部分而忽略了具体表象下存在的其余对称性，比如k空间中谐振子波模就是存在简并度的，这一点在黑体辐射的经典分析中已经得到体现。 &emsp;&emsp;对于凝聚态体系，自旋都是极其重要的，自旋自由度就是磁性系统的基础，但是对于自旋，我们也可以在完全应用量子场论之前做一点经典的统计，比如著名的伊辛模型，这个经典的自旋体系（区别于海森堡模型的自旋）可以做平均场，当然对于一维的伊辛模型的平均场得出的有限温相变点是很离谱的错误，但这不妨碍做高维的伊辛模型平均场（精确很多），这个模型往里挖还能引入很多新的思想，比如重整化，这里就不继续讨论了。 &emsp;&emsp;能级不连续作为两朵大乌云中的一朵，在黑体辐射的研究被揭露了出来，无相互作用玻色子系统的能隙是产生玻色-爱因斯坦凝聚的关键，实验上在多原子分子的平均能量中我们也能看到阶梯状的随温度变化的情形，经典统计是得不出这样奇妙的结果的，毕竟经典系统是无能隙的。 结束语&emsp;&emsp;经典统计是研究多体系统的重要工具，也是进入量子统计的基础，在经典统计中很多方法在量子统计中依然有用武之地，比如平均场和团簇展开都是非常常见的技术，其思想更是在物理学中占有极重的地位，因此经典统计对于我们的同学实属应该学好、必须学好的一种理论，这对今后的学习与研究都是有大益处的。","link":"/draft/zhj-2.html"},{"title":"(转)以最小作用量原理为第一性原理的经典演绎","text":"钟昊均 摘要&emsp;&emsp;朗道力学行文之简洁有力的基础，我认为是抓住了经典力学的本质——最小作用量原理，并以其为第一性原理，通过变分法的数学演绎来建立整个经典力学体系。 拉格朗日分析&emsp;&emsp;很多人认为朗道的书很难，但我认为难的其实并不是朗道书里面花哨的积分或者各种推导技巧，难的是朗道在书中体现的对理论体系的物理本质的思考。比如在拉格朗日函数等于T-V的推导时，其实是合理地从积分泛函的物理本质结合数学结构的基础上合理猜出了动能在笛卡尔坐标中的一般形式，顺便还得出了笛卡尔系下质量的定义——拉格朗日函数对伽利略变换时全导数项的乘数因子。在建立三大守恒律与三大对称性的桥梁时更是体现了朗道对对称性的深刻理解，通过不变量的构造展现了诺特定理的内涵，不过没有在书中完整地展现和演绎诺特定理是有点遗憾的。 &emsp;&emsp;这样的演绎方法最大的好处就是理论的结构和体系非常完整和优美，在具体力学过程的演绎上也是从拉格朗日函数上出发利用拉氏方程来解决具体问题。我以为正是广义坐标的引入体现了力学体系的一般性，这使得拉格朗日力学具有普适性，通过在逻辑上先不考虑约束的存在，而后再通过约束减少方程个数，这也使得拉格朗日力学相比利用几何约束来构造牛顿方程的牛顿力学更加自然，这些共同形成了拉格朗日力学对牛顿力学的先进性——本质上完成了力学空间从欧氏空间到位形空间的拓展。 哈密顿分析&emsp;&emsp;但是这对于经典力学的研究并没有画上一个句号——力学的几何特征并没有在拉格朗日力学中得到最大的显现。对于定义在位形空间的拉氏力学而言，一个可积系统在宏观上很有可能是杂乱无章的，比如对于统计物理中的各种模型，使用拉氏量来描述体系是不够能体现体系的某些共同特征的；同时在处理坐标和速度的地位上也是有一点缺憾的——位置和速度可以分别同时给定，这就隐含了位置与速度在描述运动的地位上是不是可以相等的问题。这时，利用勒让德变换就能够将拉格朗日力学过渡到更加先进的哈密顿力学上来。 &emsp;&emsp;勒让德变换在变换拉格朗日函数的时候很像分部积分，通过对微分拉氏量的数学变换，可以构造出一个以广义动量和广义坐标为变量的描述力学的量——哈密顿量。哈密顿力学的威力就在于更高的对称性，通过参数空间的拓展，由哈密顿量导出的正则方程拥有了拉氏方程不具有的高对称性，同时方程阶数由二阶降到一阶、方程个数则翻了倍，我们由此可以从正则方程中得到更多对系统的描述，在数值计算时甚至可以提高效率和可靠性。这种对称性使得定义在“相空间”的哈密顿力学有能力开始展现更多的力学系统的几何特征，力学系统在相空间中可以自然形成流形，同时刘维尔定理告诉我们相空间体积元对正则变换不变，这给了我们研究力学系统在相空间中流形上的性质时最好的工具，正如朗道在“正则变换”一节中说的那样：“对这种可能变换类型的扩大是力学的哈密顿方法的重要优点之一”。 &emsp;&emsp;当然，哈密顿力学中的重要概念——泊松括号，这种算符的存在使得运动积分的构造变得程序化，毕竟两个运动积分的泊松括号也是运动积分，这对于力学的不变量理论是极其重要的。但是泊松括号的意义不仅仅在于此，其在量子力学中的映射——对易子算符对量子力学具有重要意义。 结束语&emsp;&emsp;从朗道力学的目录来看，全书非常精炼，为了完成力学体系的构造，没有一节是冗余的，信息量非常大，在研读哈密顿力学之时有很多推导的细节都非常强调亲手推导，这种高屋建瓴的视角正是理论工作者所需要的。","link":"/draft/zhj-1.html"},{"title":"诗集","text":"还在等待诗人的到来…","link":"/poems/index.html"},{"title":"标签","text":"","link":"/tags/index.html"}],"posts":[{"title":"数据库系统系统考前复习","text":"绪论数据库系统概述 数据库的四个基本概念：数据、数据库、数据库管理系统、数据库系统 数据管理的三个阶段：人工管理阶段、文件系统阶段、数据库系统阶段 数据模型 数据模型是对现实世界数据特征的抽象 两类数据模型：概念模型(按照用户的观点对数据和信息建模)、逻辑模型(网状、层次、关系模型等。按照计算机系统的观点对数据建模，用于DBMS实现)和物理模型(描述数据在磁盘上的表示方法和存取方法) 信息世界的基本概念：实体、属性、码、实体型、实体集、联系。 概念模型可用E-R图来描述现实世界。 数据模型由三部分组成：数据结构(描述系统的静态特性)、数据操作(描述系统的动态特性)、完整性约束。 数据操作：对数据库中各种元素的实例允许执行的操作的集合 数据操作的类型：查询、更新(插入、删除和修改) 常用的数据模型：层次模型、网状模型、关系模型、面向对象数据模型、对象关系数据模型、半结构化数据模型、非结构化数据模型、图模型 层次模型用树形结构来表示各类实体以及实体间的联系(有且只有一个结点没有双亲结点，这个结点称为根结点，其他结点有且仅有一个双亲结点) 网状模型用网状结构来表示各类实体以及实体间的联系(允许一个以上的结点无双亲，一个结点可以有多于一个的双亲) 关系数据库系统采用关系模型作为数据组织的方式 数据库系统的结构和组成 数据库系统的三级模式结构：外模式、模式、内模式 模式(也称逻辑模式)：数据库中全体数据的逻辑结构和特征的描述，是所有用户的公共数据视图。 外模式(也称子模式或用户模式)：是模式的子集，一个模式可以有多个外模式，反映了不同用户的应用需求。一个外模式可以被多个应用使用，一个应用只能使用一个外模式。 内模式(也称储存模式)：是数据物理结构和储存方式的描述，一个数据库只能有一个内模式。 数据库、数据库管理系统、应用程序、数据库管理员 关系数据库关系模型 关系模式是型，关系是值 关系模式可以定义为$R(U, D, DOM, F)$，$R$为关系名，$U$为组成该关系的属性名集合，$D$为U中属性所来自的域，$DOM$为属性向域的映像集合，$F$为属性间数据的依赖关系的集合。 在一个给定的应用领域中，所有关系的集合构成一个关系数据库 关系的三类完整性约束：实体完整性、参照完整性和用户定义的完整性 实体完整性：关系的主属性不能取空值 参照完整性：外码要不取空值，要不取相对应参照关系中存在的主码值。 关系代数与演算 关系数据库语言的分类：关系代数语言、关系演算语言、具有关系代数和关系演算双重特点的语言(SQL) 关系代数是一种抽象的查询语言，他用对关系的运算来表达查询 关系运算：选择、投影、连接、除运算 以数理逻辑中的谓词演算为基础，按照谓词变元不同分为元组关系演算和域关系演算 关系数据库标准语言SQL数据定义 模式定义(CREATE SCHEMA、DROP SCHEMA) 表定义(CREATE TABLE、DROP TABLE、ALTER TABLE) 索引定义(CREATE INDEX、ALTER INDEX、DROP ) 视图定义 数据查询单表查询 语法：SELECT [DISTINCT] &lt;列名&gt;FROM &lt;表名1&gt; [WHERE &lt;表名2&gt;][GROUP BY &lt;表达式&gt; [HAVING &lt;表达式&gt;]][ORDER BY &lt;&gt; [ASC|DESC]] BETWEEN …. AND …. IN … ， NOT IN … LIKE ‘&lt;匹配字符串&gt;’(通配符) IS NULL，IS NOT NULL 聚集函数：COUNT()、SUM()、AVG()、MAX()、MIN() 连接查询 等值连接：”=”SELECT Student.*, SC.*FROM Student, SCWHERE Student.Sno = SC.Sno 自身连接：SELECT First.Cname, Second.CnameFROM Course First, Course SecondWhere First.Cpno = Second.Cpno 外连接：SELECT Student.SnoFROM Student LEFT OUT JOIN SC ON(Student.Sno = SC.Sno) 多表连接：SELECT Student.SnoFROM Student, SC, CourseWHERE Student.Sno = SC.SnoAND SC.Cno=Course.Cno 嵌套查询 一个SELECT-FROM-WHERE语句被称为一个查询块，将一个查询块嵌套在另一个查询块的WHERE子句或HAVING短语的条件中的查询称为嵌套查询。SELECT SnameFROM StudentWHERE Sno IN( SELECT SNO FROM SC WHERE Cno=’2’) 上层的查询块称为外层查询或父查询 下层的查询块称为内层查询或子查询 子查询的限制：不能使用GROUP BY 使用ANY或者ALL谓词时必须同时使用比较运算 EXIST谓词不返回任何数据，只返回TRUE或者False 集合查询 集合操作的种类：UNION、INTERSECT、EXCEPT 参加集合操作的各查询结果的列数必须相同，对应项的数据类型也必须相同 UNION：将多个查询结果合并起来时，自动去掉重复元组 UNION ALL：将多个查询结果合并起来时，保留重复元组 数据更新数据插入 两种插入数据方式：插入元组、插入子查询结果 插入元组语句格式：INSERTINTO &lt;表名&gt; [(&lt;属性列1&gt;[,&lt;属性列2&gt;])]VALUES (&lt;常量1&gt;[,&lt;常量2&gt;]) 插入子查询语句格式：INSERTINTO &lt;表名&gt; [(&lt;属性列1&gt;[,&lt;属性列2&gt;])]子查询 修改数据 语句格式UPDATE &lt;表名&gt;SET &lt;列名&gt;=&lt;表达式&gt;[,&lt;列名&gt;=&lt;表达式&gt;…][WHERE &lt;条件&gt;] 关系数据库管理系统在执行修改语句时会检查修改操作是否破坏表上已定义的完整性规则 删除数据 语句格式DELETE FROM &lt;表名&gt;[WHERE &lt;条件&gt;] 空值处理 用IS NULL或者IS NOT NULL来判断空值 有NOT NULL约束的、加了UNIQUE限制的、码属性不能取空值 空值与另一个值得运算结果为空值 空值与另一个值得比较结果为UNKONW 视图 视图是从一个或者几个基本表中导出的虚表，只存放定义，不存放数据，基表中的数据发生变化视图中的数据也随之发生变化 建立视图：CREATE VIEW &lt;视图名&gt;AS &lt;子查询&gt;[WITH CHECK OPTION] 删除视图：DROP VIEW &lt;视图名&gt; [CASCADE] 数据库安全性数据库安全性控制 数据库安全性控制的常用方法：用户身份鉴别(用户名密码)、存取控制(权限，GRANT、REVOKE)、视图(把保密数据对无权存取这些数据的用户隐藏)、审计、数据加密(储存加密、传输加密) 数据库完整性实体完整性 通过PRIMARY KEY定义 为加快索引速度，对主码进行B+树索引 参照完整性 外码只能是空值或者对应主码中某个已经存在的值 参照完整性违约处理：拒绝(默认策略)、级联操作、设置为空值 用户定义完整性 针对某一应用的数据必须满足的语义要求 两种约束：属性上的约束条件(NOT NULL、UNIQUE、CHECK)、元组上的约束条件(CHECK) 完整性约束命名子句 CONSTRANT &lt;完整性约束名&gt; &lt;完整性约束条件&gt; 完整性约束条件包括NOT NULL、UNIQUE、PRIMARY KEY、FOREIGN KEY、CHECK等 断言 CREATE ASSERTION：断言不为真的操作拒绝执行 触发器 CREATE TRIGGER 触发器有触发时间触发，并由数据库服务器自动执行 一个数据表上可能定义了多个触发器，遵循下面顺序执行：执行该表上的BEFORE触发器、激活触发器的SQL语句、执行该表上的AFTER触发器。","link":"/2021/10/16/DBMS/"},{"title":"计算机网络考前复习","text":"计算机网络概述 网络协议(Network protocol)简称协议。协议规定了通信实体之间所交换的消息的格式、意义、顺序以及针对收到信息或发生的事件所采取的行动。 协议的三要素：语法、语义、时序 计算机网络结构：网络边缘、接入网络/物理介质、网络核心(核心网络) 网络核心的关键功能：路由(确定分组从源到目的的传输路径)+转发(将分组从路由器的输入端口交换至正确的输出端口) 端系统通过接入ISP连接到Internet 多路复用：频分多路复用(FDM)、时分多路复用(TDM)、波分多路复用(WDM)、码分多路复用(CDM，相同频率载波、不同的码片序列) 数据交换的类型：电路交换(独占资源)、报文交换、分组交换 电路交换：建立连接(呼叫/电路建立)、通信、释放连接(拆除电路) 报文交换：发送信息整体 分组交换：报文拆出一系列相对较小的数据包，产生额外开销 速率/数据率/数据传输速率/比特率 单位：b/s(或bps) 通常指额定速率或标称速率 带宽：数字信道所能传达的最高数据率 单位：b/s(或bps) 四种分组延迟：节点处理延迟(差错检测、确定输出链路)、排队延迟(等待输出链路可用)、传输延迟(分组大小/传输速率)、传播延迟(物理链路长度/信号传播速度) 时延带宽积：传播时延*带宽($d_{prop}\\times R (bits)$) OSI参考模型(从上到下：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层) 物理层：接口特性(机械、电气、功能、规程)、比特编码、数据率、比特同步、传输模式(单工、半双工、全双工) 数据链路层：组帧、物理寻址、流量控制、差错控制、访问控制 网络层：逻辑寻址(IP地址)、路由、分组转发 传输层：端到端进程间的完整报文传输 会话层：对话控制(建立、维护)、同步 表示层：数据表示转化、加密/解密、压缩/解压缩 应用层：支持用户通过用户代理(浏览器)或网络接口使用网络(服务，FTP/SMTP/HTTP) TCP/IP参考模型从上到下：应用层(HTTP、SMTP、DNS、RTP)、运输层(TCP、UDP)、网际层(IP)、网络接口层 五层参考模型：综合OSI和TCP/IP的优点(从上到下：应用层、传输层、网络层、链路层、物理层) 网络应用网络应用基本原理 网络应用的体系结构：客户机/服务器结构(C/S)、点对点结构(P2P)、混合结构(Napster) 进程间通信：套接字Socket 寻址方式：IP地址+端口号 Internet提供的传输服务：TCP服务(面向连接、可靠传输、流量控制、拥塞控制)、UDP服务(无连接、不可靠的数据传输) Web应用 HTTP连接的两种类型：非持久性连接(HTTP 1.0版本，每个TCP连接最多传输一个对象)、持久性连接(HTTP 1.1版本，每个TCP连接允许传输多个对象) HTTP协议有两类消息：请求消息(request)、响应消息(response) 上传输入的方法：POST、GET HTTP/1.0：GET、POST、HEAD HTTP/1.1：GET、POST、HEAD、DELETE HTTP响应状态代码：200 OK、301 Moved Permanently、400 Bad request、404 Not Found、505 HTTP Version Not Surported Cookie的组件：HTTP响应/请求消息的头部行、保存在客户端主机上的Cookie文件、Web服务器端的后台数据库 Web缓存/代理服务器技术 Email应用 构成组件：邮件客户端、邮件服务器、SMTP协议 SMTP协议通过TCP进行email信息的可靠传输 传输过程的三个阶段：握手、信息的传输、关闭 多媒体扩展MIME：通过邮件头部增加额外的行以声明内容类型 邮件访问协议：POP3、IMAP、HTTP DNS应用 DNS服务：域名指向IP地址的翻译 递归查询 P2P应用 索引：IP地址+端口号 集中式索引：节点加入时通知中央服务器地址和内容。问题：单点失效问题、性能瓶颈、版权问题 泛洪查询：完全分布式的架构 层次式覆盖网络：超级节点+子节点 Socket编程 应用编程接口API：位于应用层和传输层中间。 WSAStartup：使用socket的应用程序在使用socket前必须首先调用WSAStartup。 WSACleanup：完成对socket的应用后调用，释放资源。 socket(protofamily,type,proto)：创建套接字。参数一：协议族。参数二:套接字类型。参数三：协议号。 closesocket(SOCKET sd)：关闭套接字。 bind(sd,localaddr,adrrlen)：绑定套接字的本地端点地址(IP地址+端口号)。 listen(sd,queuesize):套接字置为监听状态(仅服务器调用)。 connect(sd,saddr,saddrlen)：客户程序使客户套接字sd与服务器特定端口的套接字sadrr进行连接。 accept(sd,caddr,caddrlen)：服务器从处于监听状态的套接字sd请求队列中取出一个客户请求。(仅服务器调用) send/sendto：TCP或调用了connect函数的UDP/UDP与没调用connect的UDP。 recv/recvfrom：TCP或调用了connect函数的UDP/UDP与没调用connect的UDP。 getsockopt/setsockopt：设置/获取套接字参数。 TCP客户端软件流程：确定服务器IP地址与端口号、创建套接字、分配本地端点地址(IP地址+端口号)、连接服务器(套接字)、遵循应用层协议进行通信、关闭/释放连接。 UDP客户端软件流程：确定服务器IP地址与端口号、创建套接字、分配本地端点地址(IP地址+端口号)、指定服务器端点地址，构造UDP数据报、遵循应用层协议进行通信、关闭/释放连接。 4种类型基本服务器：循环无连接服务器、循环面向连接服务器、并发无连接服务器、并发面向连接服务器。 传输层传输层服务 多路复用/分用、可靠数据传输机制、流量控制机制、拥塞控制机制 提供进程之间的逻辑通信机制(TCP/UDP) 复用和分用 接收端多路分用 发送端多路复用 无连接分用：利用端口号创建socket，收到UDP段后检查其目的端口号，将UDP段导向绑在该端口号的socket。 面向连接的分用：接收端根据源IP+端口号和目的IP+端口号选择合适的Socket。 无连接传输协议 UDP Checksum校验和 可靠数据传输 Rdt 2.0：利用校验和检测位错误，正确则告诉发送方分组已正确接收(ACK)，错误则告诉发送方有错误(NAK)，发送方收到NAK后重传分组。 Rdt 2.1：对ACK和NAK添加checksum，为每个分组增加序列号避免重复分组。 Rdt 2.2：只使用ACK，接收方通过ACK告知最后一个被成功接收的分组。 Rdt 3.0：添加定时器，解决分组丢失的问题。 滑动窗口协议 发送方在收到ack之前连续发送多个分组 Go-Back-N(GBN)协议，窗口尺寸为N，最多允许N个分组未确认。为空中的分组设置定时器。ACK(n)：n及以前的分组被正确接收。超时则重传大于等于n的还未收到ACK的分组。 Select-Repeat(SR)协议，设置缓存机制，缓存乱序到达的分组。为每个分组设置定时器。 网络层网络层服务 从发送主机向接受主机传送数据段 核心功能：转发+路由 网络服务模型：无连接服务、连接服务 虚电路网络与数据报网络 数据报网络提供网络层无连接服务 虚电路网络提供网络层连接服务 类似于传输层的TCP/UDP，但是网络层服务是主机到主机的服务，由网络核心实现。 虚电路：一条从源主机到目的主机，类似于电路的逻辑连接。(分组交换，使用链路全部带宽) 虚电路通信过程：呼叫建立、数据传输、拆除呼叫。 数据报网络：网络层无连接、每个分组携带目标地址、路由器根据分组的目标地址转发分组。 IPv4协议 网络链路存在最大传输单元(MTU) 大IP分组向较小MTU链路转发时，可以被分片，IP分片到达目的主机后进行重组。 IP地址：32比特(IPv4) 网络号(NetID)：高位比特 主机号(HostID)：低位比特 IP子网：IP地址具有相同网络号的设备接口，不跨越路由器可以彼此物理连通的借口。 A类地址：0.0.0.0~127.255.255.255 B类地址：128.0.0.0~191.255.255.255 C类地址：192.0.0.0~223.255.255.255 D类地址：224.0.0.0~239.255.255.255 E类地址：240.0.0.0~255.255.255.255 特殊IP地址： NetID HostID 作为IP分组源地址 作为IP分组目的地址 用途 全0 全0 可以 不可以 在本网中表示本机，路由表中表示默认路由 全0 特定值 不可以 可以 本网内某个特定主机 全1 全1 不可以 可以 本网广播地址 特定值 全0 不可以 不可以 网络地址，表示一个网络 特定值 全1 不可以 可以 对特定网络所有主机广播 127 非全0或非全1的任何数 可以 可以 环回地址 私有IP地址： Class NetIDs Blocks A 10 1 B 172.16 to 172.31 16 C 192.168.0 to 192.168.255 256 子网划分：将原网络的主机号的部分比特设为子网号。 子网掩码：NetID、SubID全取1，HostID全取0。 A类子网掩码：255.0.0.0 B类子网掩码：255.255.0.0 C类子网掩码：255.255.255.0 借用3比特划分子网的B网的子网掩码：255.255.224.0 无类域间路由聚合(CIDR) 无类域间路由聚合：a.b.c.d/x，x为前缀长度。 路由聚合：最长前缀匹配优先 DHCP协议 主机广播-发现报文 DHCP服务器-提供报文 主机请求IP地址-请求报文 DHCP服务器分配IP地址-确认报文 网络地址转换(NAT) 所有离开本地网络去往Internet的数据报的源IP地址需替换为相同的NAT IP地址以及不同的端口号。 违背端到端通信原则，开发者必须考虑NAT的存在,e.g. P2P。 ICMP协议 互联网控制报文协议ICMP支持主机和路由器：差错报告、网络探寻 两类ICMP报文：差错报告报文(目的不可达、源抑制、超时/超期、参数问题、重定向)、网络探寻报文(回声请求与应答报文、时间戳请求与应答报文) IPv6 IPv6数据报格式：固定长度的40字节基本首部，不允许分片。 移除checksum，加快每跳时间。 路由算法 链路状态路由算法：Dijkstra算法，所有节点掌握网络拓扑和链路费用，计算从一个节点到其他所有节点的最短路径。 距离向量路由算法：Bellman-Ford算法，计算到邻居的费用与邻居到目标费用的最小值。存在无穷技术问题。 层次路由：聚合路由器为一个区域-自治系统AS。同一AS内的路由器运行相同的算法。 网关路由器位于AS边缘，通过链路连接其他AS的网关路由器。 热土豆路由：将分组转发给最近的网关路由器 Internet路由 Internet采用层次路由 常见的AS内部路由协议：路由信息协议(RIP)、开放最短路径优先(OSRP)、内部网关路由协议(IGRP) RIP：距离度量,存在最大跳步数(e.g. 15)，每条链路1个跳步。 RIP：每隔30s，邻居交换一次DV(距离向量)，成为通告。180s没有收到通告则该链路失效。 RIP：每次通告最多25个目的的子网。 OSPF：采用链路状态路由算法，利用Dijkstra计算路由。 OSPF：允许使用多条相同费用的路径。 BGP：eBGP(从邻居AS获取子网可达信息)、IBGP(向所有AS内部路由器传播子网可达信息) 数据链路层数据链路层服务 数据链路层负责通过一条链路，从一个节点向另一个物理链路直接相连的相邻节点传送数据报。 组帧、链路接入、相邻节点间可靠交付、流量控制、差错检测、差错纠正、全双工和半双工通信控制。 差错编码 检错码、纠错码 对于检错码，如果编码集的汉明距离$d_s=r+1$，则该检错码可以检测$r$位的差错。 对于纠错码，如果编码集的汉明距离$d_s=2r+1$，则该纠错码可以纠正$r$位的差错。 奇偶校验码：1比特校验位(检测奇数位差错)、二维奇偶校验(检测奇数位差错、部分偶数位差错，纠正同一行/列的奇数位错) checksum 循环冗余校验码CRC 多路访问控制(MAC)协议 采用分布式算法决定节点如何共享信道，即决策节点何时可以传输数据。 信道划分MAC协议(多路复用技术，TDMA、FDMA、CDMA、WDMA)、随机访问MAC协议(冲突恢复机制)、轮转MAC机制(节点轮流使用信道) 随机访问MAC协议 两个或多个结点同时传输，检测冲突并从冲突中恢复。 典型的随机访问MAC协议：时隙ALOHA、ALOHA、CSMA、CSMA/CD、CSMA/CA 时隙ALOHA：假定所有帧大小相同，时间被划分为等长的时隙，结点只能在时隙开始时刻发送帧，结点间时钟同步，如果两个或两个以上结点发送帧则结点检测到冲突。 时隙ALOHA：无冲突——在下一时隙继续发送新的帧，冲突——下一时隙以概率p重传该帧，直至成功。 ALOHA：有新的帧生成时立即发送，冲突可能性增大，效率比时隙ALOHA更差！ 载波监听多路访问协议CSMA：发送帧之前监听信道，信道空闲发送完整帧，信道忙则推迟发送。问题：继续发送冲突帧，浪费信道资源。 CSMA/CD：检测到冲突后传输终止，减少信道浪费。“边发边听，不发不听。” 轮转访问MAC协议 主节点轮流邀请从属节点发送数据 局域网ARP协议 48位MAC地址：固化在网卡的ROM中，有时也可以软件设置。作用于局域网内标识一个帧从哪个接口发出，到达哪个物理相连的其他接口。 ARP表：LAN中的每个IP结点(主机、路由器)维护一个表，存储某些LAN节点的IP/MAC地址映射关系。 以太网 物理拓扑：总线、星型 以太网的MAC协议：采用二进制指数退避算法的CSMA/CD 以太网交换机：储存-转发以太网帧，检验到达帧的目的MAC地址，选择性向一个或多个输出链路转发帧。主机感知不到交换机的存在。即插即用。自学习。 虚拟局域网VLAN：支持VLAN划分的交换机，可以在一个物理LAN架构上配置、定义多个VLAN。 PPP协议 常见的点对点数据链路控制协议：HDLC、PPP PPP设计需求：组帧、比特透明传输、差错检测、连接活性检测、网络层地址协商。 802.11无线局域网 无线信道很难实现边发送边检测冲突。 发送端首先利用CSMA向BS发送一个很短的RTS帧，BS广播一个CTS帧。通过很小的预约帧避免数据帧冲突。","link":"/2021/10/15/ComputerNet/"},{"title":"Awesome-Meta-Learning","text":"Zero-Shot / One-shot / Few-Show / Low-Shot Learning Siamese Neural Networks for One-shot Image Recognition, (2015), Gregory Koch, Richard Zemel, Ruslan Salakhutdinov. Siamese Neural Network是一种Metric Learning的方法，适用于类别数量特别多或者类别数量不确定的情况。当类别的样本数量过少时，传统方法难以学到较好的效果，故使用Siamese Neural Network去学习一个相似性度量，如欧氏距离、cosine距离等。训练时最小化来自相同类别的一对样本的损失，最大化来自不同类别的一对样本的损失。用这个度量去比较和匹配新的未知类别的样本。 Prototypical Networks for Few-shot Learning, (2017), Jake Snell, Kevin Swersky, Richard S. Zemel. 在Few-Shot情况下，Prototypical Network对每个类别的样本做embedding后对结果求均值，作为该类别的prototype。在对新样本做预测的时候，新样本的embedding结果距离哪个prototype最近就将该样本分到此类别。在Zero-Shot情况下，meta-data向量不是由训练集的support样本产生的，而是根据每个类的属性描述、原始数据等生成的。如下图： Gaussian Prototypical Networks for Few-Shot Learning on Omniglot, (2017), Stanislav Fort.还没看 Matching Networks for One Shot Learning, (2017), Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra. 融合了参数化和非参数化的方法，训练时将样本作为序列输入LSTM，让模型纵观所有的样本并进行输出。测试时用LSTM对测试样本进行K此迭代编码进行输出。使用Cosine距离对两者的相似度进行度量，随后做softmax。 Learning to Compare: Relation Network for Few-Shot Learning, (2017), Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, Timothy M. Hospedales. One-shot Learning with Memory-Augmented Neural Networks, (2016), Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, Timothy Lillicrap. Optimization as a Model for Few-Shot Learning, (2016), Sachin Ravi and Hugo Larochelle. An embarrassingly simple approach to zero-shot learning, (2015), B Romera-Paredes, Philip H. S. Torr. Low-shot Learning by Shrinking and Hallucinating Features, (2017), Bharath Hariharan, Ross Girshick. Low-shot learning with large-scale diffusion, (2018), Matthijs Douze, Arthur Szlam, Bharath Hariharan, Hervé Jégou. Low-Shot Learning with Imprinted Weights, (2018), Hang Qi, Matthew Brown, David G. Lowe. One-Shot Video Object Segmentation, (2017), S. Caelles and K.K. Maninis and J. Pont-Tuset and L. Leal-Taixe’ and D. Cremers and L. Van Gool. One-Shot Video Object Segmentation, (2017), S. Caelles and K.K. Maninis and J. Pont-Tuset and L. Leal-Taixe’ and D. Cremers and L. Van Gool. One-Shot Learning for Semantic Segmentation, (2017), Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, Byron Boots. Few-Shot Segmentation Propagation with Guided Networks, (2018), Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alexei A. Efros, Sergey Levine. Few-Shot Semantic Segmentation with Prototype Learning, (2018), Nanqing Dong and Eric P. Xing. Dynamic Few-Shot Visual Learning without Forgetting, (2018), Spyros Gidaris, Nikos Komodakis. Feature Generating Networks for Zero-Shot Learning, (2017), Yongqin Xian, Tobias Lorenz, Bernt Schiele, Zeynep Akata. Meta-Learning Deep Visual Words for Fast Video Object Segmentation, (2019), Harkirat Singh Behl, Mohammad Najafi, Anurag Arnab, Philip H.S. Torr. Model Agnostic Meta Learning Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks, (2017), Chelsea Finn, Pieter Abbeel, Sergey Levine. 与模型无关的元学习方案，可以与采用梯度下降方法更新参数的算法相结合，而不改变参数的数量。如图所示，MAML的目标是学习一个$\\theta$，使得其在新任务上只经过一次或几次梯度下降就可以达到较好的结果。故MAML的损失函数为在support-set上进行一次梯度下降后，在query-set上的损失。由于计算梯度的时候要计算二阶导数，故也可舍弃一定的performance采用一阶近似。 Adversarial Meta-Learning, (2018), Chengxiang Yin, Jian Tang, Zhiyuan Xu, Yanzhi Wang. On First-Order Meta-Learning Algorithms, (2018), Alex Nichol, Joshua Achiam, John Schulman. Meta-SGD: Learning to Learn Quickly for Few-Shot Learning, (2017), Zhenguo Li, Fengwei Zhou, Fei Chen, Hang Li. Gradient Agreement as an Optimization Objective for Meta-Learning, (2018), Amir Erfan Eshratifar, David Eigen, Massoud Pedram. Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace, (2018), Yoonho Lee, Seungjin Choi. A Simple Neural Attentive Meta-Learner, (2018), Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, Pieter Abbeel. Personalizing Dialogue Agents via Meta-Learning, (2019), Zhaojiang Lin, Andrea Madotto, Chien-Sheng Wu, Pascale Fung. How to train your MAML, (2019), Antreas Antoniou, Harrison Edwards, Amos Storkey. Libraries Tensorflow Federated learn2learn Datasets Omniglot mini-ImageNet ILSVRC Nuscenes Reference: https://github.com/sudharsan13296/Awesome-Meta-Learning","link":"/2021/10/08/meta_learning/"},{"title":"为什么短线投机的预期收益不是零而是一赢二平七亏损?","text":"这是一个很有意思的问题，如果靠扔飞镖来随机选取股票，理论上来讲，说短线投机行为的预期回报率应该是市场的平均回报率。但显然这在大部分情况下都不应该是一赢二平七亏损，不然我们可以很轻易的反向操作从而获利。那么，是什么因素导致了这一现象呢？ 资本资产定价模型CAPM为了解释这一现象，我们从CAPM模型讲起。 $$R_p-R_f=\\alpha+\\beta(R_m-R_f)$$ 其中，$R_p$为投资组合的收益率，$R_f$为无风险收益率，$R_m$为市场的收益率，而$\\alpha$就代表超额收益。 盈亏的不对称性来自于哪里通过CAPM模型，我们可以做出以下推论： 随机选取股票的方式$\\alpha$为0，投资组合的收益率来自于$\\beta$，也就是说虽然我们可以通过控制仓位或者加杠杆等方式改变$\\beta$，但最终能够赚钱纯靠运气。 一赢代表的人群包括了通过$\\beta$靠运气赚钱的人和靠正的$\\alpha$赚钱的人。 二平代表的人能力和运气相互抵消。 七亏损代表的人群包括通过$\\beta$靠运气亏钱的人和靠负的$\\alpha$亏钱的人。 运气大家都是一样的，2-3的不对称性说明少数人有正的$\\alpha$，他们持续从大部分负$\\alpha$的人手里不断地赚钱。 也就是说，这10%的人赚了其他70%的人的钱说明了一个问题，那就是新入股市的人们自带的$\\alpha$是负的！没有经验的小白选股还不如扔飞镖乱选！这真是一个反直觉的问题。 为什么大部分人的$\\alpha$都是负的呢？在分析这个问题之前，我们要深入思考一些问题，你赚的钱来自哪里？你按照什么逻辑赚的钱($\\alpha$的逻辑)？ 博弈！博弈！博弈！ 这与赌场十分类似，我们用赌场来举例，赌场的微小的正$\\alpha$来自于手续费，而赌客的$\\alpha$来自于人与人之间的博弈，虽然玩猜大小的游戏时，赌客的$\\alpha$都是负的，但在玩德州扑克等游戏时，有人可以通过记牌和计算概率的方式取得正的$\\alpha$，这会导致不会记牌的赌客获得一个绝对值更大的负$\\alpha$。 回到股市，证券交易所的$\\alpha$来自于股民们的手续费，当你随机交易时，你的预期收益就是只亏损掉手续费，但当你通过分析来买卖股票时，由于大部分人的经验都来自于实际生活，都是大致相同的，在通过这些经验分析时会做出趋同的判断，则可能会被人预判到你的分析，从而在博弈中得到一个负的$\\alpha$。 总结来说，因为大部分人的分析是趋同的，有经验的投资者可以轻易的预判到你的分析，从而他们可以在博弈中取得正的$\\alpha$而你会得到负的$\\alpha$。 如何取得正的$\\alpha$ 价值投资：通过超越大多数人的专业知识对公司进行更准确的判断取得正的$\\alpha$。 博弈(量化)：找到在与其他投资者博弈的过程中胜出的技巧。 第二种方式是收益最高的方式，也是最难的方式，就算你找到了一个$\\alpha$，它也会随着时间逐渐的失效。曾经的海龟策略、双均线策略以及打板等手法都可以做到很高的收益，然而随着用的人越来越多，他们就会在博弈的时候越来越被针对，最终失效。 价值投资是最适合散户的一种方式，因为散户大多有着自己的本职工作，很难做到每天花费大量的时间与其他的投资者进行动态的博弈。然而一个企业的基本面的变化一般是比较缓慢的，我们只需要在前期花费时间调研，选好后坚定持有，静待花开。","link":"/2021/04/27/alpha/"},{"title":"价值投资思路","text":"太长不想看版 预期收益：长期年化10%以上 持股时间：基本面不变化除非较为高估不卖 买入：在合理的估值买入伟大的公司 持有：顶住波动，坚定持有 卖出：在较为高估时卖出 买入篇找到一个好的企业什么是好？ “好”这个概念很模糊，我们怎样去定义一个企业是好还是不好呢？从投资的角度来看，股价持续上涨的企业就是好企业，而股票的价格又是围绕着企业的价值上下波动的，虽然市场对于价格与价值背离的忍耐度可能有时候比较强，但没有业绩支撑的股票在爆炒之后总是一地鸡毛。如此，我们的问题就变成了怎么找到一个越来越有价值的企业，具体一点可以表现为利润越来越多的企业。 企业利润的增长来源于以下几个方面，首先可以来源于需求的增加，如果该企业处于一个蓬勃发展的行业，市场需求不断扩大，即使该企业在与竞争对手的竞争中不占优势也能不断实现业绩的增长，就想俗话说：风口来了猪都能飞起来。在这种情况下，单纯的扩大产能就可以实现利润的增长，但一旦市场需求达到饱和，该种企业也就从风口掉了下来。利润增长的第二个来源是价格的提升或者成本的降低，价格的提升一般在垄断类型的企业中比较常见，而成本的降低往往来自于产能的增加、人员结构优化、原材料获取成本降低等。优秀的企业往往在市占率提高的同时还能提高自己的净利率。 从利润的角度我们明白了什么是一个好的企业，但还有一个至关重要的问题就是利润的增长能否持续，茅台之所以神是因为他的高增长持续了近二十年而看不到减缓的迹象。我们接下来进行探究什么样的企业可以常年稳定持续增长。首先，企业处于弱周期行业，比如消费行业、医疗行业，不管这个世界发生了什么事情，我们总是要吃饭，总是要看病的。其次，垄断地位，比如茅台之于白酒、腾讯之于互联网。垄断意味着提价权，意味着更高的毛利，意味着很难被别的公司超越。 买入的时机虽然，按照上述逻辑，我们很轻易的找到了很多公司，但如果在极为高估时顶着60倍的PE买入茅台，难道要拿住5年等待解套吗？因此，在一个合理的估值买入伟大的公司，这才是我们要做的事情。 如何估值是一个很难的问题，最简单的方法是看公司的市盈率PE的高低在历史上处于什么样的水平，一般来说，熊市时整个市场的股票PE都偏低，而牛市整个市场的股票PE都偏高。所以最理想的情况就是在熊市低估时买进，忽略期间的巨额波动，在牛市高估时卖出。 持有篇价值投资的最难之处在于持有，就算在一个较为低估的时候买入，一切逻辑都十分看好，但当很多股票走出翻倍行情时，你持有的股票震荡了两年，你的心态会有变化吗？你明知道拿住就可以翻倍，但每次打开账户，里面的钱越来越少，你的心态会有变化吗？好不容易等来一次拉升，赚了20个点，但由于还没有高估，你没有卖出，几天后又跌了回去，你的心态又会怎么变？ 所以，想做价值投资，最简单的方法就是少看盘，当在估值低位分批进场后，卸载软件吧！ 卖出篇如何衡量牛市到了哪个阶段，就看身边人的狂热情况，大量的韭菜冲入股市，新闻联播开始播报股市气象，大妈喊出股市10000点的口号之后，就可以卖了。虽然可能卖出之后股票每天都还在涨，千万不要追高买回来！学会空仓，等到股市崩盘，一地鸡毛，没有人再谈论股票时，你就可以继续开始你的下一次轮回！","link":"/2021/03/15/value-investment/"},{"title":"深度学习","text":"回顾机器学习定义一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。 机器学习的核心 数据 模型分类 有监督学习 回归 线性回归 分类 SVM 无监督学习 聚类 主成分分析 半监督学习 增强学习(Reinforcement Learning)学习过程(监督学习) 损失函数(loss function) 优化方法 梯度下降深度学习是什么 wiki：深度学习（英语：Deep Learning）是机器学习的分支，是一种以人工神经网络为架构，对数据进行表征学习的算法。 深度学习是机器学习的子集 深度学习和传统机器学习算法的异同数据方面 Andrew Ng：“与深度学习类似的是，火箭发动机是深度学习模型，燃料是我们可以提供给这些算法的海量数据。 计算量方面深度学习在更新模型网络权重时涉及大量矩阵运算，在CPU上跑速度会很慢，而传统机器学习算法随便一台电脑就可以跑。因此深度学习最好在GPU上跑。 耗时量级： 传统机器学习：秒、分钟、小时 深度学习：小时、天、周 输入特征方面机器学习依赖于人类精心设计的特征才能取得较好的结果，深度学习主张让算法自己从原始数据中发现特征。不用太过高深的先验知识做支撑，但因此对数据量的需求比较大。 深度学习算法人工神经网络(ANN)其中的一个节点：激活函数一定是一个非线性函数，用来增加网络的复杂性。不然不管网络有多少层，始终是一个线性函数。常用激活函数： relu x if x &gt; 0 0 if x &lt;= 0 tanh 卷积神经网络(CNN)卷积在深度学习里的卷积,与数学上的和信号处理上关于卷积的概念有些不同. CNN结构两条基本假设： 最底层特征都是局部性的，也就是说，我们用10x10这样大小的过滤器就能表示边缘等底层特征 图像上不同位置处特征是类似的，也就是说，我们能用同样的一组分类器来描述不同位置的图像——平移不变性 关键：局部连接，权值共享，池化 CNN在处理图像数据时与ANN相比有着巨大的优势，通过局部连接、权值共享和池化大大减少了参数的数量，从而大大减少计算量、减少过拟合并大大提高模型的表现。 模型训练模型训练的过程可以认为是使损失函数最小化的过程 模型泛化与过拟合、欠拟合问题因为深度学习的表达能力很强，当你的模型的表现很好时，你需要警惕，是模型学到了规律还是说模型记住了数据。检测方法也很简单，前者在一个陌生的数据集上表现依然很好，而后者反之。因此，在设计模型的时候也要考虑使用一些方式来尽可能的避免过拟合，从而得到较好的泛化能力。 书籍推荐入门书籍：该书简单易懂，为keras之父写的书。好上手，学了就能用，里面有很多demo可以跑。 进阶书籍：该书讲了很多数学、线代、概率论还有优化的东西，被奉为深度学习圣经，俗称“花书”，适合作为工具书，在手边随时查阅，入门较吃力。 主流深度学习框架 Tensorflow Pytorch Keras Paddlepaddle 总结 人工智能&gt;机器学习&gt;深度学习 需要更多的数据量和算力 ANN 输出是输入的复杂非线性函数 CNN 局部连接 权值共享 池化 模型训练 损失函数 梯度下降 注意过拟合与欠拟合","link":"/2020/03/20/deep-learning/"},{"title":"Third Poem","text":"HIDING&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie Booming, booming bus. Rains dropped on the windows. Outside is the dusky sky With dark clouds hiding somewhere. Inside is a complacent heart Without being complimenting. Hiding the disappointment From a beautiful fragile mind, Where intellect should always stand. I show my courage to my cold little heart, which might suffer from wild heat of tricking, but would not go into hiding Any more.","link":"/2019/12/17/poem3/"},{"title":"机器学习概览","text":"机器学习是什么定义 wiki：机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。 Arthur samuel：机器学习是在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。 一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。 让我引用一张图片来说明：注意：这里是用有监督模型举例(后面会对这个名词进一步解释) 针对机器学习最重要的内容 数据：经验只有转化为了计算机能够理解的数据，才能让计算机从中学习。谁的数据量大、质量高，谁就占据了机器学习和人工只能领域最有利的资本。 模型：即算法，有了数据之后，可以设计模型，通过数据来训练这个模型。这个模型就是机器学习的核心，作为用来产生决策的中枢。 数据的分类 结构化数据———储存在数据库中的数据 非结构化数据——-语音信号、图像图形、自然语言 机器学习能干什么 语音识别、机器翻译(微软Cortana、苹果Siri、科大讯飞、谷歌翻译) 人脸识别(微信、支付宝、宿舍门禁) 量化交易(预测股市) 房价预测 推荐系统(淘宝、京东、抖音) 医生/老中医 解微分方程、不定积分(见：AI拿下高数一血，求解微分方程、不定积分只需1秒，成绩远超Matlab) 寻找淹没在背景噪声中的小信号(Higgs Boson Machine Learning Challenge、引力波信号、引力透镜参数预测) 好用的机器学习库以及书籍推荐scikit-learn：最有名且易用好上手，广泛作为机器学习的入门库(Python)书籍： 机器学习的分类按照模型的学习方式，我们可以分为如下几类： 有监督学习对于数据集中的每一条数据，我们在把它交给算法前就有了相应的“正确答案”，我们的算法就是在基于这些我们人为给定的“正确答案”在做预测。 有监督学习的任务一般是回归或者分类问题： 回归：线性回归比如通过商品房的地段、高度、外形、面积、采光面积等参数来预测商品房价格。预测结果是一个连续的值。 分类：支持向量机(SVM)、决策树、逻辑回归、朴素贝叶斯、KNN比如通过西瓜的颜色、大小、重量、花纹等等预测西瓜甜不甜。请注意，这里我们预测的输出只包括甜和不甜，是一个离散值，这是一个二分类的问题。反之，若是我们输出的是西瓜介于0到1之间的的甜度(0是一点都不甜，1是超级甜)，那这个分类问题就转化为了回归问题。 无监督学习对于数据集中的所有数据，他们没有一个相应的“正确答案”。算法要做的是利用算法自动的将数据归类，也叫做聚类。 KMEAN 半监督学习介于监督学习和无监督学习之间的一种方式。即一部分数据有标记，一部分数据没有标记。 增强学习增强学习是一种有反馈的学习方式。 例子：贪吃蛇问题一个N×N的格子里，定义贪吃蛇每一步上下左右随机行走，碰到墙或者自身则得到负反馈，吃到果子得到正反馈，在训练很多轮以后，贪吃蛇就学会了如何躲避墙和自身去吃果子。 机器学习的一般步骤数据采集和标记 构建数据集：收集尽可能的多的特征，给出数据标记（人工或自动） 预测房价：面积大小、房间数、地段、楼龄等；芝麻信用：海量的用户交易数据； 数据清洗 对数据中的单位进行统一 去掉重复数据、噪声和数据缺失 特征选择 从哪些特征对进行机器学习是有用的；人工设计或自动选择 模型选择 根据问题选择模型，聚类还是分类，回归还是分类 模型训练和测试 把数据集合分为训练集和测试集，用训练集训练模型，训练完成后用测试集测试模型的精度。(测试集必须是模型没有见过的数据) 模型性能评估与优化 训练时长，训练数据是否足够，是否能满足需要 模型使用 训练好的模型进行存储，以备下次使用 机器如何学习损失函数例如：用一维线性回归举例: $$ y = kx+b $$ x就是我们的input，y就是我们的label，我们首先给算法一定的(x,y)数据，算法拟合出来一条直线方程，当我们再输入x数据时，模型能够预测出相应的y。这就是一个简单的有监督机器学习例子。但是，机器怎么知道哪个k和b是最好的呢？ 也就是说，我们需要用一个指标来衡量模型和数据的拟合程度，而模型的预测值和真实值的差，我们叫做损失函数。在这里，我们训练的一个线性回归模型，可以是让MSE(均方误差)最小，MSE在这里被称为这个回归模型的损失函数，它代表了预测值与真实值的偏离程度。而我们机器学习的过程就是通过改变k和b使得损失函数取得最小值。$$L_{MSE}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$这里的m表示数据个数。 除此之外，对于二分类问题来说，常用的损失函数是二元交叉熵损失(Logistic损失): $$L_{\\text {logistic }}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}[-y_{i} \\log \\hat{y}{i}-\\left(1-y{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)]$$ 这里的 $\\hat{y}$ 代表预测y=1的概率。 梯度下降我们知道了我们的目标是让k和b最小，那我们怎么实现呢？接下来我们来看一下它的解决方案——使用梯度下降算法来更新参数。 这里 $\\theta_0$ 和 $\\theta_1$ 分别代表k和b。 我们从一个随机的k和b出发，沿着向下最快的路径行走，直到达到最低点，即此时k和b收敛于一个定值，这个定值就是我们想要得到的使得损失函数最小的值。 代码实现本文使用scikit-learn实现一个线性回归模型举例： 12345from sklearn.linear_model import LinearRegression #从sklearn引入线性回归模型model=LinearRegression() #声明线性回归模型model.fit(X_train,Y_train) #用训练数据训练模型Y_pred=model.predict(X_test) #用模型对测试数据做预测print(Y_pred) #输出预测结果 常用的API有： API 解释 fit(X_train,Y_train) 训练模型 predict(X_test) 预测测试数据的结果 score(X_test,Y_test) 测试预测数据的score(例如正确率) 这里只是展示了很少很少的API，还有很多非常非常实用的API及教程参见官方文档 总结 1.获取数据 2.处理数据(80%的时间) 3.训练模型(20%的时间) 4.进行预测 5.观察结果，不满意则重复2.3.4.步，满意则保存模型","link":"/2019/12/14/ml-start/"},{"title":"Second Poem","text":"POUNDING&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie Pounding, pounding heart, Why you pump so hard? Feeling a bunch of vessels Extending to my roots, Where blood poured After running across Wvery finger of my nerves. Who am I Besides a cluster of cells, Tramping in the sounds...","link":"/2019/12/14/poem2/"},{"title":"First Poem","text":"LOST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie Chilled night, chilly light, Wind rolling sky, Cloud over head. Ashes tangled my hair, Mind lost from ear. Running from sunlight, The music ignite a fire, Lit a face in the dusky theater air; The dance stepped the melody Into my eyes gently. Tears shining In the reflected screen light, Heart flipping Over the woebegone rejoicing With the beauty of art. I've lost my blue From their bloody mouth. Taping up and down, Their tip of tongue fan a fame. When there is no longer a pure face But tech race on the screen, They stop looking into themselves. I've shut my soul from heat of lies, But lost in my chilling heart. Looking outside From the window of fear, Everyone seems to be tired. Walking over again Like nothing changed there, I want my feeling back.","link":"/2019/12/12/poem1/"},{"title":"Hexo+markdown开始博客写作","text":"目录 Hexo安装 Hexo使用 Markdown写作 Hexo安装引用Hexo官方文档： 什么是 Hexo?Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装安装 Hexo 只需几分钟时间，若您在安装过程中遇到问题或无法找到解决方式，请提交问题，我们会尽力解决您的问题。 安装前提安装 Hexo 相当简单，只需要先安装下列应用程序即可：Node.js (Node.js 版本需不低于 8.10，建议使用 Node.js 10.0 及以上版本)Git 如果您的电脑中已经安装上述必备程序，那么恭喜您！你可以直接前往 安装 Hexo 步骤。如果您的电脑中尚未安装所需要的程序，请根据以下安装指示完成安装。 安装 Git Windows：下载并安装 git. Mac：使用 Homebrew, MacPorts 或者下载 安装程序。 Linux (Ubuntu, Debian)：sudo apt-get install git-core Linux (Fedora, Red Hat, CentOS)：sudo yum install git-core 安装 Node.jsNode.js 为大多数平台提供了官方的 安装程序。对于中国大陆地区用户，可以前往 淘宝 Node.js 镜像 下载。 其它的安装方法： Windows：通过 nvs（推荐）或者nvm 安装。 Mac：使用 Homebrew 或 MacPorts 安装。 Linux（DEB/RPM-based）：从 NodeSource 安装。 其它：使用相应的软件包管理器进行安装，可以参考由 Node.js 提供的 指导对于 Mac 和 Linux 同样建议使用 nvs 或者 nvm，以避免可能会出现的权限问题。 这里需要用到刚才安装的git，在git bush里进行如下操作： 安装 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。 $ npm install -g hexo-cli 这样，Hexo就成功安装好了，下一步就可以着手在本地搭建自己的博客了。 Hexo使用如果你已经有了一个建好的网站，并且已经上传到了github，可将网站代码克隆到本地。 在选定的文件夹下： 1$ git clone git@github.com:heros979/My_blog.git clone后面是你的github代码库的地址，示例是我的，你们要改成你们自己的。 如果你是第一次建立自己的网站，在你想要的（随意哪个都行）文件夹里右键进入git bush，输入： 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 其中folder是你存放代码的文件夹名字 好了，我们的文件夹里有了很多新的文件,如下： 12345678.├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 其中 _config.yml 是网站的配置信息，您可以在此配置大部分的参数。如果是第一次建立的话，要在deploy处加入自己的github地址。例如我的： 我们要写的博客就放在 _post 文件夹里。在根目录可以使用如下方式新建post 1hexo new &lt;post-name&gt; 我们在写完post后在根目录输入： 123hexo cleanhexo ghexo d 即可将我们的网站部署到VPS服务器和我们的github上。 Markdown写作打开我们刚才建立的post-name.md文件，我们要用markdown的语法来写这个页面，markdown是一个很简单的写作工具，也很好看，很实用。 例子1： 123456*开辟鸿蒙，谁为情种？* #这是斜体**都只为风月情浓。** #这是加粗***奈何天，伤怀日，*** #这是斜体加粗&lt;font size=4&gt;寂寥时，试遣愚衷。&lt;/font&gt; #自定义字体大小~~因此上，演出这~~ #这是删除线&gt; 怀金悼玉的《红楼梦》。 # 这是引用 开辟鸿蒙，谁为情种？都只为风月情浓。奈何天，伤怀日，寂寥时，试遣愚衷。因此上，演出这 怀金悼玉的《红楼梦》。 例子2： 123456789101112131415161718192021222324## CALLED BACK #这是标题，几个#就是几级标题&lt;font face=&quot;微软雅黑&quot; size=6 color=#FF0000 &gt;Just lost when I was saved!&lt;/font&gt; #修改字体字号颜色&lt;table&gt;&lt;tr&gt;&lt;td bgcolor=orange&gt;Just felt the world go by!&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; #修改背景色Just girt me for the onset with eternity,When breath blew back,And on the other sideI heard recede the disappointed tide!Therefore, as one returned, I feel,Odd secrets of the line to tell!Some sailor, skirting foreign shores,Some pale reporter from the awful doors Before the seal!Next time, to stay!Next time, the things to seeBy ear unheard,Unscrutinized by eye.Next time, to tarry,While the ages steal,—Slow tramp the centuries,And the cycles wheel. CALLED BACKJust lost when I was saved! Just felt the world go by! Just girt me for the onset with eternity, When breath blew back, And on the other side I heard recede the disappointed tide! Therefore, as one returned, I feel,Odd secrets of the line to tell!Some sailor, skirting foreign shores,Some pale reporter from the awful doorsBefore the seal! Next time, to stay!Next time, the things to seeBy ear unheard,Unscrutinized by eye. Next time, to tarry,While the ages steal,—Slow tramp the centuries,And the cycles wheel. 更多语法，比如标题、换行、注释、分割线、引用、代码块、列表、链接、表格、图片、流程图、LaTeX。 参见：Markdown快速入门，Markdown语法(字体,样式,公式,背景,图表等)","link":"/2019/12/11/writing/"},{"title":"DigitalOcean+Hexo+Nginx+Namecheap搭建个人博客","text":"强烈推荐Github学生包，内含大量对学生的免费福利，包括不限于Jetbrain全家桶，AWS，Azure，DigitalOcean，Namecheap，name等。本文基于Github学生包里的DigitalOcean $50和Namecheap一年个人域名搭建个人博客。 技术架构 VPS：DigitalOcean 域名注册：NameCheap 博客框架：Hexo 自动部署：githook 前期准备 学生，或者有一个.edu后缀的邮箱 可以给国外付款的visa卡，或者PayPal（可使用银联的借记卡） $5用来激活账户 Github学生认证这一步网络上有大量教程，本文不再赘述。 参见： 注册Github并进行学生认证 Github教程 学生认证 注册Paypal亲测中国银行借记卡(有union标志)可用。 参见： 注册申请PayPal支付账户 PayPal注册绑卡使用教程 注册DigitalOcean这一步也很简单，参见网上教程，只是最近注册完之后增加了一步verify，需要实名认证。processing的过程有点慢，我的认证13个小时才给我成功，如果卡在processing不用急，睡一觉就好了。 参见： 在GitHub Students Developer Pack申请DigitalOcean的50刀优惠码 从领取Github教育礼包到DigitalOcean购买服务器 申请一个服务器新建一个实例（droplets）,系统选择ubuntu（centos也可），standard（标准型，我们用来搭建个人博客足够了）： 价格选择最便宜的，这样可以用接近一年 重点来了！！！ 服务器既然在国外，那速度肯定是首要考虑的。所以在选择服务器所在地区的时候，首先我们可以在官网测一下到达每个地方的速度，选择最快的地方搭建我们的服务器。在这里我选择了Frankfurt： 下面是一些常规选项，IPV6看个人需求可要可不要，其他有些是付费的，没需求就不用改 最后点击create droplet等待30s即创建好第一个服务器，随后服务器IP，Root，密码会发到你的邮箱里。 本地配置hexo怎么安装，怎么使用，怎么用markdown写作，怎么部署到远程VPS服务器，在下一篇博客会讲。 部署Hexo到远程VPS服务器putty输入服务器IP和密码远程连接，在第一次登录时由于DigitalOcean的安全策略会让你修改自己的登录密码。 具体过程参见：简书-VPS部署Hexo网站 现在，在浏览器输入 http://[yourIP] 就可以看到你的个人网站了！下一步我们通过设置域名来访问。 注册域名/域名解析从Github学生包界面进入namecheap，挑选没有被别人占用的域名，确认后设置域名解析：Namecheap域名如何绑定IP 等待几分钟后，就可以通过你的.me域名进入网站了！","link":"/2019/12/10/blog-init/"},{"title":"2019大数据算法赛总结","text":"之前一直对机器学习比较感兴趣，大三开学开始学习深度学习，正巧赶上全国高校计算机挑战赛开赛，就报名了我参加的第一个大数据方向的竞赛。这个比赛入门门槛很低，随便找个模型调用一下sklearn库就能跑出结果，但想争夺一个好的名次还是不太容易。 概览报名费用：¥150（每队） 参加人数： 290队 最终排名： 62 最终得分（AUC）：0.733 使用模型：XGBOOST 数据预处理数据概况train.csv文件中包括60000条记录，每一条记录包括Data字段和如下几列，每一列的含义如下: 缺失数据我们用Python读取数据首先查看有无缺失数据的情况，发现每一列都有60000条数据没有缺失值，有可能数据已经进行了缺失值的填充，后面再进行判断。 粗略观察数据和label的关系我们对每一列和label的关系进行绘图，观察他们的相关程度和是否有线性或者非线性的关系，例如下图： 扔掉基本没有差别的列：[‘E3’,’E5’, ‘E8’, ‘E9’,’E19’,’E26’’E29’] C2也要被丢掉，因为C2中绝大部分值都是一个相同的值，占比82.5%而其他的值的占比最多的才0.1%。 CTR（Click-Through-Rate）问题长尾效应CTR问题有的特点就是海量离散特征，且存在长尾效应（Long Tail Effect）： 即80%的效益来自于20%的特征，也就是说一个特征可能有成千上万种取值，但只有取值的频率最高的那些是最有用的，如果我们不对此进行处理，长尾的现象可能会降低我们模型的表现。我们由此对类别特征做了label-encoder，按照出现频率对频率较高的特征进行映射，并将出现频率很低的那些都映射为一个相同的值。 分箱处理有时候我们对有连续意思的离散特征做分箱处理会使模型的表现提高。 举个例子早上7点26分和早上7点27分对于是否会点击一个广告基本没有任何区别，但早上和晚上可能会有所不同。但如果直接把没有分箱的数据送给模型的话，模型会认为早上7点26分和早上7点27分就是两个完全不同的时间，因此我们可以简单的把时间信息按照早上、中午、下午、晚上划分，或者更细致一点的考虑的话，晚上人们的时间规划可能很不一样因此也可以划分的更细致一点，比如晚上按小时划分。 分箱方法虽然目前有很多做分箱的理论，但因为我们需要做分箱的特征只有一列，且我们需要分箱的数据在密度图下可以明显看出区别，故我们人工估计待分箱的箱数以及分箱的位置，我们认为该种方式比聚类方法分箱有直接、暴力、可解释性强等优点。 经过尝试，我们发现对C3进行分箱是一个很好的选择，它能显著提高我们模型的表现： 不平衡数据/Ensemble通过观察给出的数据我们发现这是一个不平衡数据集。有83%的人不会点击广告，而只有17%的人会点击广告。就算我们训练一个只会输出不会点击广告的模型，那我们也会有83%的正确率，我们试过欠采样和SMOTE但效果都不佳，因此我们采用Ensemble方式，抽取全部点击广告的人且对不会点击广告的数据进行欠采样使得抽出的数据集平衡，有放回的抽取N次，训练N个模型取平均得到最后的结果，模型的表现会有很大的提升。 模型选择我们尝试了LR，RandomForest，RandomForest+LR，GBDT，GBDT+LR，XGBOOST，XGBOOST+LR,MLR，DeepFM等模型 第一版模型无脑One-hot，发现随着特征列数的增加，所有的模型都趋于一个相同的值，AUC：0.710 第二版模型进行长尾数据的处理，缩减特征的维度再做One-hot，模型效果有所改善，但改善不大忘记最后结果了 第三版模型细致处理长尾数据，不要One-hot，XGBOOST细致调参，得分AUC上了0.720 第四版模型用DeepFM做相同的事情，重新调参，AUC：0.720 第五版模型细致处理长尾数据，不要One-hot，做Ensemble，XGBOOST的AUC上了0.728，DeepFM的AUC上了0.721 第六版模型将XGBOOST和DeepFM做模型融合（结果取平均），成功将AUC提到0.731 最终模型细致选取使用的特征，细致处理长尾数据，不要One-hot，做Ensemble，对C3进行分箱，训练50个单独的XGBOOST做平均，AUC：0.733","link":"/2019/12/09/summury-of-big-data-competition/"}],"tags":[{"name":"课程复习","slug":"课程复习","link":"/tags/%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0/"},{"name":"元学习","slug":"元学习","link":"/tags/%E5%85%83%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"投资","slug":"投资","link":"/tags/%E6%8A%95%E8%B5%84/"},{"name":"VPS","slug":"VPS","link":"/tags/VPS/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"网站","slug":"网站","link":"/tags/%E7%BD%91%E7%AB%99/"},{"name":"poem","slug":"poem","link":"/tags/poem/"},{"name":"bigdata","slug":"bigdata","link":"/tags/bigdata/"},{"name":"总结","slug":"总结","link":"/tags/%E6%80%BB%E7%BB%93/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"写作","slug":"写作","link":"/tags/%E5%86%99%E4%BD%9C/"}],"categories":[{"name":"课程","slug":"课程","link":"/categories/%E8%AF%BE%E7%A8%8B/"},{"name":"论文","slug":"论文","link":"/categories/%E8%AE%BA%E6%96%87/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"投资","slug":"投资","link":"/categories/%E6%8A%95%E8%B5%84/"},{"name":"博客","slug":"博客","link":"/categories/%E5%8D%9A%E5%AE%A2/"},{"name":"诗集","slug":"诗集","link":"/categories/%E8%AF%97%E9%9B%86/"}]}