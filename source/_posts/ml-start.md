---
title: 机器学习概览
date: 2019-12-14 12:07:39
tags: [机器学习]
mathjax: true
---
## 1.机器学习是什么

>机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。 [wiki](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)

机器学习是基于数据的算法，是让计算机从数据中学习的算法。

让我[引用](https://www.jianshu.com/p/25ef14c072ad)一张图片用有监督学习来说明：
![机器学习过程](https://upload-images.jianshu.io/upload_images/15134928-7fa468b68d7c7c2b.png)

首先，我们将我们的数据(input)和数据的标记(label)输入给算法，算法能够给出一个训练好的模型，当我们传递给这个模型没有见过的数据(input)时，它能够预测出该数据(input)的标记(label)。

例如：
<!--more-->
用一维线性回归举例:

 $$ y = kx+b $$

x就是我们的input，y就是我们的label，我们首先给算法一定的(x,y)数据，算法拟合出来一条直线方程，当我们再输入x数据时，模型能够预测出相应的y。这就是一个简单的有监督机器学习例子。当然，这里我隐藏了很多细节问题，后面我们会继续进行讨论。

## 2.机器学习能干什么

* 语音识别、机器翻译(微软Cortana、苹果Siri、科大讯飞、谷歌翻译)
* 人脸识别(微信、支付宝、宿舍门禁)
* 量化交易(预测股市)
* 房价预测
* 推荐系统(淘宝、京东、抖音等) 
* ~~老中医~~
* 解微分方程、不定积分(见：[AI拿下高数一血，求解微分方程、不定积分只需1秒，成绩远超Matlab](https://zhuanlan.zhihu.com/p/98174049?utm_source=zhihu&utm_medium=social&utm_oi=667848254054731776))
* 寻找淹没在背景噪声中的小信号([Higgs Boson Machine Learning Challenge](https://www.kaggle.com/c/higgs-boson)、引力波信号)

## 3.机器学习的分类

按照模型的学习方式，我们可以分为如下几类：

### 3.1.有监督学习

对于数据集中的每一条数据，我们在把它交给算法前就有了相应的“正确答案”，我们的算法就是在基于这些我们人为给定的“正确答案”在做预测。

有监督学习的任务一般是回归或者分类问题：

#### 3.1.1.回归

比如通过商品房的地段、高度、外形、面积、采光面积等参数来预测商品房价格。预测结果是一个连续的值。

#### 3.1.2.分类

比如通过西瓜的颜色、大小、重量、花纹等等预测西瓜甜不甜。请注意，这里我们预测的输出只包括甜和不甜，是一个离散值，这是一个二分类的问题。反之，若是我们输出的是西瓜介于0到1之间的的甜度(0是一点都不甜，1是超级甜)，那这个分类问题就转化为了回归问题。

### 3.2.无监督学习

对于数据集中的所有数据，他们没有一个相应的“正确答案”，也就是没有label。算法要做的是利用算法自动的将数据归类，也叫做**聚类**。

### 3.3.半监督学习

介于监督学习和无监督学习之间的一种方式。即一部分数据有标记，一部分数据没有标记。

### 3.4.增强学习

增强学习是一种有反馈的学习方式。

例子：贪吃蛇问题  
一个N×N的格子里，定义贪吃蛇每一步前左右随机行走，碰到墙或者自身则得到负反馈，吃到果子得到正反馈，在训练很多轮以后，贪吃蛇就学会了如何躲避墙和自身去吃果子。

## 4.机器如何学习

### 4.1.损失函数

还是用我们刚才那个线性回归的例子来说明：
 $$ y = kx+b $$
训练一个线性回归模型，实际上就是通过拟合来得到模型中的未知的参数k和b。拟合的过程在这里就是让MSE(均方误差)最小，MSE在这里被称为这个回归模型的**损失函数**，它代表了预测值与真实值的偏离程度。而我们机器学习的过程就是通过改变k和b使得**损失函数**取得**最小值**。
$$
L_{MSE}(\hat{y}, y)=\frac{1}{m} \sum_{i=1}^{m}\left(y_{i}-\hat{y}_{i}\right)^{2}
$$
这里的m表示数据个数。

除此之外，对于**二分类**问题来说，常用的损失函数是二元交叉熵损失(Logistic损失):

$$
L_{\text {logistic }}(\hat{y}, y)=\frac{1}{m} \sum_{i=1}^{m}[-y_{i} \log \hat{y}_{i}-\left(1-y_{i}\right) \log \left(1-\hat{y}_{i}\right)]
$$

这里的 $\hat{y}$ 代表预测y=1的概率。
### 4.2.梯度下降

我们知道了我们的目标是让k和b最小，那我们怎么实现呢？接下来我们来看一下它的解决方案——使用**梯度下降**算法来更新参数。

![梯度下降](/asset/gradient.jpeg)

这里 $\theta_0$ 和 $\theta_1$ 分别代表k和b。

我们从一个随机的k和b出发，沿着向下最快的路径行走，直到达到最低点，即此时k和b收敛于一个定值，这个定值就是我们想要得到的使得损失函数最小的值。
