{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-icarus/source/css/cyberpunk.styl","path":"css/cyberpunk.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/css/default.styl","path":"css/default.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/back_to_top.js","path":"js/back_to_top.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/animation.js","path":"js/animation.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/column.js","path":"js/column.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/avatar.png","path":"img/avatar.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/logo.jpg","path":"img/logo.jpg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/favicon.jpg","path":"img/favicon.jpg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/favicon.svg","path":"img/favicon.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/og_image.png","path":"img/og_image.png","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/logo.svg","path":"img/logo.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-bottom-black.svg","path":"img/razor-bottom-black.svg","modified":1,"renderable":1},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-top-black.svg","path":"img/razor-top-black.svg","modified":1,"renderable":1},{"_id":"source/asset/A2.png","path":"asset/A2.png","modified":1,"renderable":0},{"_id":"source/asset/AI.jpeg","path":"asset/AI.jpeg","modified":1,"renderable":0},{"_id":"source/asset/Big-Data.jpg","path":"asset/Big-Data.jpg","modified":1,"renderable":0},{"_id":"source/asset/ANN.jpg","path":"asset/ANN.jpg","modified":1,"renderable":0},{"_id":"source/asset/C3.png","path":"asset/C3.png","modified":1,"renderable":0},{"_id":"source/asset/E3.png","path":"asset/E3.png","modified":1,"renderable":0},{"_id":"source/asset/E4.png","path":"asset/E4.png","modified":1,"renderable":0},{"_id":"source/asset/CNN.jpg","path":"asset/CNN.jpg","modified":1,"renderable":0},{"_id":"source/asset/Long_tail.png","path":"asset/Long_tail.png","modified":1,"renderable":0},{"_id":"source/asset/C3box.png","path":"asset/C3box.png","modified":1,"renderable":0},{"_id":"source/asset/E2.png","path":"asset/E2.png","modified":1,"renderable":0},{"_id":"source/asset/book.jpg","path":"asset/book.jpg","modified":1,"renderable":0},{"_id":"source/asset/create-droplets.png","path":"asset/create-droplets.png","modified":1,"renderable":0},{"_id":"source/asset/create.jpg","path":"asset/create.jpg","modified":1,"renderable":0},{"_id":"source/asset/dl_data.jpg","path":"asset/dl_data.jpg","modified":1,"renderable":0},{"_id":"source/asset/deeplearning.jpg","path":"asset/deeplearning.jpg","modified":1,"renderable":0},{"_id":"source/asset/dltoml.jpg","path":"asset/dltoml.jpg","modified":1,"renderable":0},{"_id":"source/asset/deploy.png","path":"asset/deploy.png","modified":1,"renderable":0},{"_id":"source/asset/final.png","path":"asset/final.png","modified":1,"renderable":0},{"_id":"source/asset/juanji.jpg","path":"asset/juanji.jpg","modified":1,"renderable":0},{"_id":"source/asset/fit.jpg","path":"asset/fit.jpg","modified":1,"renderable":0},{"_id":"source/asset/hiding.jpg","path":"asset/hiding.jpg","modified":1,"renderable":0},{"_id":"source/asset/keras_book.jpg","path":"asset/keras_book.jpg","modified":1,"renderable":0},{"_id":"source/asset/gradient.jpeg","path":"asset/gradient.jpeg","modified":1,"renderable":0},{"_id":"source/asset/learn.gif","path":"asset/learn.gif","modified":1,"renderable":0},{"_id":"source/asset/lost3.jpg","path":"asset/lost3.jpg","modified":1,"renderable":0},{"_id":"source/asset/lost4.jpg","path":"asset/lost4.jpg","modified":1,"renderable":0},{"_id":"source/asset/missing_value.png","path":"asset/missing_value.png","modified":1,"renderable":0},{"_id":"source/asset/ldlx.jpg","path":"asset/ldlx.jpg","modified":1,"renderable":0},{"_id":"source/asset/lost2.jpg","path":"asset/lost2.jpg","modified":1,"renderable":0},{"_id":"source/asset/lost1.jpg","path":"asset/lost1.jpg","modified":1,"renderable":0},{"_id":"source/asset/prize.png","path":"asset/prize.png","modified":1,"renderable":0},{"_id":"source/asset/others.png","path":"asset/others.png","modified":1,"renderable":0},{"_id":"source/asset/pounding.jpg","path":"asset/pounding.jpg","modified":1,"renderable":0},{"_id":"source/asset/region.png","path":"asset/region.png","modified":1,"renderable":0},{"_id":"source/asset/sspyfa2011.doc","path":"asset/sspyfa2011.doc","modified":1,"renderable":0},{"_id":"source/asset/svm.jpg","path":"asset/svm.jpg","modified":1,"renderable":0},{"_id":"source/asset/top_img.jpg","path":"asset/top_img.jpg","modified":1,"renderable":0},{"_id":"source/asset/top_img.gif","path":"asset/top_img.gif","modified":1,"renderable":0},{"_id":"source/asset/train.png","path":"asset/train.png","modified":1,"renderable":0},{"_id":"source/asset/whatisml.png","path":"asset/whatisml.png","modified":1,"renderable":0},{"_id":"source/asset/write.gif","path":"asset/write.gif","modified":1,"renderable":0},{"_id":"source/asset/scikit.png","path":"asset/scikit.png","modified":1,"renderable":0},{"_id":"source/asset/typing.jpg","path":"asset/typing.jpg","modified":1,"renderable":0},{"_id":"source/asset/write.jpeg","path":"asset/write.jpeg","modified":1,"renderable":0}],"Cache":[{"_id":"source/_posts/alpha.md","hash":"963319b57df2c9c200102912d421c3656d1cbd80","modified":1619514675195},{"_id":"source/_posts/deep-learning.md","hash":"3118f21f5769d3084778eab188d5c58fb49b83a3","modified":1612194504980},{"_id":"source/_posts/blog-init.md","hash":"398a7d9f4c9e765de6d3070fef5649856f4ec7cb","modified":1612190468984},{"_id":"source/_posts/poem1.md","hash":"f808485bf431996dc3c88200f2281e71f8611d10","modified":1612194544944},{"_id":"source/_posts/value-investment.md","hash":"cbca52e71ba3aa025740f21a159cdcf2c15cf055","modified":1615800507514},{"_id":"source/_posts/ml-start.md","hash":"3d1f2858b004c28f9b5557d0769f28e37416a627","modified":1612194881569},{"_id":"source/_posts/poem2.md","hash":"726a3f1c7c1a900ae314c16565098786dddfe6f4","modified":1612194551330},{"_id":"source/_posts/summury-of-big-data-competition.md","hash":"ecc4be87e5dc7d30b900f458668a4f8ab51b8a6c","modified":1612192187061},{"_id":"source/_posts/poem3.md","hash":"32f678e5046dcccc713fa7f967f24f16878f24a5","modified":1612194555421},{"_id":"source/_posts/writing.md","hash":"d40ed36d652cf29c8278ca97e65ada6213240b80","modified":1612192190448},{"_id":"source/_posts/zhj-2.md","hash":"77f469f35c2c5ca9ed6f7c3ec7f2f8e57e6e53f9","modified":1612194818018},{"_id":"source/_posts/zhj-1.md","hash":"11acf5c3478a869694706c0f4f3c3722afc58069","modified":1612194802097},{"_id":"source/about/index.md","hash":"86bb1e11c2e70faeb1f57976844abb079c712123","modified":1578753292125},{"_id":"source/asset/A2.png","hash":"225a63348bb7ebb699c7b67b4ad8f4f8c61b11bf","modified":1572711458021},{"_id":"source/asset/ANN.jpg","hash":"dfaa0fff2936bdc1437ea3d91167cf89ed2e4f51","modified":1584783979517},{"_id":"source/asset/C3.png","hash":"27c8f1bce4a8af4ab926997e377bae3f282406ba","modified":1572711495098},{"_id":"source/asset/E3.png","hash":"830fab2060a32982e2a70c9d6df05c3cf6c82154","modified":1572711467454},{"_id":"source/asset/Long_tail.png","hash":"8638aa1b63964ee2ca4f6ce417245c8c0cbd6c1b","modified":1575811680967},{"_id":"source/asset/E4.png","hash":"245095586430d4e86c63b72a6a2ed4a2f6a5e800","modified":1572711481369},{"_id":"source/asset/CNN.jpg","hash":"720d41a95b297ef989556116f5394c119f83d4ec","modified":1584783667416},{"_id":"source/asset/C3box.png","hash":"b89c21edbf21753a904e2e5c29e736d5d2805194","modified":1573878016074},{"_id":"source/asset/E2.png","hash":"a8af6e917b11b636a8ee36588f874806d38840c3","modified":1572711448247},{"_id":"source/asset/deeplearning.jpg","hash":"22eaa592e9e460822022c57ee9bef77b8bd1a6d7","modified":1584713939177},{"_id":"source/asset/dltoml.jpg","hash":"a8c2783a5a3add8e7b309e568a19b95d7305cabf","modified":1584713205513},{"_id":"source/asset/final.png","hash":"f10fc0a574f4c8c73d4a859eaa8b2cdeb58fad5f","modified":1575996160735},{"_id":"source/asset/deploy.png","hash":"6a679ef8de2a84e455cd6eaed41305b890623c5b","modified":1576076270966},{"_id":"source/asset/juanji.jpg","hash":"bd819ecfd5a8b811a6fce73148cccbaef0bbe28d","modified":1584784246784},{"_id":"source/asset/hiding.jpg","hash":"a07a3a378f3ffa46d4cf54c76b2a370f2ba8c0da","modified":1578727329516},{"_id":"source/asset/fit.jpg","hash":"d295cefac397d98a495bacbf110a14c2ff4a2170","modified":1584863382447},{"_id":"source/asset/gradient.jpeg","hash":"f6a8fc4866e512eedf3e164e3833cc8d0dadffa3","modified":1576302557705},{"_id":"source/asset/learn.gif","hash":"130e460cda1fa9838aaac69463d61ed6e3e23303","modified":1584786371814},{"_id":"source/asset/missing_value.png","hash":"7c75463b5e26844b7f6baeffeb3def7e8237a733","modified":1575809723535},{"_id":"source/asset/ldlx.jpg","hash":"c1c8f17ec95afaf7f0fb4db92d71362e73dd00bd","modified":1579133161715},{"_id":"source/asset/region.png","hash":"c66cf8c404f69df780b6e4129ddb9c50900a482d","modified":1575995949184},{"_id":"source/asset/svm.jpg","hash":"a0125c1d46f7b5944e67f55466fd69780c958d0b","modified":1584781285273},{"_id":"source/asset/train.png","hash":"e80e97d57e9bebdef0aca378bac2e8b483774a2a","modified":1575809392691},{"_id":"source/draft/Investment notes.md","hash":"b40bc93a16a615847c7347ae1355dbea9119bba2","modified":1619509882075},{"_id":"source/categories/index.md","hash":"0c30ceacd3e9e7c484d4225838e769b68e78d3f2","modified":1578753214467},{"_id":"source/poems/index.md","hash":"58f5090500548923d1349fb32edef65080fce23d","modified":1576080850603},{"_id":"source/tags/index.md","hash":"d172e9d2eddb8af0082b4c92a85f1076c27143e0","modified":1578753197694},{"_id":"source/asset/write.jpeg","hash":"746bfcdc5fae22fa1b2281174f737b3d5aa47dab","modified":1578728719526},{"_id":"source/asset/create-droplets.png","hash":"8e9e0eb85a7ba09d828cc957dbc58c493d1ad933","modified":1575995549588},{"_id":"source/asset/create.jpg","hash":"0a8f5fd938c8fe009f5c98f86b8b10ce05980c13","modified":1578728517102},{"_id":"source/asset/book.jpg","hash":"1e2d9b27afb532843b6976c7d308dafd9472479e","modified":1584873959334},{"_id":"source/asset/dl_data.jpg","hash":"6816e3853b41f938a04caae30867b806abe41194","modified":1584714126516},{"_id":"source/asset/lost4.jpg","hash":"ee603107d2fd383976a6a68b5eec68984f02d81a","modified":1578726186117},{"_id":"source/asset/lost3.jpg","hash":"b5fab9bc6bb1620278edfceaf441e83948d2ed2b","modified":1578726168308},{"_id":"source/asset/prize.png","hash":"933cce62167f27f1fab4cfc93a2c29efb020033c","modified":1575995884695},{"_id":"source/asset/others.png","hash":"af36dceacefcbecb8a9aea4d30146054f13287aa","modified":1575996076972},{"_id":"source/asset/typing.jpg","hash":"064442a36bda4b8c6fbdd41d3ef6e77fbacea165","modified":1578728744517},{"_id":"source/asset/pounding.jpg","hash":"a05407303a6836527f55b803210338e8da35f6d6","modified":1578727111533},{"_id":"source/asset/AI.jpeg","hash":"396e52d69521647304642eaf568a55881d54e1f2","modified":1578380833516},{"_id":"source/asset/lost1.jpg","hash":"8f7a9a99c6ed8a592fb88ccb1504e052a9e637a8","modified":1578725899309},{"_id":"source/asset/sspyfa2011.doc","hash":"cb98b7e34acafabfec7d209e59e20a18e19d007a","modified":1580209814973},{"_id":"node_modules/hexo-theme-icarus/layout/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/donate/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/comment/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/misc/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/search/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/share/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/package.json","hash":"cb6ade5502a5a2a45901677950f6ef23b068c271","modified":1612185621330},{"_id":"node_modules/hexo-theme-icarus/include/config.js","hash":"1ff0f174e9670074ad2bee890d5b6da486800c9a","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/dependency.js","hash":"d30dbcefd58619f6705d6369b644bc7ba44d2421","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/register.js","hash":"a974b56a1fbb254f1ae048cc2221363faaccec25","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/scripts/index.js","hash":"0c666db6fcb4ffc4d300f4e108c00ee42b1cbbe6","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/CONTRIBUTING.md","hash":"70254c6778c1e41bb2ff222bbf3a70b2239b9bc1","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/LICENSE","hash":"86037e5335a49321fa73b7815cab542057fac944","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/README.md","hash":"247ec8047ee3105d31099dd8e6ca498b6bff1336","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/de.yml","hash":"01d9c27c3c9224d8c58b1cf7099ef008b9411a45","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/en.yml","hash":"1bdb74288808c4d306b46630860e586a1fcd88a7","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/es.yml","hash":"48fe3d7d304239b5e5e93f63600093700d6f0fed","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/id.yml","hash":"c04fca89e536d539a8bf95980bff7dff79125ba3","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/fr.yml","hash":"14765cc6216b4af5a05341315c9f6ee54d636a78","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/ja.yml","hash":"7568f246a21813527ccea8a1da72a9526aa2d233","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/pl.yml","hash":"612639b0e0a15185c12eecfe3e8913b629ecdfdf","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/ko.yml","hash":"f2211a8ca1d73f05a1c931aa11accb4e34c483c4","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/tr.yml","hash":"96d94f7a5d56b2682ae2792f0808139d4b950704","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/pt-BR.yml","hash":"531b22c71fab8aae60ddc7599aaa5f46140cf2c1","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/ru.yml","hash":"bf784c6f31e8fb7ed78509468bddecd447c3b73b","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/vn.yml","hash":"64307bfa16d851334e2f37a29a84d644624c2060","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/archive.jsx","hash":"05677e93d4a43f417dbbf0d63ca37a99e6349e3b","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/zh-TW.yml","hash":"92f8cf599ad06bb14f79f4806ac4c695f60044d7","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/category.jsx","hash":"fd15e4eac32de9ac8687aeb3dbe179ab61375700","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/languages/zh-CN.yml","hash":"82ff607b1671b88d259d10b6ce8ca1bb05b2cff4","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/page.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/categories.jsx","hash":"b8ad43e28a4990d222bfbb95b032f88555492347","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/index.jsx","hash":"0a84a2348394fa9fc5080dd396bd28d357594f47","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/layout.jsx","hash":"a5829907b219e95266f7ed5ee6203e60e2273f93","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/post.jsx","hash":"d26c2db57e5a88d6483a03aeb51cda9d191d8cea","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/migration/head.js","hash":"269ba172013cbd2f10b9bc51af0496628081329b","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/tags.jsx","hash":"2c42cb64778235dd220c563a27a92108ddc50cc4","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/migration/v3_v4.js","hash":"9faf2184d7fe87debfbe007f3fc9079dcbcafcfe","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/config.json","hash":"ac633f9d349bca4f089d59d2c3738b57376f1b31","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/util/console.js","hash":"59cf9d277d3ac85a496689bd811b1c316001641d","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/card.styl","hash":"f78674422eb408cd17c17bbdc3ee1ebe4a453e05","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/tag.jsx","hash":"d2f18cac32ca2725d34ccff3f2051c623be6c892","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/migration/v2_v3.js","hash":"3ccb2d2ce11018bebd7172da66faecc3983bff00","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/button.styl","hash":"0fb35b4786be1b387c751fa2849bc71523fcedd4","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/article.styl","hash":"be25e890113e926bbac3bf1461d7ce5427914774","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/footer.styl","hash":"a4ad715dee38b249538ac6cce94efc9b355a904b","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/base.styl","hash":"2bca6ad099949d52236c87db8db1002ffb99774c","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/codeblock.styl","hash":"30bee4cf6792e9665eb648cc20b352d9eaff1207","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/donate.styl","hash":"8d0af00628c13134b5f30a558608e7bebf18c2ec","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/helper.styl","hash":"9f3393e6122cc9f351091bfab960674e962da343","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/navbar.styl","hash":"ecc73c8ad504c0fa4bb910eb51500c14e0a8d662","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/widget.styl","hash":"af07ee43e209b6361eed22171bb50efaef7cbfeb","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/pagination.styl","hash":"b81bcd7ff915b4e9299533addc01bc4575ec35e3","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/timeline.styl","hash":"ea61798a09bffdda07efb93c2ff800b63bddc4c4","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/article.jsx","hash":"f31c5a78fc4b2d72acf4583342be9e72ac9adf21","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/comment.jsx","hash":"427089c33002707b76e2f38709459a6824fd0f9b","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/plugin.styl","hash":"dc98160142c95ef81ba4789351002a5fcf29c04c","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/style/responsive.styl","hash":"207083fe287612cddee6608b541861b14ac8de81","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/footer.jsx","hash":"e060f18cf0e4f942b8835e255652ca95b6dee8f2","modified":1612798230265},{"_id":"node_modules/hexo-theme-icarus/include/style/search.styl","hash":"416737e1da4e7e907bd03609b0fee9e2aacfe56c","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/head.jsx","hash":"bcee2e258d13af6ac439ee6adaeefc06ea384e7e","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/navbar.jsx","hash":"fcd9fd4624dee49207ef09ea2a1c63f524f3710c","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/donates.jsx","hash":"889fb0a7ccc502f0a43b4a18eb330e351e50493c","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/plugins.jsx","hash":"f6826c1a5f5f59f4a0aa00c63bdb0ad4ff4eab69","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/search.jsx","hash":"6f244a37293031670a2964fe424ecd062e591d7b","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/scripts.jsx","hash":"a6b7a2891bbc7c71dcc2a0e756e9c9847bbb6729","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/share.jsx","hash":"c9fb0319ad5e5a10ad3636b26a6c2afed14c590f","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/css/default.styl","hash":"b01da3028e5a1267a40aaae5c86a11187a2259e3","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/common/widgets.jsx","hash":"689cf4a6b79337b11d1d56afa9dda09223a809a1","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/plugin/back_to_top.jsx","hash":"7fc0c5aaabd7d0eaff04cb68ec139442dc3414e8","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/css/cyberpunk.styl","hash":"ae17d3528df0c3f089df14a06b7bd82f1bc5fed9","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/css/style.styl","hash":"5b9815586e993a6ccbe8cdcfc0c65ea38fc315ac","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/plugin/animejs.jsx","hash":"e2aa27c3501a58ef1e91e511557b77395c2c02aa","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/js/column.js","hash":"0baee024ab67474c073a4c41b495f3e7f0df4505","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/js/back_to_top.js","hash":"d91f10c08c726135a13dfa1f422c49d8764ef03f","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/js/animation.js","hash":"12cedd5caaf9109eed97e50eeab8f883f6e49be3","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/layout/widget/profile.jsx","hash":"6fb534c9e9ba8c22449d11fc68b891c343a3a742","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/img/avatar.png","hash":"0d8236dcca871735500e9d06bbdbe0853ed6775b","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/img/favicon.svg","hash":"16fd847265845063a16596761cddb32926073dd2","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/img/logo.jpg","hash":"ad9b1e0425523f0026135d0cf410e8a5c4bd6163","modified":1612191987130},{"_id":"node_modules/hexo-theme-icarus/source/img/favicon.jpg","hash":"93c3b583385f0d53e309825ddd627f7115ed1c58","modified":1612191238107},{"_id":"node_modules/hexo-theme-icarus/source/js/main.js","hash":"13e4b1c4fa287f3db61aae329ad093a81992f23d","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/img/logo.svg","hash":"e9b5c1438ddb576693a15d0713b2a1d9ceda4be9","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/img/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/article.json","hash":"8d78149f44629d0848921c6fb9c008b03cef3116","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/comment.json","hash":"bd30bd9ffc84e88e704384acdfcaab09019a744f","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/donates.json","hash":"ae86e6f177bedf4afbe638502c12635027539305","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/head.json","hash":"98889f059c635e6bdbd51effd04cf1cf44968a66","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-top-black.svg","hash":"201f1171a43ce667a39091fe47c0f278857f18f0","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/footer.json","hash":"09d706cbb94d6da9a0d15c719ce7139325cae1c7","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/source/img/razor-bottom-black.svg","hash":"a3eda07b1c605b456da9cdf335a1075db5e5d72c","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/navbar.json","hash":"6691e587284c4cf450e0288680d5ff0f3565f090","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/sidebar.json","hash":"eb241beaec4c73e3085dfb3139ce72e827e20549","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/providers.json","hash":"97ec953d497fb53594227ae98acaef8a8baa91da","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/plugins.json","hash":"6036a805749816416850d944f7d64aaae62e5e75","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/search.json","hash":"985fbcbf47054af714ead1a124869d54f2a8b607","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/share.json","hash":"cf4f9ff4fb27c3541b35f57db355c228fa6873e4","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/common/widgets.json","hash":"d000b4d1d09bdd64756265aa4cd2ea980ab7ddc7","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/plugin/back_to_top.json","hash":"dc0febab7e7b67075d0ad3f80f5ec8b798b68dea","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/plugin/animejs.json","hash":"e62ab6e20bd8862efa1ed32e7c0db0f8acbcfdec","modified":499162500000},{"_id":"node_modules/hexo-theme-icarus/include/schema/widget/profile.json","hash":"690ee1b0791cab47ea03cf42b5b4932ed2aa5675","modified":499162500000},{"_id":"source/asset/scikit.png","hash":"d6e68b2fbcbe02253dda16464601718a0a6d3b87","modified":1578329301320},{"_id":"source/asset/keras_book.jpg","hash":"0255bfc3134981b83d454ca284e6df729516b2ab","modified":1584874005520},{"_id":"source/asset/whatisml.png","hash":"1139b9d2d77c06c2ddcf892b44497bbdda365cf1","modified":1578325652300},{"_id":"source/asset/Big-Data.jpg","hash":"87259869bd26747004908e4706b414fb2a124db8","modified":1578728966313},{"_id":"source/asset/lost2.jpg","hash":"a49f03f5d35c5170de106aa40a925a47ab55a355","modified":1578726155945},{"_id":"source/asset/top_img.gif","hash":"91bd2a2a9819056950b653b5a014c471e087d59f","modified":1578377066310},{"_id":"source/asset/write.gif","hash":"6e4af2c70241150b91a126e8690a030b51140447","modified":1578729018483},{"_id":"source/asset/top_img.jpg","hash":"83de4fe019ab6429f219be11397adf9d82538e80","modified":1578377185584},{"_id":"public/js/algolia.js","hash":"a8df0c0abeeb4ee1d2d720161f3aea7339380704","modified":1621322883489},{"_id":"public/js/google_cse.js","hash":"1a9881669dfdeb2b3214074eee0d3e01e52db2c4","modified":1621322883489},{"_id":"public/content.json","hash":"31b90c5c1cb392a5ddc08953c947cf07143c7fb5","modified":1621322883489},{"_id":"public/js/insight.js","hash":"86bbdb7305d9bf19ad62d2ca2cf169fc8d9f9d31","modified":1621322883489},{"_id":"public/manifest.json","hash":"fe210f695a826b55a9ad3b442a74245325aeb7a0","modified":1621322883489},{"_id":"public/js/toc.js","hash":"da6fb757a1b083b8ed138bf29aad3a7bf8ec4f11","modified":1621322883489},{"_id":"public/draft/Investment notes.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1621322883489},{"_id":"public/categories/index.html","hash":"9bdaf0ccc5ffa2242fa4a9fc7fefb6fde25a7e94","modified":1621322883489},{"_id":"public/about/index.html","hash":"63922eae09a571c60aca0b514fc62ef95bf66f1f","modified":1621322883489},{"_id":"public/poems/index.html","hash":"44dbbdfe8f22e452186de74919704c67c1d177a3","modified":1621322883489},{"_id":"public/tags/index.html","hash":"1e80ef6907e0adefd584ddd34d167ef1e8073d46","modified":1621322883489},{"_id":"public/2021/04/27/alpha/index.html","hash":"a078c70e0e3f4b38fed051380229485b1fefe03e","modified":1621322883489},{"_id":"public/2021/03/15/value-investment/index.html","hash":"b48e40620abe8456e548079a6ba8ba1d0431a133","modified":1621322883489},{"_id":"public/2020/03/20/deep-learning/index.html","hash":"038726e148a9e5f2c40ad0dbd89781ab85c55264","modified":1621322883489},{"_id":"public/2020/01/16/zhj-2/index.html","hash":"d473ad3e50ba0292177cf2ff92d310c0f2eb96b9","modified":1621322883489},{"_id":"public/2019/12/17/poem3/index.html","hash":"06ac7a36f0f68aee88569d16f14f1b530793ff8e","modified":1621322883489},{"_id":"public/2020/01/16/zhj-1/index.html","hash":"4b43118a56f4a8db5aa939a297c1340b7dd3ad77","modified":1621322883489},{"_id":"public/2019/12/14/ml-start/index.html","hash":"099a2d61281a4fa71a29e13bb2a5706f517744fd","modified":1621322883489},{"_id":"public/2019/12/14/poem2/index.html","hash":"d1b61c43a73c7f2d2baf9dfe940455d841e624b1","modified":1621322883489},{"_id":"public/2019/12/12/poem1/index.html","hash":"c01afe32fd5bea2e05ab85b68a231119f207e86a","modified":1621322883489},{"_id":"public/2019/12/10/blog-init/index.html","hash":"d6770affcdc5ca4354041ce916b55513f0462197","modified":1621322883489},{"_id":"public/2019/12/11/writing/index.html","hash":"6a3566e00f960fcdf786d3e7549a81d974a95c89","modified":1621322883489},{"_id":"public/2019/12/09/summury-of-big-data-competition/index.html","hash":"138fb221f0b07e901be169276b32a823e0f48574","modified":1621322883489},{"_id":"public/archives/index.html","hash":"3f85a6d7626c710179bd73bbaadf3b2657eb5ae8","modified":1621322883489},{"_id":"public/archives/page/2/index.html","hash":"6010341a35fafd3bc1f5fa06d3141f39f5f46097","modified":1621322883489},{"_id":"public/archives/2019/12/index.html","hash":"6af6a2a394e3240ae7a09dd3405ec3033d7a861c","modified":1621322883489},{"_id":"public/archives/2019/index.html","hash":"3ac427ab17abbd6f32d7fc5550979a45994f00c0","modified":1621322883489},{"_id":"public/archives/2020/index.html","hash":"55ee99ccb6ea95029dc4cf6149861d86e6c4cc6a","modified":1621322883489},{"_id":"public/archives/2020/01/index.html","hash":"0209abc8e8a313e6d5963c949d973175fb79ddff","modified":1621322883489},{"_id":"public/archives/2020/03/index.html","hash":"fe6581193d8a1b4049455a45dbd8f2f24d26e77c","modified":1621322883489},{"_id":"public/archives/2021/03/index.html","hash":"6fb92a3c9aed2f35297b494706d6914f33897a24","modified":1621322883489},{"_id":"public/archives/2021/index.html","hash":"413e5b83503adbe2d709768fb8a19fb4e64d8b9e","modified":1621322883489},{"_id":"public/archives/2021/04/index.html","hash":"3ff67d8dfe3656d59828a8cc203b2c56a513fd0b","modified":1621322883489},{"_id":"public/categories/投资/index.html","hash":"3934f5a84237325992b9233410656bb2df9db45a","modified":1621322883489},{"_id":"public/categories/机器学习/index.html","hash":"b9c7ef08d2ebb485d1038da644df15283b1029fc","modified":1621322883489},{"_id":"public/categories/诗集/index.html","hash":"a1b3abb2ace6a7866cd02ac2491f6719ad1290e6","modified":1621322883489},{"_id":"public/index.html","hash":"9ce8e1e0167bb5e0c8348fc88be0e3ad20bad0bb","modified":1621322883489},{"_id":"public/categories/博客/index.html","hash":"752a8f3798a27fcec62feafc504e2faa4c819974","modified":1621322883489},{"_id":"public/categories/物理/index.html","hash":"81584083bdfc88aac75f0e5004de4211334e4720","modified":1621322883489},{"_id":"public/page/2/index.html","hash":"f79f05596c5835ef4c101fcea4fd2fd136288e28","modified":1621322883489},{"_id":"public/tags/投资/index.html","hash":"9bb335000ccd315a410e75f5edfe2c15776cec93","modified":1621322883489},{"_id":"public/tags/机器学习/index.html","hash":"b0a1cf7f8304d5a876b2d346b4676e6bca522d25","modified":1621322883489},{"_id":"public/tags/VPS/index.html","hash":"6e189a06cf3a7dcade638853e6429a269bdb4b46","modified":1621322883489},{"_id":"public/tags/hexo/index.html","hash":"324e5a8856a04e4ce1fd6b6411bd2746bd38bb60","modified":1621322883489},{"_id":"public/tags/hginx/index.html","hash":"82531b0761e802c5812f7b88cd027de828258cd0","modified":1621322883489},{"_id":"public/tags/网站/index.html","hash":"39bb192321a8664cf3ccb483a1876d343cb19ef5","modified":1621322883489},{"_id":"public/tags/poem/index.html","hash":"4f267e63b794ef807499b8841b648dda365e66c1","modified":1621322883489},{"_id":"public/tags/bigdata/index.html","hash":"b8905dfaef9db57cdd97ebabd21e3565b20796cd","modified":1621322883489},{"_id":"public/tags/总结/index.html","hash":"5e66fd9c5d0ecc96374c1acdaba3846a39c91957","modified":1621322883489},{"_id":"public/tags/markdown/index.html","hash":"1442b8f0ba215d46b88dc25773165e620c844410","modified":1621322883489},{"_id":"public/tags/写作/index.html","hash":"01d0b3c0949bdf1da53b85b2ff9e182ff721043e","modified":1621322883489},{"_id":"public/tags/读后感/index.html","hash":"323b744cda68e6bf8280a5d577bfeb1827cab2af","modified":1621322883489},{"_id":"public/img/avatar.png","hash":"0d8236dcca871735500e9d06bbdbe0853ed6775b","modified":1621322883489},{"_id":"public/img/logo.jpg","hash":"ad9b1e0425523f0026135d0cf410e8a5c4bd6163","modified":1621322883489},{"_id":"public/img/favicon.jpg","hash":"93c3b583385f0d53e309825ddd627f7115ed1c58","modified":1621322883489},{"_id":"public/img/favicon.svg","hash":"16fd847265845063a16596761cddb32926073dd2","modified":1621322883489},{"_id":"public/img/razor-bottom-black.svg","hash":"a3eda07b1c605b456da9cdf335a1075db5e5d72c","modified":1621322883489},{"_id":"public/img/og_image.png","hash":"b03f163096ca9c350ec962feee9836277b5c2509","modified":1621322883489},{"_id":"public/img/razor-top-black.svg","hash":"201f1171a43ce667a39091fe47c0f278857f18f0","modified":1621322883489},{"_id":"public/img/logo.svg","hash":"e9b5c1438ddb576693a15d0713b2a1d9ceda4be9","modified":1621322883489},{"_id":"public/asset/A2.png","hash":"225a63348bb7ebb699c7b67b4ad8f4f8c61b11bf","modified":1621322883489},{"_id":"public/asset/ANN.jpg","hash":"dfaa0fff2936bdc1437ea3d91167cf89ed2e4f51","modified":1621322883489},{"_id":"public/asset/C3.png","hash":"27c8f1bce4a8af4ab926997e377bae3f282406ba","modified":1621322883489},{"_id":"public/asset/E3.png","hash":"830fab2060a32982e2a70c9d6df05c3cf6c82154","modified":1621322883489},{"_id":"public/asset/E4.png","hash":"245095586430d4e86c63b72a6a2ed4a2f6a5e800","modified":1621322883489},{"_id":"public/asset/CNN.jpg","hash":"720d41a95b297ef989556116f5394c119f83d4ec","modified":1621322883489},{"_id":"public/asset/Long_tail.png","hash":"8638aa1b63964ee2ca4f6ce417245c8c0cbd6c1b","modified":1621322883489},{"_id":"public/asset/C3box.png","hash":"b89c21edbf21753a904e2e5c29e736d5d2805194","modified":1621322883489},{"_id":"public/asset/E2.png","hash":"a8af6e917b11b636a8ee36588f874806d38840c3","modified":1621322883489},{"_id":"public/asset/deeplearning.jpg","hash":"22eaa592e9e460822022c57ee9bef77b8bd1a6d7","modified":1621322883489},{"_id":"public/asset/dltoml.jpg","hash":"a8c2783a5a3add8e7b309e568a19b95d7305cabf","modified":1621322883489},{"_id":"public/asset/deploy.png","hash":"6a679ef8de2a84e455cd6eaed41305b890623c5b","modified":1621322883489},{"_id":"public/asset/final.png","hash":"f10fc0a574f4c8c73d4a859eaa8b2cdeb58fad5f","modified":1621322883489},{"_id":"public/asset/juanji.jpg","hash":"bd819ecfd5a8b811a6fce73148cccbaef0bbe28d","modified":1621322883489},{"_id":"public/asset/fit.jpg","hash":"d295cefac397d98a495bacbf110a14c2ff4a2170","modified":1621322883489},{"_id":"public/asset/hiding.jpg","hash":"a07a3a378f3ffa46d4cf54c76b2a370f2ba8c0da","modified":1621322883489},{"_id":"public/asset/gradient.jpeg","hash":"f6a8fc4866e512eedf3e164e3833cc8d0dadffa3","modified":1621322883489},{"_id":"public/asset/learn.gif","hash":"130e460cda1fa9838aaac69463d61ed6e3e23303","modified":1621322883489},{"_id":"public/asset/missing_value.png","hash":"7c75463b5e26844b7f6baeffeb3def7e8237a733","modified":1621322883489},{"_id":"public/asset/ldlx.jpg","hash":"c1c8f17ec95afaf7f0fb4db92d71362e73dd00bd","modified":1621322883489},{"_id":"public/asset/region.png","hash":"c66cf8c404f69df780b6e4129ddb9c50900a482d","modified":1621322883489},{"_id":"public/asset/svm.jpg","hash":"a0125c1d46f7b5944e67f55466fd69780c958d0b","modified":1621322883489},{"_id":"public/asset/train.png","hash":"e80e97d57e9bebdef0aca378bac2e8b483774a2a","modified":1621322883489},{"_id":"public/asset/write.jpeg","hash":"746bfcdc5fae22fa1b2281174f737b3d5aa47dab","modified":1621322883489},{"_id":"public/asset/create-droplets.png","hash":"8e9e0eb85a7ba09d828cc957dbc58c493d1ad933","modified":1621322883489},{"_id":"public/asset/book.jpg","hash":"1e2d9b27afb532843b6976c7d308dafd9472479e","modified":1621322883489},{"_id":"public/asset/dl_data.jpg","hash":"6816e3853b41f938a04caae30867b806abe41194","modified":1621322883489},{"_id":"public/asset/create.jpg","hash":"0a8f5fd938c8fe009f5c98f86b8b10ce05980c13","modified":1621322883489},{"_id":"public/asset/lost3.jpg","hash":"b5fab9bc6bb1620278edfceaf441e83948d2ed2b","modified":1621322883489},{"_id":"public/asset/lost4.jpg","hash":"ee603107d2fd383976a6a68b5eec68984f02d81a","modified":1621322883489},{"_id":"public/asset/prize.png","hash":"933cce62167f27f1fab4cfc93a2c29efb020033c","modified":1621322883489},{"_id":"public/asset/others.png","hash":"af36dceacefcbecb8a9aea4d30146054f13287aa","modified":1621322883489},{"_id":"public/asset/typing.jpg","hash":"064442a36bda4b8c6fbdd41d3ef6e77fbacea165","modified":1621322883489},{"_id":"public/asset/pounding.jpg","hash":"a05407303a6836527f55b803210338e8da35f6d6","modified":1621322883489},{"_id":"public/js/back_to_top.js","hash":"d91f10c08c726135a13dfa1f422c49d8764ef03f","modified":1621322883489},{"_id":"public/js/animation.js","hash":"12cedd5caaf9109eed97e50eeab8f883f6e49be3","modified":1621322883489},{"_id":"public/js/column.js","hash":"0baee024ab67474c073a4c41b495f3e7f0df4505","modified":1621322883489},{"_id":"public/js/main.js","hash":"13e4b1c4fa287f3db61aae329ad093a81992f23d","modified":1621322883489},{"_id":"public/css/cyberpunk.css","hash":"0d8dc4bf137bd7ccd0125f1bb6af45c1e51d7489","modified":1621322883489},{"_id":"public/css/default.css","hash":"f36bff9e79cb531e5ac60c5242c449644d3c7750","modified":1621322883489},{"_id":"public/css/style.css","hash":"f36bff9e79cb531e5ac60c5242c449644d3c7750","modified":1621322883489},{"_id":"public/asset/AI.jpeg","hash":"396e52d69521647304642eaf568a55881d54e1f2","modified":1621322883489},{"_id":"public/asset/lost1.jpg","hash":"8f7a9a99c6ed8a592fb88ccb1504e052a9e637a8","modified":1621322883489},{"_id":"public/asset/keras_book.jpg","hash":"0255bfc3134981b83d454ca284e6df729516b2ab","modified":1621322883489},{"_id":"public/asset/sspyfa2011.doc","hash":"cb98b7e34acafabfec7d209e59e20a18e19d007a","modified":1621322883489},{"_id":"public/asset/scikit.png","hash":"d6e68b2fbcbe02253dda16464601718a0a6d3b87","modified":1621322883489},{"_id":"public/asset/Big-Data.jpg","hash":"87259869bd26747004908e4706b414fb2a124db8","modified":1621322883489},{"_id":"public/asset/whatisml.png","hash":"1139b9d2d77c06c2ddcf892b44497bbdda365cf1","modified":1621322883489},{"_id":"public/asset/top_img.gif","hash":"91bd2a2a9819056950b653b5a014c471e087d59f","modified":1621322883489},{"_id":"public/asset/lost2.jpg","hash":"a49f03f5d35c5170de106aa40a925a47ab55a355","modified":1621322883489},{"_id":"public/asset/write.gif","hash":"6e4af2c70241150b91a126e8690a030b51140447","modified":1621322883489},{"_id":"public/asset/top_img.jpg","hash":"83de4fe019ab6429f219be11397adf9d82538e80","modified":1621322883489}],"Category":[{"name":"投资","_id":"ckotps9ed0004lsvw5q9443hm"},{"name":"机器学习","_id":"ckotps9ej000clsvwejuc915c"},{"name":"博客","_id":"ckotps9em000ilsvwa8ob7fgj"},{"name":"诗集","_id":"ckotps9et000slsvwa5hcb8ng"},{"name":"物理","_id":"ckotps9f2001mlsvw5e7o64rp"}],"Data":[],"Page":[{"title":"about","date":"2020-01-11T14:24:30.000Z","_content":"待完工..","source":"about/index.md","raw":"---\ntitle: about\ndate: 2020-01-11 22:24:30\n---\n待完工..","updated":"2020-01-11T14:34:52.125Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckotps9e40000lsvwbdnj2mcp","content":"<p>待完工..</p>\n","site":{"data":{}},"excerpt":"","more":"<p>待完工..</p>\n"},{"title":"投资随记","date":"2021-03-22T13:42:05.000Z","tags":"投资","categories":"投资","_content":"\n本文以日记的形式，记录我人在股市、学习投资的一些感想。\n\n**人在股市，慢就是快。**\n\n<!--more-->\n\n## 2021/03/22\n\n- 安全边际\n\n经过市场最近一个月的深度回调，让我深刻认识到了买入时安全边际的重要性。诚然，有着机构和媒体的推波助澜，找到一支股价已经在高位的优质股非常容易。但显然此时的买入是不具有安全边际的，虽然追高买入也还可能会有持续不断收益(比如2020年的白酒、新能源)，但谁也无法准确预测到它能涨到多高，但一旦遭遇市场风格的变化，回调往往是突然地、剧烈的、没有反弹的。那么如果你的成本没有安全边际的话，你大概率是从浮盈迅速变为浮亏的，此时割又不舍得割，不割又一路下跌，价格见底却又因为都被套住而没钱补仓的尴尬局面。\n\n因此，投资思路应该是管住手，如果没在行情刚刚启动的时候进入，那么不管涨的多疯，都不要去博傻，等优质股被错杀到一个合理的位置时，再逐步建仓，分批买入。\n\n- 每次买入都要有充足的理由\n\n证券分析员或许有着极高的说服能力，看了他的分析，似乎就觉得某家公司大有可为、未来可期。然而，如果让你经过思考后逐条列出在当前价位买入某某公司的原因时，或许你又会感觉写出的原因不是那么可靠。\n\n- 今日优质股记录\n\n贵州茅台：\n股王，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率16.47%。\n价格1989.99元，动态PE为55.42，PE百分位92.44%，极度高估。\n\n海天味业：\n调味品龙头，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率27.21%。\n价格151.66元，动态PE为80.63，PE百分位90.78%，极度高估。\n\n- 当前持股\n\n长电科技：\n半导体封装测试龙头，受益于半导体国产替代大趋势，叠加芯片需求大增，产能满载，业绩预增。\n成本40.20，当前价格34.08，浮亏15.23%。\n市销率为2.09，百分位66.84%，估值偏高。\n\n## 2021/03/25\n\n- 分散投资\n\n不要把鸡蛋放在一个篮子里，也最好不要把篮子放在一个桌子上，不然一波股灾桌子塌了。分散投资有很多好处，其一是在基本不影响收益的情况下减小收益曲线的波动，其二是增加投资组合的抗风险能力，避免由于踩雷财务造假导致整个账户崩盘。\n\n- 耐心\n\n在市场热度高时，好公司的估值都很高。很难找到一个可以接受的安全边际买入建仓，此时需要的就是耐心，从极度高估等到高估，从高估等到合理估值，可能能等到低估，也可能等不到低估。由于我们无法预测股票的短期走势，在公司基本面不变的情况下，我们可以选择分批进场的方式，在估值跌至50分位以下时考虑开始建仓，跌至40分位加仓，跌至30分位再加仓，跌至20分位时重仓，删app。\n\n## 2021/4/27\n\n简单复盘一下近两天的窒息操作。耐不住长线价投的我，自以为发现了很好的短线机会，进场被吊锤。\n\n一周前传出华为自动驾驶的路试视频，北汽蓝谷应声3连板，随后就是三天的回调，以为自动驾驶会是一波像医美和碳中和的大行情，在26日水下低吸北汽蓝谷，当日最高涨幅7个点，但收盘只是微红。今天低开后就不断下杀，我在亏8个点时破位清仓。随后同板块的常熟气饰走出一波反包行情，封板前的一瞬间成功上车。但无奈很快被砸开，当日共亏13个点。\n\n不得不说，我这两天的这种短线行为简直就是赌博一般的行为，虽然刺激，但无疑是在浪费时间和钱财，还好大部分仓位都在长电，短线只是小仓位玩一玩，不然就欲哭无泪了。\n\n- 短线中的Alpha和Beta\n>y= Alpha+x*Beta\n>x: 代表市场收益 \n>Beta: 反映的是弹性，是投资组合对市场的敏感度\n>Alpha: 由投资者自身水平导致的超额收益\n\n这是我经常思考的一个问题，短线操作到底存不存在Alpha。目前我的思考是存在，但发现Alpha的人会将其保密。若不存在，股市上不可能出现8年10万倍的传说。而将其公开出去又会导致其因其广为人知的属性而变成Beta。","source":"draft/Investment notes.md","raw":"---\ntitle: 投资随记\ndate: 2021-03-22 21:42:05\ntags: 投资\ncategories: 投资\n---\n\n本文以日记的形式，记录我人在股市、学习投资的一些感想。\n\n**人在股市，慢就是快。**\n\n<!--more-->\n\n## 2021/03/22\n\n- 安全边际\n\n经过市场最近一个月的深度回调，让我深刻认识到了买入时安全边际的重要性。诚然，有着机构和媒体的推波助澜，找到一支股价已经在高位的优质股非常容易。但显然此时的买入是不具有安全边际的，虽然追高买入也还可能会有持续不断收益(比如2020年的白酒、新能源)，但谁也无法准确预测到它能涨到多高，但一旦遭遇市场风格的变化，回调往往是突然地、剧烈的、没有反弹的。那么如果你的成本没有安全边际的话，你大概率是从浮盈迅速变为浮亏的，此时割又不舍得割，不割又一路下跌，价格见底却又因为都被套住而没钱补仓的尴尬局面。\n\n因此，投资思路应该是管住手，如果没在行情刚刚启动的时候进入，那么不管涨的多疯，都不要去博傻，等优质股被错杀到一个合理的位置时，再逐步建仓，分批买入。\n\n- 每次买入都要有充足的理由\n\n证券分析员或许有着极高的说服能力，看了他的分析，似乎就觉得某家公司大有可为、未来可期。然而，如果让你经过思考后逐条列出在当前价位买入某某公司的原因时，或许你又会感觉写出的原因不是那么可靠。\n\n- 今日优质股记录\n\n贵州茅台：\n股王，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率16.47%。\n价格1989.99元，动态PE为55.42，PE百分位92.44%，极度高估。\n\n海天味业：\n调味品龙头，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率27.21%。\n价格151.66元，动态PE为80.63，PE百分位90.78%，极度高估。\n\n- 当前持股\n\n长电科技：\n半导体封装测试龙头，受益于半导体国产替代大趋势，叠加芯片需求大增，产能满载，业绩预增。\n成本40.20，当前价格34.08，浮亏15.23%。\n市销率为2.09，百分位66.84%，估值偏高。\n\n## 2021/03/25\n\n- 分散投资\n\n不要把鸡蛋放在一个篮子里，也最好不要把篮子放在一个桌子上，不然一波股灾桌子塌了。分散投资有很多好处，其一是在基本不影响收益的情况下减小收益曲线的波动，其二是增加投资组合的抗风险能力，避免由于踩雷财务造假导致整个账户崩盘。\n\n- 耐心\n\n在市场热度高时，好公司的估值都很高。很难找到一个可以接受的安全边际买入建仓，此时需要的就是耐心，从极度高估等到高估，从高估等到合理估值，可能能等到低估，也可能等不到低估。由于我们无法预测股票的短期走势，在公司基本面不变的情况下，我们可以选择分批进场的方式，在估值跌至50分位以下时考虑开始建仓，跌至40分位加仓，跌至30分位再加仓，跌至20分位时重仓，删app。\n\n## 2021/4/27\n\n简单复盘一下近两天的窒息操作。耐不住长线价投的我，自以为发现了很好的短线机会，进场被吊锤。\n\n一周前传出华为自动驾驶的路试视频，北汽蓝谷应声3连板，随后就是三天的回调，以为自动驾驶会是一波像医美和碳中和的大行情，在26日水下低吸北汽蓝谷，当日最高涨幅7个点，但收盘只是微红。今天低开后就不断下杀，我在亏8个点时破位清仓。随后同板块的常熟气饰走出一波反包行情，封板前的一瞬间成功上车。但无奈很快被砸开，当日共亏13个点。\n\n不得不说，我这两天的这种短线行为简直就是赌博一般的行为，虽然刺激，但无疑是在浪费时间和钱财，还好大部分仓位都在长电，短线只是小仓位玩一玩，不然就欲哭无泪了。\n\n- 短线中的Alpha和Beta\n>y= Alpha+x*Beta\n>x: 代表市场收益 \n>Beta: 反映的是弹性，是投资组合对市场的敏感度\n>Alpha: 由投资者自身水平导致的超额收益\n\n这是我经常思考的一个问题，短线操作到底存不存在Alpha。目前我的思考是存在，但发现Alpha的人会将其保密。若不存在，股市上不可能出现8年10万倍的传说。而将其公开出去又会导致其因其广为人知的属性而变成Beta。","updated":"2021-04-27T07:51:22.075Z","path":"draft/Investment notes.html","comments":1,"layout":"page","_id":"ckotps9eb0002lsvwcp1gcu1a","content":"<p>本文以日记的形式，记录我人在股市、学习投资的一些感想。</p>\n<p><strong>人在股市，慢就是快。</strong></p>\n<a id=\"more\"></a>\n\n<h2 id=\"2021-03-22\"><a href=\"#2021-03-22\" class=\"headerlink\" title=\"2021/03/22\"></a>2021/03/22</h2><ul>\n<li>安全边际</li>\n</ul>\n<p>经过市场最近一个月的深度回调，让我深刻认识到了买入时安全边际的重要性。诚然，有着机构和媒体的推波助澜，找到一支股价已经在高位的优质股非常容易。但显然此时的买入是不具有安全边际的，虽然追高买入也还可能会有持续不断收益(比如2020年的白酒、新能源)，但谁也无法准确预测到它能涨到多高，但一旦遭遇市场风格的变化，回调往往是突然地、剧烈的、没有反弹的。那么如果你的成本没有安全边际的话，你大概率是从浮盈迅速变为浮亏的，此时割又不舍得割，不割又一路下跌，价格见底却又因为都被套住而没钱补仓的尴尬局面。</p>\n<p>因此，投资思路应该是管住手，如果没在行情刚刚启动的时候进入，那么不管涨的多疯，都不要去博傻，等优质股被错杀到一个合理的位置时，再逐步建仓，分批买入。</p>\n<ul>\n<li>每次买入都要有充足的理由</li>\n</ul>\n<p>证券分析员或许有着极高的说服能力，看了他的分析，似乎就觉得某家公司大有可为、未来可期。然而，如果让你经过思考后逐条列出在当前价位买入某某公司的原因时，或许你又会感觉写出的原因不是那么可靠。</p>\n<ul>\n<li>今日优质股记录</li>\n</ul>\n<p>贵州茅台：<br>股王，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率16.47%。<br>价格1989.99元，动态PE为55.42，PE百分位92.44%，极度高估。</p>\n<p>海天味业：<br>调味品龙头，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率27.21%。<br>价格151.66元，动态PE为80.63，PE百分位90.78%，极度高估。</p>\n<ul>\n<li>当前持股</li>\n</ul>\n<p>长电科技：<br>半导体封装测试龙头，受益于半导体国产替代大趋势，叠加芯片需求大增，产能满载，业绩预增。<br>成本40.20，当前价格34.08，浮亏15.23%。<br>市销率为2.09，百分位66.84%，估值偏高。</p>\n<h2 id=\"2021-03-25\"><a href=\"#2021-03-25\" class=\"headerlink\" title=\"2021/03/25\"></a>2021/03/25</h2><ul>\n<li>分散投资</li>\n</ul>\n<p>不要把鸡蛋放在一个篮子里，也最好不要把篮子放在一个桌子上，不然一波股灾桌子塌了。分散投资有很多好处，其一是在基本不影响收益的情况下减小收益曲线的波动，其二是增加投资组合的抗风险能力，避免由于踩雷财务造假导致整个账户崩盘。</p>\n<ul>\n<li>耐心</li>\n</ul>\n<p>在市场热度高时，好公司的估值都很高。很难找到一个可以接受的安全边际买入建仓，此时需要的就是耐心，从极度高估等到高估，从高估等到合理估值，可能能等到低估，也可能等不到低估。由于我们无法预测股票的短期走势，在公司基本面不变的情况下，我们可以选择分批进场的方式，在估值跌至50分位以下时考虑开始建仓，跌至40分位加仓，跌至30分位再加仓，跌至20分位时重仓，删app。</p>\n<h2 id=\"2021-4-27\"><a href=\"#2021-4-27\" class=\"headerlink\" title=\"2021/4/27\"></a>2021/4/27</h2><p>简单复盘一下近两天的窒息操作。耐不住长线价投的我，自以为发现了很好的短线机会，进场被吊锤。</p>\n<p>一周前传出华为自动驾驶的路试视频，北汽蓝谷应声3连板，随后就是三天的回调，以为自动驾驶会是一波像医美和碳中和的大行情，在26日水下低吸北汽蓝谷，当日最高涨幅7个点，但收盘只是微红。今天低开后就不断下杀，我在亏8个点时破位清仓。随后同板块的常熟气饰走出一波反包行情，封板前的一瞬间成功上车。但无奈很快被砸开，当日共亏13个点。</p>\n<p>不得不说，我这两天的这种短线行为简直就是赌博一般的行为，虽然刺激，但无疑是在浪费时间和钱财，还好大部分仓位都在长电，短线只是小仓位玩一玩，不然就欲哭无泪了。</p>\n<ul>\n<li>短线中的Alpha和Beta<blockquote>\n<p>y= Alpha+x*Beta<br>x: 代表市场收益<br>Beta: 反映的是弹性，是投资组合对市场的敏感度<br>Alpha: 由投资者自身水平导致的超额收益</p>\n</blockquote>\n</li>\n</ul>\n<p>这是我经常思考的一个问题，短线操作到底存不存在Alpha。目前我的思考是存在，但发现Alpha的人会将其保密。若不存在，股市上不可能出现8年10万倍的传说。而将其公开出去又会导致其因其广为人知的属性而变成Beta。</p>\n","site":{"data":{}},"excerpt":"<p>本文以日记的形式，记录我人在股市、学习投资的一些感想。</p>\n<p><strong>人在股市，慢就是快。</strong></p>","more":"<h2 id=\"2021-03-22\"><a href=\"#2021-03-22\" class=\"headerlink\" title=\"2021/03/22\"></a>2021/03/22</h2><ul>\n<li>安全边际</li>\n</ul>\n<p>经过市场最近一个月的深度回调，让我深刻认识到了买入时安全边际的重要性。诚然，有着机构和媒体的推波助澜，找到一支股价已经在高位的优质股非常容易。但显然此时的买入是不具有安全边际的，虽然追高买入也还可能会有持续不断收益(比如2020年的白酒、新能源)，但谁也无法准确预测到它能涨到多高，但一旦遭遇市场风格的变化，回调往往是突然地、剧烈的、没有反弹的。那么如果你的成本没有安全边际的话，你大概率是从浮盈迅速变为浮亏的，此时割又不舍得割，不割又一路下跌，价格见底却又因为都被套住而没钱补仓的尴尬局面。</p>\n<p>因此，投资思路应该是管住手，如果没在行情刚刚启动的时候进入，那么不管涨的多疯，都不要去博傻，等优质股被错杀到一个合理的位置时，再逐步建仓，分批买入。</p>\n<ul>\n<li>每次买入都要有充足的理由</li>\n</ul>\n<p>证券分析员或许有着极高的说服能力，看了他的分析，似乎就觉得某家公司大有可为、未来可期。然而，如果让你经过思考后逐条列出在当前价位买入某某公司的原因时，或许你又会感觉写出的原因不是那么可靠。</p>\n<ul>\n<li>今日优质股记录</li>\n</ul>\n<p>贵州茅台：<br>股王，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率16.47%。<br>价格1989.99元，动态PE为55.42，PE百分位92.44%，极度高估。</p>\n<p>海天味业：<br>调味品龙头，食品饮料板块，弱周期，高天花板，ROE为33%，资产负债率27.21%。<br>价格151.66元，动态PE为80.63，PE百分位90.78%，极度高估。</p>\n<ul>\n<li>当前持股</li>\n</ul>\n<p>长电科技：<br>半导体封装测试龙头，受益于半导体国产替代大趋势，叠加芯片需求大增，产能满载，业绩预增。<br>成本40.20，当前价格34.08，浮亏15.23%。<br>市销率为2.09，百分位66.84%，估值偏高。</p>\n<h2 id=\"2021-03-25\"><a href=\"#2021-03-25\" class=\"headerlink\" title=\"2021/03/25\"></a>2021/03/25</h2><ul>\n<li>分散投资</li>\n</ul>\n<p>不要把鸡蛋放在一个篮子里，也最好不要把篮子放在一个桌子上，不然一波股灾桌子塌了。分散投资有很多好处，其一是在基本不影响收益的情况下减小收益曲线的波动，其二是增加投资组合的抗风险能力，避免由于踩雷财务造假导致整个账户崩盘。</p>\n<ul>\n<li>耐心</li>\n</ul>\n<p>在市场热度高时，好公司的估值都很高。很难找到一个可以接受的安全边际买入建仓，此时需要的就是耐心，从极度高估等到高估，从高估等到合理估值，可能能等到低估，也可能等不到低估。由于我们无法预测股票的短期走势，在公司基本面不变的情况下，我们可以选择分批进场的方式，在估值跌至50分位以下时考虑开始建仓，跌至40分位加仓，跌至30分位再加仓，跌至20分位时重仓，删app。</p>\n<h2 id=\"2021-4-27\"><a href=\"#2021-4-27\" class=\"headerlink\" title=\"2021/4/27\"></a>2021/4/27</h2><p>简单复盘一下近两天的窒息操作。耐不住长线价投的我，自以为发现了很好的短线机会，进场被吊锤。</p>\n<p>一周前传出华为自动驾驶的路试视频，北汽蓝谷应声3连板，随后就是三天的回调，以为自动驾驶会是一波像医美和碳中和的大行情，在26日水下低吸北汽蓝谷，当日最高涨幅7个点，但收盘只是微红。今天低开后就不断下杀，我在亏8个点时破位清仓。随后同板块的常熟气饰走出一波反包行情，封板前的一瞬间成功上车。但无奈很快被砸开，当日共亏13个点。</p>\n<p>不得不说，我这两天的这种短线行为简直就是赌博一般的行为，虽然刺激，但无疑是在浪费时间和钱财，还好大部分仓位都在长电，短线只是小仓位玩一玩，不然就欲哭无泪了。</p>\n<ul>\n<li>短线中的Alpha和Beta<blockquote>\n<p>y= Alpha+x*Beta<br>x: 代表市场收益<br>Beta: 反映的是弹性，是投资组合对市场的敏感度<br>Alpha: 由投资者自身水平导致的超额收益</p>\n</blockquote>\n</li>\n</ul>\n<p>这是我经常思考的一个问题，短线操作到底存不存在Alpha。目前我的思考是存在，但发现Alpha的人会将其保密。若不存在，股市上不可能出现8年10万倍的传说。而将其公开出去又会导致其因其广为人知的属性而变成Beta。</p>"},{"title":"分类","date":"2020-01-11T14:24:16.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: 分类\ndate: 2020-01-11 22:24:16\ntype: categories\n---\n","updated":"2020-01-11T14:33:34.467Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ckotps9ef0006lsvw2xidcutw","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"诗集","date":"2019-12-11T16:12:57.000Z","_content":"还在等待诗人的到来...","source":"poems/index.md","raw":"---\ntitle: 诗集\ndate: 2019-12-12 00:12:57\n---\n还在等待诗人的到来...","updated":"2019-12-11T16:14:10.603Z","path":"poems/index.html","comments":1,"layout":"page","_id":"ckotps9eh0008lsvwbiuo8lqy","content":"<p>还在等待诗人的到来…</p>\n","site":{"data":{}},"excerpt":"","more":"<p>还在等待诗人的到来…</p>\n"},{"title":"标签","date":"2019-12-11T16:02:39.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: 标签\ndate: 2019-12-12 00:02:39\ntype: tags\ncomments: false\n---\n","updated":"2020-01-11T14:33:17.694Z","path":"tags/index.html","layout":"page","_id":"ckotps9ei000alsvw3rem0qma","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"为什么短线投机的预期收益不是零而是一赢二平七亏损?","date":"2021-04-27T07:50:41.000Z","_content":"\n这是一个很有意思的问题，如果靠扔飞镖来随机选取股票，理论上来讲，说短线投机行为的预期回报率应该是市场的平均回报率。但显然这在大部分情况下都不应该是一赢二平七亏损，不然我们可以很轻易的反向操作从而获利。那么，是什么因素导致了这一现象呢？\n<!--more-->\n\n## 资本资产定价模型CAPM\n\n为了解释这一现象，我们从CAPM模型讲起。\n\n$$R_p-R_f=\\alpha+\\beta(R_m-R_f)$$\n\n其中，$R_p$为投资组合的收益率，$R_f$为无风险收益率，$R_m$为市场的收益率，而$\\alpha$就代表超额收益。\n\n## 盈亏的不对称性来自于哪里\n\n通过CAPM模型，我们可以做出以下推论：\n\n1. 随机选取股票的方式$\\alpha$为0，投资组合的收益率来自于$\\beta$，也就是说虽然我们可以通过控制仓位或者加杠杆等方式改变$\\beta$，但最终能够赚钱纯靠运气。\n\n2. 一赢代表的人群包括了通过$\\beta$靠运气赚钱的人和靠正的$\\alpha$赚钱的人。\n\n3. 二平代表的人能力和运气相互抵消。\n\n4. 七亏损代表的人群包括通过$\\beta$靠运气亏钱的人和靠负的$\\alpha$亏钱的人。\n\n5. 运气大家都是一样的，2-3的不对称性说明少数人有正的$\\alpha$，他们持续从大部分负$\\alpha$的人手里不断地赚钱。\n\n也就是说，这10%的人赚了其他70%的人的钱说明了一个问题，那就是新入股市的人们自带的$\\alpha$是负的！没有经验的小白选股还不如扔飞镖乱选！这真是一个反直觉的问题。\n\n## 为什么大部分人的$\\alpha$都是负的呢？\n\n在分析这个问题之前，我们要深入思考一些问题，你赚的钱来自哪里？你按照什么逻辑赚的钱($\\alpha$的逻辑)？\n\n**博弈！博弈！博弈！**\n\n这与赌场十分类似，我们用赌场来举例，赌场的微小的正$\\alpha$来自于手续费，而赌客的$\\alpha$来自于人与人之间的博弈，虽然玩猜大小的游戏时，赌客的$\\alpha$都是负的，但在玩德州扑克等游戏时，有人可以通过记牌和计算概率的方式取得正的$\\alpha$，这会导致不会记牌的赌客获得一个绝对值更大的负$\\alpha$。\n\n回到股市，证券交易所的$\\alpha$来自于股民们的手续费，当你随机交易时，你的预期收益就是只亏损掉手续费，但当你通过分析来买卖股票时，由于大部分人的经验都来自于实际生活，都是大致相同的，在通过这些经验分析时会做出趋同的判断，则可能会被人预判到你的分析，从而在博弈中得到一个负的$\\alpha$。\n\n总结来说，因为**大部分人的分析是趋同的，有经验的投资者可以轻易的预判到你的分析**，从而他们可以在博弈中取得正的$\\alpha$而你会得到负的$\\alpha$。\n\n## 如何取得正的$\\alpha$\n\n1. 价值投资：通过超越大多数人的专业知识对公司进行更准确的判断取得正的$\\alpha$。\n2. 博弈(量化)：找到在与其他投资者博弈的过程中胜出的技巧。\n\n第二种方式是收益最高的方式，也是最难的方式，就算你找到了一个$\\alpha$，它也会随着时间逐渐的失效。曾经的海龟策略、双均线策略以及打板等手法都可以做到很高的收益，然而随着用的人越来越多，他们就会在博弈的时候越来越被针对，最终失效。\n\n价值投资是最适合散户的一种方式，因为散户大多有着自己的本职工作，很难做到每天花费大量的时间与其他的投资者进行动态的博弈。然而一个企业的基本面的变化一般是比较缓慢的，我们只需要在前期花费时间调研，选好后坚定持有，静待花开。","source":"_posts/alpha.md","raw":"---\ntitle: 为什么短线投机的预期收益不是零而是一赢二平七亏损?\ndate: 2021-04-27 15:50:41\ntags: 投资\ncategories: 投资\n---\n\n这是一个很有意思的问题，如果靠扔飞镖来随机选取股票，理论上来讲，说短线投机行为的预期回报率应该是市场的平均回报率。但显然这在大部分情况下都不应该是一赢二平七亏损，不然我们可以很轻易的反向操作从而获利。那么，是什么因素导致了这一现象呢？\n<!--more-->\n\n## 资本资产定价模型CAPM\n\n为了解释这一现象，我们从CAPM模型讲起。\n\n$$R_p-R_f=\\alpha+\\beta(R_m-R_f)$$\n\n其中，$R_p$为投资组合的收益率，$R_f$为无风险收益率，$R_m$为市场的收益率，而$\\alpha$就代表超额收益。\n\n## 盈亏的不对称性来自于哪里\n\n通过CAPM模型，我们可以做出以下推论：\n\n1. 随机选取股票的方式$\\alpha$为0，投资组合的收益率来自于$\\beta$，也就是说虽然我们可以通过控制仓位或者加杠杆等方式改变$\\beta$，但最终能够赚钱纯靠运气。\n\n2. 一赢代表的人群包括了通过$\\beta$靠运气赚钱的人和靠正的$\\alpha$赚钱的人。\n\n3. 二平代表的人能力和运气相互抵消。\n\n4. 七亏损代表的人群包括通过$\\beta$靠运气亏钱的人和靠负的$\\alpha$亏钱的人。\n\n5. 运气大家都是一样的，2-3的不对称性说明少数人有正的$\\alpha$，他们持续从大部分负$\\alpha$的人手里不断地赚钱。\n\n也就是说，这10%的人赚了其他70%的人的钱说明了一个问题，那就是新入股市的人们自带的$\\alpha$是负的！没有经验的小白选股还不如扔飞镖乱选！这真是一个反直觉的问题。\n\n## 为什么大部分人的$\\alpha$都是负的呢？\n\n在分析这个问题之前，我们要深入思考一些问题，你赚的钱来自哪里？你按照什么逻辑赚的钱($\\alpha$的逻辑)？\n\n**博弈！博弈！博弈！**\n\n这与赌场十分类似，我们用赌场来举例，赌场的微小的正$\\alpha$来自于手续费，而赌客的$\\alpha$来自于人与人之间的博弈，虽然玩猜大小的游戏时，赌客的$\\alpha$都是负的，但在玩德州扑克等游戏时，有人可以通过记牌和计算概率的方式取得正的$\\alpha$，这会导致不会记牌的赌客获得一个绝对值更大的负$\\alpha$。\n\n回到股市，证券交易所的$\\alpha$来自于股民们的手续费，当你随机交易时，你的预期收益就是只亏损掉手续费，但当你通过分析来买卖股票时，由于大部分人的经验都来自于实际生活，都是大致相同的，在通过这些经验分析时会做出趋同的判断，则可能会被人预判到你的分析，从而在博弈中得到一个负的$\\alpha$。\n\n总结来说，因为**大部分人的分析是趋同的，有经验的投资者可以轻易的预判到你的分析**，从而他们可以在博弈中取得正的$\\alpha$而你会得到负的$\\alpha$。\n\n## 如何取得正的$\\alpha$\n\n1. 价值投资：通过超越大多数人的专业知识对公司进行更准确的判断取得正的$\\alpha$。\n2. 博弈(量化)：找到在与其他投资者博弈的过程中胜出的技巧。\n\n第二种方式是收益最高的方式，也是最难的方式，就算你找到了一个$\\alpha$，它也会随着时间逐渐的失效。曾经的海龟策略、双均线策略以及打板等手法都可以做到很高的收益，然而随着用的人越来越多，他们就会在博弈的时候越来越被针对，最终失效。\n\n价值投资是最适合散户的一种方式，因为散户大多有着自己的本职工作，很难做到每天花费大量的时间与其他的投资者进行动态的博弈。然而一个企业的基本面的变化一般是比较缓慢的，我们只需要在前期花费时间调研，选好后坚定持有，静待花开。","slug":"alpha","published":1,"updated":"2021-04-27T09:11:15.195Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9e70001lsvwbabueop5","content":"<p>这是一个很有意思的问题，如果靠扔飞镖来随机选取股票，理论上来讲，说短线投机行为的预期回报率应该是市场的平均回报率。但显然这在大部分情况下都不应该是一赢二平七亏损，不然我们可以很轻易的反向操作从而获利。那么，是什么因素导致了这一现象呢？</p>\n<a id=\"more\"></a>\n\n<h2 id=\"资本资产定价模型CAPM\"><a href=\"#资本资产定价模型CAPM\" class=\"headerlink\" title=\"资本资产定价模型CAPM\"></a>资本资产定价模型CAPM</h2><p>为了解释这一现象，我们从CAPM模型讲起。</p>\n<p>$$R_p-R_f=\\alpha+\\beta(R_m-R_f)$$</p>\n<p>其中，$R_p$为投资组合的收益率，$R_f$为无风险收益率，$R_m$为市场的收益率，而$\\alpha$就代表超额收益。</p>\n<h2 id=\"盈亏的不对称性来自于哪里\"><a href=\"#盈亏的不对称性来自于哪里\" class=\"headerlink\" title=\"盈亏的不对称性来自于哪里\"></a>盈亏的不对称性来自于哪里</h2><p>通过CAPM模型，我们可以做出以下推论：</p>\n<ol>\n<li><p>随机选取股票的方式$\\alpha$为0，投资组合的收益率来自于$\\beta$，也就是说虽然我们可以通过控制仓位或者加杠杆等方式改变$\\beta$，但最终能够赚钱纯靠运气。</p>\n</li>\n<li><p>一赢代表的人群包括了通过$\\beta$靠运气赚钱的人和靠正的$\\alpha$赚钱的人。</p>\n</li>\n<li><p>二平代表的人能力和运气相互抵消。</p>\n</li>\n<li><p>七亏损代表的人群包括通过$\\beta$靠运气亏钱的人和靠负的$\\alpha$亏钱的人。</p>\n</li>\n<li><p>运气大家都是一样的，2-3的不对称性说明少数人有正的$\\alpha$，他们持续从大部分负$\\alpha$的人手里不断地赚钱。</p>\n</li>\n</ol>\n<p>也就是说，这10%的人赚了其他70%的人的钱说明了一个问题，那就是新入股市的人们自带的$\\alpha$是负的！没有经验的小白选股还不如扔飞镖乱选！这真是一个反直觉的问题。</p>\n<h2 id=\"为什么大部分人的-alpha-都是负的呢？\"><a href=\"#为什么大部分人的-alpha-都是负的呢？\" class=\"headerlink\" title=\"为什么大部分人的$\\alpha$都是负的呢？\"></a>为什么大部分人的$\\alpha$都是负的呢？</h2><p>在分析这个问题之前，我们要深入思考一些问题，你赚的钱来自哪里？你按照什么逻辑赚的钱($\\alpha$的逻辑)？</p>\n<p><strong>博弈！博弈！博弈！</strong></p>\n<p>这与赌场十分类似，我们用赌场来举例，赌场的微小的正$\\alpha$来自于手续费，而赌客的$\\alpha$来自于人与人之间的博弈，虽然玩猜大小的游戏时，赌客的$\\alpha$都是负的，但在玩德州扑克等游戏时，有人可以通过记牌和计算概率的方式取得正的$\\alpha$，这会导致不会记牌的赌客获得一个绝对值更大的负$\\alpha$。</p>\n<p>回到股市，证券交易所的$\\alpha$来自于股民们的手续费，当你随机交易时，你的预期收益就是只亏损掉手续费，但当你通过分析来买卖股票时，由于大部分人的经验都来自于实际生活，都是大致相同的，在通过这些经验分析时会做出趋同的判断，则可能会被人预判到你的分析，从而在博弈中得到一个负的$\\alpha$。</p>\n<p>总结来说，因为<strong>大部分人的分析是趋同的，有经验的投资者可以轻易的预判到你的分析</strong>，从而他们可以在博弈中取得正的$\\alpha$而你会得到负的$\\alpha$。</p>\n<h2 id=\"如何取得正的-alpha\"><a href=\"#如何取得正的-alpha\" class=\"headerlink\" title=\"如何取得正的$\\alpha$\"></a>如何取得正的$\\alpha$</h2><ol>\n<li>价值投资：通过超越大多数人的专业知识对公司进行更准确的判断取得正的$\\alpha$。</li>\n<li>博弈(量化)：找到在与其他投资者博弈的过程中胜出的技巧。</li>\n</ol>\n<p>第二种方式是收益最高的方式，也是最难的方式，就算你找到了一个$\\alpha$，它也会随着时间逐渐的失效。曾经的海龟策略、双均线策略以及打板等手法都可以做到很高的收益，然而随着用的人越来越多，他们就会在博弈的时候越来越被针对，最终失效。</p>\n<p>价值投资是最适合散户的一种方式，因为散户大多有着自己的本职工作，很难做到每天花费大量的时间与其他的投资者进行动态的博弈。然而一个企业的基本面的变化一般是比较缓慢的，我们只需要在前期花费时间调研，选好后坚定持有，静待花开。</p>\n","site":{"data":{}},"excerpt":"<p>这是一个很有意思的问题，如果靠扔飞镖来随机选取股票，理论上来讲，说短线投机行为的预期回报率应该是市场的平均回报率。但显然这在大部分情况下都不应该是一赢二平七亏损，不然我们可以很轻易的反向操作从而获利。那么，是什么因素导致了这一现象呢？</p>","more":"<h2 id=\"资本资产定价模型CAPM\"><a href=\"#资本资产定价模型CAPM\" class=\"headerlink\" title=\"资本资产定价模型CAPM\"></a>资本资产定价模型CAPM</h2><p>为了解释这一现象，我们从CAPM模型讲起。</p>\n<p>$$R_p-R_f=\\alpha+\\beta(R_m-R_f)$$</p>\n<p>其中，$R_p$为投资组合的收益率，$R_f$为无风险收益率，$R_m$为市场的收益率，而$\\alpha$就代表超额收益。</p>\n<h2 id=\"盈亏的不对称性来自于哪里\"><a href=\"#盈亏的不对称性来自于哪里\" class=\"headerlink\" title=\"盈亏的不对称性来自于哪里\"></a>盈亏的不对称性来自于哪里</h2><p>通过CAPM模型，我们可以做出以下推论：</p>\n<ol>\n<li><p>随机选取股票的方式$\\alpha$为0，投资组合的收益率来自于$\\beta$，也就是说虽然我们可以通过控制仓位或者加杠杆等方式改变$\\beta$，但最终能够赚钱纯靠运气。</p>\n</li>\n<li><p>一赢代表的人群包括了通过$\\beta$靠运气赚钱的人和靠正的$\\alpha$赚钱的人。</p>\n</li>\n<li><p>二平代表的人能力和运气相互抵消。</p>\n</li>\n<li><p>七亏损代表的人群包括通过$\\beta$靠运气亏钱的人和靠负的$\\alpha$亏钱的人。</p>\n</li>\n<li><p>运气大家都是一样的，2-3的不对称性说明少数人有正的$\\alpha$，他们持续从大部分负$\\alpha$的人手里不断地赚钱。</p>\n</li>\n</ol>\n<p>也就是说，这10%的人赚了其他70%的人的钱说明了一个问题，那就是新入股市的人们自带的$\\alpha$是负的！没有经验的小白选股还不如扔飞镖乱选！这真是一个反直觉的问题。</p>\n<h2 id=\"为什么大部分人的-alpha-都是负的呢？\"><a href=\"#为什么大部分人的-alpha-都是负的呢？\" class=\"headerlink\" title=\"为什么大部分人的$\\alpha$都是负的呢？\"></a>为什么大部分人的$\\alpha$都是负的呢？</h2><p>在分析这个问题之前，我们要深入思考一些问题，你赚的钱来自哪里？你按照什么逻辑赚的钱($\\alpha$的逻辑)？</p>\n<p><strong>博弈！博弈！博弈！</strong></p>\n<p>这与赌场十分类似，我们用赌场来举例，赌场的微小的正$\\alpha$来自于手续费，而赌客的$\\alpha$来自于人与人之间的博弈，虽然玩猜大小的游戏时，赌客的$\\alpha$都是负的，但在玩德州扑克等游戏时，有人可以通过记牌和计算概率的方式取得正的$\\alpha$，这会导致不会记牌的赌客获得一个绝对值更大的负$\\alpha$。</p>\n<p>回到股市，证券交易所的$\\alpha$来自于股民们的手续费，当你随机交易时，你的预期收益就是只亏损掉手续费，但当你通过分析来买卖股票时，由于大部分人的经验都来自于实际生活，都是大致相同的，在通过这些经验分析时会做出趋同的判断，则可能会被人预判到你的分析，从而在博弈中得到一个负的$\\alpha$。</p>\n<p>总结来说，因为<strong>大部分人的分析是趋同的，有经验的投资者可以轻易的预判到你的分析</strong>，从而他们可以在博弈中取得正的$\\alpha$而你会得到负的$\\alpha$。</p>\n<h2 id=\"如何取得正的-alpha\"><a href=\"#如何取得正的-alpha\" class=\"headerlink\" title=\"如何取得正的$\\alpha$\"></a>如何取得正的$\\alpha$</h2><ol>\n<li>价值投资：通过超越大多数人的专业知识对公司进行更准确的判断取得正的$\\alpha$。</li>\n<li>博弈(量化)：找到在与其他投资者博弈的过程中胜出的技巧。</li>\n</ol>\n<p>第二种方式是收益最高的方式，也是最难的方式，就算你找到了一个$\\alpha$，它也会随着时间逐渐的失效。曾经的海龟策略、双均线策略以及打板等手法都可以做到很高的收益，然而随着用的人越来越多，他们就会在博弈的时候越来越被针对，最终失效。</p>\n<p>价值投资是最适合散户的一种方式，因为散户大多有着自己的本职工作，很难做到每天花费大量的时间与其他的投资者进行动态的博弈。然而一个企业的基本面的变化一般是比较缓慢的，我们只需要在前期花费时间调研，选好后坚定持有，静待花开。</p>"},{"title":"深度学习","date":"2020-03-20T02:45:58.000Z","mathjax":true,"cover":"/asset/AI.jpeg","toc":true,"_content":"## 回顾机器学习\n\n### 定义\n一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。\n<!--more-->\n![机器学习过程](/asset/whatisml.png)\n\n### 机器学习的核心\n- 数据\n- 模型\n### 分类\n- 有监督学习\n  - 回归\n    - 线性回归\n  - 分类\n    - SVM\n![SVM](/asset/svm.jpg)\n- 无监督学习\n  - 聚类\n  - 主成分分析\n- 半监督学习\n- 增强学习(Reinforcement Learning)\n### 学习过程(监督学习)\n- 损失函数(loss function)\n- 优化方法\n- 梯度下降\n![梯度下降](/asset/gradient.jpeg)\n## 深度学习是什么\n>[wiki](https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0)：深度学习（英语：Deep Learning）是机器学习的分支，是一种以人工神经网络为架构，对数据进行表征学习的算法。\n\n![深度学习与机器学习的关系](/asset/dltoml.jpg)\n**<center>深度学习是机器学习的子集</center>**\n\n## 深度学习和传统机器学习算法的异同\n\n### 数据方面\n\n>Andrew Ng：“与深度学习类似的是，火箭发动机是深度学习模型，燃料是我们可以提供给这些算法的海量数据。\n\n![深度学习与数据的关系](/asset/dl_data.jpg)\n### 计算量方面\n深度学习在更新模型网络权重时涉及大量矩阵运算，在CPU上跑速度会很慢，而传统机器学习算法随便一台电脑就可以跑。因此深度学习最好在GPU上跑。\n\n耗时量级：\n- 传统机器学习：秒、分钟、小时\n- 深度学习：小时、天、周\n\n### 输入特征方面\n机器学习依赖于人类精心设计的特征才能取得较好的结果，深度学习主张让算法自己从原始数据中发现特征。不用太过高深的先验知识做支撑，但因此对数据量的需求比较大。\n\n## 深度学习算法\n\n### 人工神经网络(ANN)\n\n![ANN](/asset/deeplearning.jpg)\n其中的一个节点：\n![ANN](/asset/ANN.jpg)\n激活函数一定是一个非线性函数，用来增加网络的复杂性。不然不管网络有多少层，始终是一个线性函数。\n常用激活函数：\n\n- relu\n  - x if x > 0\n  - 0 if x <= 0\n- tanh\n\n### 卷积神经网络(CNN)\n#### 卷积\n在深度学习里的卷积,与数学上的和信号处理上关于卷积的概念有些不同.\n![卷积](/asset/juanji.jpg)\n#### CNN结构\n\n两条基本假设：\n\n- 最底层特征都是局部性的，也就是说，我们用10x10这样大小的过滤器就能表示边缘等底层特征\n- 图像上不同位置处特征是类似的，也就是说，我们能用同样的一组分类器来描述不同位置的图像——**平移不变性**\n\n![CNN](/asset/CNN.jpg)\n关键：**局部连接，权值共享，池化**\n\nCNN在处理图像数据时与ANN相比有着巨大的优势，通过局部连接、权值共享和池化大大减少了参数的数量，从而大大减少计算量、减少过拟合并大大提高模型的表现。\n\n### 模型训练\n\n[模型训练](http://peizhengyijiaqin.me/2019/12/14/ml-start/#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0)的过程可以认为是使损失函数最小化的过程\n![学习过程](/asset/learn.gif)\n\n### 模型泛化与过拟合、欠拟合问题\n\n因为深度学习的表达能力很强，当你的模型的表现很好时，你需要警惕，是模型学到了规律还是说模型记住了数据。检测方法也很简单，前者在一个陌生的数据集上表现依然很好，而后者反之。因此，在设计模型的时候也要考虑使用一些方式来尽可能的避免过拟合，从而得到较好的泛化能力。\n\n![fit](/asset/fit.jpg)\n\n### 书籍推荐\n\n入门书籍：\n该书简单易懂，为keras之父写的书。好上手，学了就能用，里面有很多demo可以跑。\n![keras](/asset/keras_book.jpg)\n\n进阶书籍：\n该书讲了很多数学、线代、概率论还有优化的东西，被奉为深度学习圣经，俗称“花书”，适合作为工具书，在手边随时查阅，入门较吃力。\n![花书](/asset/book.jpg)\n\n### 主流深度学习框架\n\n- Tensorflow\n- Pytorch\n- Keras\n- Paddlepaddle\n\n### 总结\n\n- 人工智能>机器学习>深度学习\n- 需要更多的数据量和算力\n- ANN\n  - 输出是输入的复杂非线性函数\n- CNN\n  - 局部连接\n  - 权值共享\n  - 池化\n- 模型训练\n  - 损失函数\n  - 梯度下降\n- 注意过拟合与欠拟合","source":"_posts/deep-learning.md","raw":"---\ntitle: 深度学习\ndate: 2020-03-20 10:45:58\ntags: [机器学习]\ncategories: 机器学习\nmathjax: true\ncover: /asset/AI.jpeg\ntoc: true\n---\n## 回顾机器学习\n\n### 定义\n一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。\n<!--more-->\n![机器学习过程](/asset/whatisml.png)\n\n### 机器学习的核心\n- 数据\n- 模型\n### 分类\n- 有监督学习\n  - 回归\n    - 线性回归\n  - 分类\n    - SVM\n![SVM](/asset/svm.jpg)\n- 无监督学习\n  - 聚类\n  - 主成分分析\n- 半监督学习\n- 增强学习(Reinforcement Learning)\n### 学习过程(监督学习)\n- 损失函数(loss function)\n- 优化方法\n- 梯度下降\n![梯度下降](/asset/gradient.jpeg)\n## 深度学习是什么\n>[wiki](https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0)：深度学习（英语：Deep Learning）是机器学习的分支，是一种以人工神经网络为架构，对数据进行表征学习的算法。\n\n![深度学习与机器学习的关系](/asset/dltoml.jpg)\n**<center>深度学习是机器学习的子集</center>**\n\n## 深度学习和传统机器学习算法的异同\n\n### 数据方面\n\n>Andrew Ng：“与深度学习类似的是，火箭发动机是深度学习模型，燃料是我们可以提供给这些算法的海量数据。\n\n![深度学习与数据的关系](/asset/dl_data.jpg)\n### 计算量方面\n深度学习在更新模型网络权重时涉及大量矩阵运算，在CPU上跑速度会很慢，而传统机器学习算法随便一台电脑就可以跑。因此深度学习最好在GPU上跑。\n\n耗时量级：\n- 传统机器学习：秒、分钟、小时\n- 深度学习：小时、天、周\n\n### 输入特征方面\n机器学习依赖于人类精心设计的特征才能取得较好的结果，深度学习主张让算法自己从原始数据中发现特征。不用太过高深的先验知识做支撑，但因此对数据量的需求比较大。\n\n## 深度学习算法\n\n### 人工神经网络(ANN)\n\n![ANN](/asset/deeplearning.jpg)\n其中的一个节点：\n![ANN](/asset/ANN.jpg)\n激活函数一定是一个非线性函数，用来增加网络的复杂性。不然不管网络有多少层，始终是一个线性函数。\n常用激活函数：\n\n- relu\n  - x if x > 0\n  - 0 if x <= 0\n- tanh\n\n### 卷积神经网络(CNN)\n#### 卷积\n在深度学习里的卷积,与数学上的和信号处理上关于卷积的概念有些不同.\n![卷积](/asset/juanji.jpg)\n#### CNN结构\n\n两条基本假设：\n\n- 最底层特征都是局部性的，也就是说，我们用10x10这样大小的过滤器就能表示边缘等底层特征\n- 图像上不同位置处特征是类似的，也就是说，我们能用同样的一组分类器来描述不同位置的图像——**平移不变性**\n\n![CNN](/asset/CNN.jpg)\n关键：**局部连接，权值共享，池化**\n\nCNN在处理图像数据时与ANN相比有着巨大的优势，通过局部连接、权值共享和池化大大减少了参数的数量，从而大大减少计算量、减少过拟合并大大提高模型的表现。\n\n### 模型训练\n\n[模型训练](http://peizhengyijiaqin.me/2019/12/14/ml-start/#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0)的过程可以认为是使损失函数最小化的过程\n![学习过程](/asset/learn.gif)\n\n### 模型泛化与过拟合、欠拟合问题\n\n因为深度学习的表达能力很强，当你的模型的表现很好时，你需要警惕，是模型学到了规律还是说模型记住了数据。检测方法也很简单，前者在一个陌生的数据集上表现依然很好，而后者反之。因此，在设计模型的时候也要考虑使用一些方式来尽可能的避免过拟合，从而得到较好的泛化能力。\n\n![fit](/asset/fit.jpg)\n\n### 书籍推荐\n\n入门书籍：\n该书简单易懂，为keras之父写的书。好上手，学了就能用，里面有很多demo可以跑。\n![keras](/asset/keras_book.jpg)\n\n进阶书籍：\n该书讲了很多数学、线代、概率论还有优化的东西，被奉为深度学习圣经，俗称“花书”，适合作为工具书，在手边随时查阅，入门较吃力。\n![花书](/asset/book.jpg)\n\n### 主流深度学习框架\n\n- Tensorflow\n- Pytorch\n- Keras\n- Paddlepaddle\n\n### 总结\n\n- 人工智能>机器学习>深度学习\n- 需要更多的数据量和算力\n- ANN\n  - 输出是输入的复杂非线性函数\n- CNN\n  - 局部连接\n  - 权值共享\n  - 池化\n- 模型训练\n  - 损失函数\n  - 梯度下降\n- 注意过拟合与欠拟合","slug":"deep-learning","published":1,"updated":"2021-02-01T15:48:24.980Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9ec0003lsvwhqrldwlc","content":"<h2 id=\"回顾机器学习\"><a href=\"#回顾机器学习\" class=\"headerlink\" title=\"回顾机器学习\"></a>回顾机器学习</h2><h3 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h3><p>一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。</p>\n<a id=\"more\"></a>\n<p><img src=\"/asset/whatisml.png\" alt=\"机器学习过程\"></p>\n<h3 id=\"机器学习的核心\"><a href=\"#机器学习的核心\" class=\"headerlink\" title=\"机器学习的核心\"></a>机器学习的核心</h3><ul>\n<li>数据</li>\n<li>模型<h3 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h3></li>\n<li>有监督学习<ul>\n<li>回归<ul>\n<li>线性回归</li>\n</ul>\n</li>\n<li>分类<ul>\n<li>SVM<br><img src=\"/asset/svm.jpg\" alt=\"SVM\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>无监督学习<ul>\n<li>聚类</li>\n<li>主成分分析</li>\n</ul>\n</li>\n<li>半监督学习</li>\n<li>增强学习(Reinforcement Learning)<h3 id=\"学习过程-监督学习\"><a href=\"#学习过程-监督学习\" class=\"headerlink\" title=\"学习过程(监督学习)\"></a>学习过程(监督学习)</h3></li>\n<li>损失函数(loss function)</li>\n<li>优化方法</li>\n<li>梯度下降<br><img src=\"/asset/gradient.jpeg\" alt=\"梯度下降\"><h2 id=\"深度学习是什么\"><a href=\"#深度学习是什么\" class=\"headerlink\" title=\"深度学习是什么\"></a>深度学习是什么</h2><blockquote>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\">wiki</a>：深度学习（英语：Deep Learning）是机器学习的分支，是一种以人工神经网络为架构，对数据进行表征学习的算法。</p>\n</blockquote>\n</li>\n</ul>\n<p><img src=\"/asset/dltoml.jpg\" alt=\"深度学习与机器学习的关系\"><br><strong><center>深度学习是机器学习的子集</center></strong></p>\n<h2 id=\"深度学习和传统机器学习算法的异同\"><a href=\"#深度学习和传统机器学习算法的异同\" class=\"headerlink\" title=\"深度学习和传统机器学习算法的异同\"></a>深度学习和传统机器学习算法的异同</h2><h3 id=\"数据方面\"><a href=\"#数据方面\" class=\"headerlink\" title=\"数据方面\"></a>数据方面</h3><blockquote>\n<p>Andrew Ng：“与深度学习类似的是，火箭发动机是深度学习模型，燃料是我们可以提供给这些算法的海量数据。</p>\n</blockquote>\n<p><img src=\"/asset/dl_data.jpg\" alt=\"深度学习与数据的关系\"></p>\n<h3 id=\"计算量方面\"><a href=\"#计算量方面\" class=\"headerlink\" title=\"计算量方面\"></a>计算量方面</h3><p>深度学习在更新模型网络权重时涉及大量矩阵运算，在CPU上跑速度会很慢，而传统机器学习算法随便一台电脑就可以跑。因此深度学习最好在GPU上跑。</p>\n<p>耗时量级：</p>\n<ul>\n<li>传统机器学习：秒、分钟、小时</li>\n<li>深度学习：小时、天、周</li>\n</ul>\n<h3 id=\"输入特征方面\"><a href=\"#输入特征方面\" class=\"headerlink\" title=\"输入特征方面\"></a>输入特征方面</h3><p>机器学习依赖于人类精心设计的特征才能取得较好的结果，深度学习主张让算法自己从原始数据中发现特征。不用太过高深的先验知识做支撑，但因此对数据量的需求比较大。</p>\n<h2 id=\"深度学习算法\"><a href=\"#深度学习算法\" class=\"headerlink\" title=\"深度学习算法\"></a>深度学习算法</h2><h3 id=\"人工神经网络-ANN\"><a href=\"#人工神经网络-ANN\" class=\"headerlink\" title=\"人工神经网络(ANN)\"></a>人工神经网络(ANN)</h3><p><img src=\"/asset/deeplearning.jpg\" alt=\"ANN\"><br>其中的一个节点：<br><img src=\"/asset/ANN.jpg\" alt=\"ANN\"><br>激活函数一定是一个非线性函数，用来增加网络的复杂性。不然不管网络有多少层，始终是一个线性函数。<br>常用激活函数：</p>\n<ul>\n<li>relu<ul>\n<li>x if x &gt; 0</li>\n<li>0 if x &lt;= 0</li>\n</ul>\n</li>\n<li>tanh</li>\n</ul>\n<h3 id=\"卷积神经网络-CNN\"><a href=\"#卷积神经网络-CNN\" class=\"headerlink\" title=\"卷积神经网络(CNN)\"></a>卷积神经网络(CNN)</h3><h4 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h4><p>在深度学习里的卷积,与数学上的和信号处理上关于卷积的概念有些不同.<br><img src=\"/asset/juanji.jpg\" alt=\"卷积\"></p>\n<h4 id=\"CNN结构\"><a href=\"#CNN结构\" class=\"headerlink\" title=\"CNN结构\"></a>CNN结构</h4><p>两条基本假设：</p>\n<ul>\n<li>最底层特征都是局部性的，也就是说，我们用10x10这样大小的过滤器就能表示边缘等底层特征</li>\n<li>图像上不同位置处特征是类似的，也就是说，我们能用同样的一组分类器来描述不同位置的图像——<strong>平移不变性</strong></li>\n</ul>\n<p><img src=\"/asset/CNN.jpg\" alt=\"CNN\"><br>关键：<strong>局部连接，权值共享，池化</strong></p>\n<p>CNN在处理图像数据时与ANN相比有着巨大的优势，通过局部连接、权值共享和池化大大减少了参数的数量，从而大大减少计算量、减少过拟合并大大提高模型的表现。</p>\n<h3 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h3><p><a href=\"http://peizhengyijiaqin.me/2019/12/14/ml-start/#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\">模型训练</a>的过程可以认为是使损失函数最小化的过程<br><img src=\"/asset/learn.gif\" alt=\"学习过程\"></p>\n<h3 id=\"模型泛化与过拟合、欠拟合问题\"><a href=\"#模型泛化与过拟合、欠拟合问题\" class=\"headerlink\" title=\"模型泛化与过拟合、欠拟合问题\"></a>模型泛化与过拟合、欠拟合问题</h3><p>因为深度学习的表达能力很强，当你的模型的表现很好时，你需要警惕，是模型学到了规律还是说模型记住了数据。检测方法也很简单，前者在一个陌生的数据集上表现依然很好，而后者反之。因此，在设计模型的时候也要考虑使用一些方式来尽可能的避免过拟合，从而得到较好的泛化能力。</p>\n<p><img src=\"/asset/fit.jpg\" alt=\"fit\"></p>\n<h3 id=\"书籍推荐\"><a href=\"#书籍推荐\" class=\"headerlink\" title=\"书籍推荐\"></a>书籍推荐</h3><p>入门书籍：<br>该书简单易懂，为keras之父写的书。好上手，学了就能用，里面有很多demo可以跑。<br><img src=\"/asset/keras_book.jpg\" alt=\"keras\"></p>\n<p>进阶书籍：<br>该书讲了很多数学、线代、概率论还有优化的东西，被奉为深度学习圣经，俗称“花书”，适合作为工具书，在手边随时查阅，入门较吃力。<br><img src=\"/asset/book.jpg\" alt=\"花书\"></p>\n<h3 id=\"主流深度学习框架\"><a href=\"#主流深度学习框架\" class=\"headerlink\" title=\"主流深度学习框架\"></a>主流深度学习框架</h3><ul>\n<li>Tensorflow</li>\n<li>Pytorch</li>\n<li>Keras</li>\n<li>Paddlepaddle</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><ul>\n<li>人工智能&gt;机器学习&gt;深度学习</li>\n<li>需要更多的数据量和算力</li>\n<li>ANN<ul>\n<li>输出是输入的复杂非线性函数</li>\n</ul>\n</li>\n<li>CNN<ul>\n<li>局部连接</li>\n<li>权值共享</li>\n<li>池化</li>\n</ul>\n</li>\n<li>模型训练<ul>\n<li>损失函数</li>\n<li>梯度下降</li>\n</ul>\n</li>\n<li>注意过拟合与欠拟合</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h2 id=\"回顾机器学习\"><a href=\"#回顾机器学习\" class=\"headerlink\" title=\"回顾机器学习\"></a>回顾机器学习</h2><h3 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h3><p>一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。</p>","more":"<p><img src=\"/asset/whatisml.png\" alt=\"机器学习过程\"></p>\n<h3 id=\"机器学习的核心\"><a href=\"#机器学习的核心\" class=\"headerlink\" title=\"机器学习的核心\"></a>机器学习的核心</h3><ul>\n<li>数据</li>\n<li>模型<h3 id=\"分类\"><a href=\"#分类\" class=\"headerlink\" title=\"分类\"></a>分类</h3></li>\n<li>有监督学习<ul>\n<li>回归<ul>\n<li>线性回归</li>\n</ul>\n</li>\n<li>分类<ul>\n<li>SVM<br><img src=\"/asset/svm.jpg\" alt=\"SVM\"></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>无监督学习<ul>\n<li>聚类</li>\n<li>主成分分析</li>\n</ul>\n</li>\n<li>半监督学习</li>\n<li>增强学习(Reinforcement Learning)<h3 id=\"学习过程-监督学习\"><a href=\"#学习过程-监督学习\" class=\"headerlink\" title=\"学习过程(监督学习)\"></a>学习过程(监督学习)</h3></li>\n<li>损失函数(loss function)</li>\n<li>优化方法</li>\n<li>梯度下降<br><img src=\"/asset/gradient.jpeg\" alt=\"梯度下降\"><h2 id=\"深度学习是什么\"><a href=\"#深度学习是什么\" class=\"headerlink\" title=\"深度学习是什么\"></a>深度学习是什么</h2><blockquote>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\">wiki</a>：深度学习（英语：Deep Learning）是机器学习的分支，是一种以人工神经网络为架构，对数据进行表征学习的算法。</p>\n</blockquote>\n</li>\n</ul>\n<p><img src=\"/asset/dltoml.jpg\" alt=\"深度学习与机器学习的关系\"><br><strong><center>深度学习是机器学习的子集</center></strong></p>\n<h2 id=\"深度学习和传统机器学习算法的异同\"><a href=\"#深度学习和传统机器学习算法的异同\" class=\"headerlink\" title=\"深度学习和传统机器学习算法的异同\"></a>深度学习和传统机器学习算法的异同</h2><h3 id=\"数据方面\"><a href=\"#数据方面\" class=\"headerlink\" title=\"数据方面\"></a>数据方面</h3><blockquote>\n<p>Andrew Ng：“与深度学习类似的是，火箭发动机是深度学习模型，燃料是我们可以提供给这些算法的海量数据。</p>\n</blockquote>\n<p><img src=\"/asset/dl_data.jpg\" alt=\"深度学习与数据的关系\"></p>\n<h3 id=\"计算量方面\"><a href=\"#计算量方面\" class=\"headerlink\" title=\"计算量方面\"></a>计算量方面</h3><p>深度学习在更新模型网络权重时涉及大量矩阵运算，在CPU上跑速度会很慢，而传统机器学习算法随便一台电脑就可以跑。因此深度学习最好在GPU上跑。</p>\n<p>耗时量级：</p>\n<ul>\n<li>传统机器学习：秒、分钟、小时</li>\n<li>深度学习：小时、天、周</li>\n</ul>\n<h3 id=\"输入特征方面\"><a href=\"#输入特征方面\" class=\"headerlink\" title=\"输入特征方面\"></a>输入特征方面</h3><p>机器学习依赖于人类精心设计的特征才能取得较好的结果，深度学习主张让算法自己从原始数据中发现特征。不用太过高深的先验知识做支撑，但因此对数据量的需求比较大。</p>\n<h2 id=\"深度学习算法\"><a href=\"#深度学习算法\" class=\"headerlink\" title=\"深度学习算法\"></a>深度学习算法</h2><h3 id=\"人工神经网络-ANN\"><a href=\"#人工神经网络-ANN\" class=\"headerlink\" title=\"人工神经网络(ANN)\"></a>人工神经网络(ANN)</h3><p><img src=\"/asset/deeplearning.jpg\" alt=\"ANN\"><br>其中的一个节点：<br><img src=\"/asset/ANN.jpg\" alt=\"ANN\"><br>激活函数一定是一个非线性函数，用来增加网络的复杂性。不然不管网络有多少层，始终是一个线性函数。<br>常用激活函数：</p>\n<ul>\n<li>relu<ul>\n<li>x if x &gt; 0</li>\n<li>0 if x &lt;= 0</li>\n</ul>\n</li>\n<li>tanh</li>\n</ul>\n<h3 id=\"卷积神经网络-CNN\"><a href=\"#卷积神经网络-CNN\" class=\"headerlink\" title=\"卷积神经网络(CNN)\"></a>卷积神经网络(CNN)</h3><h4 id=\"卷积\"><a href=\"#卷积\" class=\"headerlink\" title=\"卷积\"></a>卷积</h4><p>在深度学习里的卷积,与数学上的和信号处理上关于卷积的概念有些不同.<br><img src=\"/asset/juanji.jpg\" alt=\"卷积\"></p>\n<h4 id=\"CNN结构\"><a href=\"#CNN结构\" class=\"headerlink\" title=\"CNN结构\"></a>CNN结构</h4><p>两条基本假设：</p>\n<ul>\n<li>最底层特征都是局部性的，也就是说，我们用10x10这样大小的过滤器就能表示边缘等底层特征</li>\n<li>图像上不同位置处特征是类似的，也就是说，我们能用同样的一组分类器来描述不同位置的图像——<strong>平移不变性</strong></li>\n</ul>\n<p><img src=\"/asset/CNN.jpg\" alt=\"CNN\"><br>关键：<strong>局部连接，权值共享，池化</strong></p>\n<p>CNN在处理图像数据时与ANN相比有着巨大的优势，通过局部连接、权值共享和池化大大减少了参数的数量，从而大大减少计算量、减少过拟合并大大提高模型的表现。</p>\n<h3 id=\"模型训练\"><a href=\"#模型训练\" class=\"headerlink\" title=\"模型训练\"></a>模型训练</h3><p><a href=\"http://peizhengyijiaqin.me/2019/12/14/ml-start/#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0\">模型训练</a>的过程可以认为是使损失函数最小化的过程<br><img src=\"/asset/learn.gif\" alt=\"学习过程\"></p>\n<h3 id=\"模型泛化与过拟合、欠拟合问题\"><a href=\"#模型泛化与过拟合、欠拟合问题\" class=\"headerlink\" title=\"模型泛化与过拟合、欠拟合问题\"></a>模型泛化与过拟合、欠拟合问题</h3><p>因为深度学习的表达能力很强，当你的模型的表现很好时，你需要警惕，是模型学到了规律还是说模型记住了数据。检测方法也很简单，前者在一个陌生的数据集上表现依然很好，而后者反之。因此，在设计模型的时候也要考虑使用一些方式来尽可能的避免过拟合，从而得到较好的泛化能力。</p>\n<p><img src=\"/asset/fit.jpg\" alt=\"fit\"></p>\n<h3 id=\"书籍推荐\"><a href=\"#书籍推荐\" class=\"headerlink\" title=\"书籍推荐\"></a>书籍推荐</h3><p>入门书籍：<br>该书简单易懂，为keras之父写的书。好上手，学了就能用，里面有很多demo可以跑。<br><img src=\"/asset/keras_book.jpg\" alt=\"keras\"></p>\n<p>进阶书籍：<br>该书讲了很多数学、线代、概率论还有优化的东西，被奉为深度学习圣经，俗称“花书”，适合作为工具书，在手边随时查阅，入门较吃力。<br><img src=\"/asset/book.jpg\" alt=\"花书\"></p>\n<h3 id=\"主流深度学习框架\"><a href=\"#主流深度学习框架\" class=\"headerlink\" title=\"主流深度学习框架\"></a>主流深度学习框架</h3><ul>\n<li>Tensorflow</li>\n<li>Pytorch</li>\n<li>Keras</li>\n<li>Paddlepaddle</li>\n</ul>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><ul>\n<li>人工智能&gt;机器学习&gt;深度学习</li>\n<li>需要更多的数据量和算力</li>\n<li>ANN<ul>\n<li>输出是输入的复杂非线性函数</li>\n</ul>\n</li>\n<li>CNN<ul>\n<li>局部连接</li>\n<li>权值共享</li>\n<li>池化</li>\n</ul>\n</li>\n<li>模型训练<ul>\n<li>损失函数</li>\n<li>梯度下降</li>\n</ul>\n</li>\n<li>注意过拟合与欠拟合</li>\n</ul>"},{"title":"DigitalOcean+Hexo+Nginx+Namecheap搭建个人博客","date":"2019-12-10T15:57:14.000Z","cover":"/asset/create.jpg","toc":true,"_content":"强烈推荐[Github学生包](https://education.github.com/pack)，内含大量对学生的免费福利，包括不限于Jetbrain全家桶，AWS，Azure，DigitalOcean，Namecheap，name等。本文基于Github学生包里的DigitalOcean $50和Namecheap一年个人域名搭建个人博客。\n\n## 技术架构\n\n- VPS：[DigitalOcean](https://cloud.digitalocean.com/)\n- 域名注册：[NameCheap](https://www.namecheap.com/)\n- 博客框架：[Hexo](https://hexo.io/zh-cn/)\n- 自动部署：[githook](https://git-scm.com/)\n<!--more-->\n## 前期准备\n\n- 学生，或者有一个.edu后缀的邮箱\n- 可以给国外付款的visa卡，或者PayPal（可使用银联的借记卡）\n- $5用来激活账户\n\n## Github学生认证\n\n这一步网络上有大量教程，本文不再赘述。\n\n参见：\n\n- [注册Github并进行学生认证](https://blog.csdn.net/qq_40176716/article/details/84679999)\n- [Github教程 学生认证](https://blog.csdn.net/qq_36667170/article/details/79084166)\n\n## 注册Paypal\n\n亲测中国银行借记卡(有union标志)可用。\n\n参见：\n\n- [注册申请PayPal支付账户](https://blog.csdn.net/love_bb/article/details/76064080)\n- [PayPal注册绑卡使用教程](https://blog.csdn.net/PecoVio/article/details/82708048)\n\n## 注册DigitalOcean\n\n这一步也很简单，参见网上教程，只是最近注册完之后增加了一步verify，需要实名认证。processing的过程有点慢，我的认证13个小时才给我成功，如果卡在processing不用急，睡一觉就好了。\n\n参见：\n\n- [在GitHub Students Developer Pack申请DigitalOcean的50刀优惠码](https://blog.csdn.net/hunzhangzui9837/article/details/84974624)\n- [从领取Github教育礼包到DigitalOcean购买服务器](https://www.jianshu.com/p/c5e7721d886c?tdsourcetag=s_pctim_aiomsg)\n\n## 申请一个服务器\n\n新建一个实例（droplets）,系统选择ubuntu（centos也可），standard（标准型，我们用来搭建个人博客足够了）：\n\n![create-droplets](/asset/create-droplets.png)\n\n价格选择最便宜的，这样可以用接近一年\n\n![prize](/asset/prize.png)\n\n重点来了！！！\n\n服务器既然在国外，那速度肯定是首要考虑的。所以在选择服务器所在地区的时候，首先我们可以在官网测一下到达每个地方的速度，选择最快的地方搭建我们的服务器。在这里我选择了Frankfurt：\n\n![region](/asset/region.png)\n\n下面是一些常规选项，IPV6看个人需求可要可不要，其他有些是付费的，没需求就不用改\n\n![others](/asset/others.png)\n\n最后点击create droplet等待30s即创建好第一个服务器，随后服务器IP，Root，密码会发到你的邮箱里。\n\n![final](/asset/final.png)\n\n## 本地配置hexo\n\n怎么安装，怎么使用，怎么用markdown写作，怎么部署到远程VPS服务器，在[下一篇博客](http://peizhengyijiaqin.me/2019/12/11/writing/)会讲。\n\n## 部署Hexo到远程VPS服务器\n\nputty输入服务器IP和密码远程连接，在第一次登录时由于DigitalOcean的安全策略会让你修改自己的登录密码。\n\n具体过程参见：\n[简书-VPS部署Hexo网站](https://www.jianshu.com/p/5cf20649f328)\n\n现在，在浏览器输入 http://[yourIP] 就可以看到你的个人网站了！下一步我们通过设置域名来访问。\n\n## 注册域名/域名解析\n\n从Github学生包界面进入namecheap，挑选没有被别人占用的域名，确认后设置域名解析：\n[Namecheap域名如何绑定IP](https://blog.csdn.net/SweetTool/article/details/87900507)\n\n等待几分钟后，就可以通过你的.me域名进入网站了！","source":"_posts/blog-init.md","raw":"---\ntitle: DigitalOcean+Hexo+Nginx+Namecheap搭建个人博客\ndate: 2019-12-10 23:57:14\ncategories: 博客\ntags: [VPS,hexo,hginx,网站]\ncover: /asset/create.jpg\ntoc: true\n---\n强烈推荐[Github学生包](https://education.github.com/pack)，内含大量对学生的免费福利，包括不限于Jetbrain全家桶，AWS，Azure，DigitalOcean，Namecheap，name等。本文基于Github学生包里的DigitalOcean $50和Namecheap一年个人域名搭建个人博客。\n\n## 技术架构\n\n- VPS：[DigitalOcean](https://cloud.digitalocean.com/)\n- 域名注册：[NameCheap](https://www.namecheap.com/)\n- 博客框架：[Hexo](https://hexo.io/zh-cn/)\n- 自动部署：[githook](https://git-scm.com/)\n<!--more-->\n## 前期准备\n\n- 学生，或者有一个.edu后缀的邮箱\n- 可以给国外付款的visa卡，或者PayPal（可使用银联的借记卡）\n- $5用来激活账户\n\n## Github学生认证\n\n这一步网络上有大量教程，本文不再赘述。\n\n参见：\n\n- [注册Github并进行学生认证](https://blog.csdn.net/qq_40176716/article/details/84679999)\n- [Github教程 学生认证](https://blog.csdn.net/qq_36667170/article/details/79084166)\n\n## 注册Paypal\n\n亲测中国银行借记卡(有union标志)可用。\n\n参见：\n\n- [注册申请PayPal支付账户](https://blog.csdn.net/love_bb/article/details/76064080)\n- [PayPal注册绑卡使用教程](https://blog.csdn.net/PecoVio/article/details/82708048)\n\n## 注册DigitalOcean\n\n这一步也很简单，参见网上教程，只是最近注册完之后增加了一步verify，需要实名认证。processing的过程有点慢，我的认证13个小时才给我成功，如果卡在processing不用急，睡一觉就好了。\n\n参见：\n\n- [在GitHub Students Developer Pack申请DigitalOcean的50刀优惠码](https://blog.csdn.net/hunzhangzui9837/article/details/84974624)\n- [从领取Github教育礼包到DigitalOcean购买服务器](https://www.jianshu.com/p/c5e7721d886c?tdsourcetag=s_pctim_aiomsg)\n\n## 申请一个服务器\n\n新建一个实例（droplets）,系统选择ubuntu（centos也可），standard（标准型，我们用来搭建个人博客足够了）：\n\n![create-droplets](/asset/create-droplets.png)\n\n价格选择最便宜的，这样可以用接近一年\n\n![prize](/asset/prize.png)\n\n重点来了！！！\n\n服务器既然在国外，那速度肯定是首要考虑的。所以在选择服务器所在地区的时候，首先我们可以在官网测一下到达每个地方的速度，选择最快的地方搭建我们的服务器。在这里我选择了Frankfurt：\n\n![region](/asset/region.png)\n\n下面是一些常规选项，IPV6看个人需求可要可不要，其他有些是付费的，没需求就不用改\n\n![others](/asset/others.png)\n\n最后点击create droplet等待30s即创建好第一个服务器，随后服务器IP，Root，密码会发到你的邮箱里。\n\n![final](/asset/final.png)\n\n## 本地配置hexo\n\n怎么安装，怎么使用，怎么用markdown写作，怎么部署到远程VPS服务器，在[下一篇博客](http://peizhengyijiaqin.me/2019/12/11/writing/)会讲。\n\n## 部署Hexo到远程VPS服务器\n\nputty输入服务器IP和密码远程连接，在第一次登录时由于DigitalOcean的安全策略会让你修改自己的登录密码。\n\n具体过程参见：\n[简书-VPS部署Hexo网站](https://www.jianshu.com/p/5cf20649f328)\n\n现在，在浏览器输入 http://[yourIP] 就可以看到你的个人网站了！下一步我们通过设置域名来访问。\n\n## 注册域名/域名解析\n\n从Github学生包界面进入namecheap，挑选没有被别人占用的域名，确认后设置域名解析：\n[Namecheap域名如何绑定IP](https://blog.csdn.net/SweetTool/article/details/87900507)\n\n等待几分钟后，就可以通过你的.me域名进入网站了！","slug":"blog-init","published":1,"updated":"2021-02-01T14:41:08.984Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9eg0007lsvwee0z67cr","content":"<p>强烈推荐<a href=\"https://education.github.com/pack\">Github学生包</a>，内含大量对学生的免费福利，包括不限于Jetbrain全家桶，AWS，Azure，DigitalOcean，Namecheap，name等。本文基于Github学生包里的DigitalOcean $50和Namecheap一年个人域名搭建个人博客。</p>\n<h2 id=\"技术架构\"><a href=\"#技术架构\" class=\"headerlink\" title=\"技术架构\"></a>技术架构</h2><ul>\n<li><p>VPS：<a href=\"https://cloud.digitalocean.com/\">DigitalOcean</a></p>\n</li>\n<li><p>域名注册：<a href=\"https://www.namecheap.com/\">NameCheap</a></p>\n</li>\n<li><p>博客框架：<a href=\"https://hexo.io/zh-cn/\">Hexo</a></p>\n</li>\n<li><p>自动部署：<a href=\"https://git-scm.com/\">githook</a></p>\n<a id=\"more\"></a>\n<h2 id=\"前期准备\"><a href=\"#前期准备\" class=\"headerlink\" title=\"前期准备\"></a>前期准备</h2></li>\n<li><p>学生，或者有一个.edu后缀的邮箱</p>\n</li>\n<li><p>可以给国外付款的visa卡，或者PayPal（可使用银联的借记卡）</p>\n</li>\n<li><p>$5用来激活账户</p>\n</li>\n</ul>\n<h2 id=\"Github学生认证\"><a href=\"#Github学生认证\" class=\"headerlink\" title=\"Github学生认证\"></a>Github学生认证</h2><p>这一步网络上有大量教程，本文不再赘述。</p>\n<p>参见：</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/qq_40176716/article/details/84679999\">注册Github并进行学生认证</a></li>\n<li><a href=\"https://blog.csdn.net/qq_36667170/article/details/79084166\">Github教程 学生认证</a></li>\n</ul>\n<h2 id=\"注册Paypal\"><a href=\"#注册Paypal\" class=\"headerlink\" title=\"注册Paypal\"></a>注册Paypal</h2><p>亲测中国银行借记卡(有union标志)可用。</p>\n<p>参见：</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/love_bb/article/details/76064080\">注册申请PayPal支付账户</a></li>\n<li><a href=\"https://blog.csdn.net/PecoVio/article/details/82708048\">PayPal注册绑卡使用教程</a></li>\n</ul>\n<h2 id=\"注册DigitalOcean\"><a href=\"#注册DigitalOcean\" class=\"headerlink\" title=\"注册DigitalOcean\"></a>注册DigitalOcean</h2><p>这一步也很简单，参见网上教程，只是最近注册完之后增加了一步verify，需要实名认证。processing的过程有点慢，我的认证13个小时才给我成功，如果卡在processing不用急，睡一觉就好了。</p>\n<p>参见：</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/hunzhangzui9837/article/details/84974624\">在GitHub Students Developer Pack申请DigitalOcean的50刀优惠码</a></li>\n<li><a href=\"https://www.jianshu.com/p/c5e7721d886c?tdsourcetag=s_pctim_aiomsg\">从领取Github教育礼包到DigitalOcean购买服务器</a></li>\n</ul>\n<h2 id=\"申请一个服务器\"><a href=\"#申请一个服务器\" class=\"headerlink\" title=\"申请一个服务器\"></a>申请一个服务器</h2><p>新建一个实例（droplets）,系统选择ubuntu（centos也可），standard（标准型，我们用来搭建个人博客足够了）：</p>\n<p><img src=\"/asset/create-droplets.png\" alt=\"create-droplets\"></p>\n<p>价格选择最便宜的，这样可以用接近一年</p>\n<p><img src=\"/asset/prize.png\" alt=\"prize\"></p>\n<p>重点来了！！！</p>\n<p>服务器既然在国外，那速度肯定是首要考虑的。所以在选择服务器所在地区的时候，首先我们可以在官网测一下到达每个地方的速度，选择最快的地方搭建我们的服务器。在这里我选择了Frankfurt：</p>\n<p><img src=\"/asset/region.png\" alt=\"region\"></p>\n<p>下面是一些常规选项，IPV6看个人需求可要可不要，其他有些是付费的，没需求就不用改</p>\n<p><img src=\"/asset/others.png\" alt=\"others\"></p>\n<p>最后点击create droplet等待30s即创建好第一个服务器，随后服务器IP，Root，密码会发到你的邮箱里。</p>\n<p><img src=\"/asset/final.png\" alt=\"final\"></p>\n<h2 id=\"本地配置hexo\"><a href=\"#本地配置hexo\" class=\"headerlink\" title=\"本地配置hexo\"></a>本地配置hexo</h2><p>怎么安装，怎么使用，怎么用markdown写作，怎么部署到远程VPS服务器，在<a href=\"http://peizhengyijiaqin.me/2019/12/11/writing/\">下一篇博客</a>会讲。</p>\n<h2 id=\"部署Hexo到远程VPS服务器\"><a href=\"#部署Hexo到远程VPS服务器\" class=\"headerlink\" title=\"部署Hexo到远程VPS服务器\"></a>部署Hexo到远程VPS服务器</h2><p>putty输入服务器IP和密码远程连接，在第一次登录时由于DigitalOcean的安全策略会让你修改自己的登录密码。</p>\n<p>具体过程参见：<br><a href=\"https://www.jianshu.com/p/5cf20649f328\">简书-VPS部署Hexo网站</a></p>\n<p>现在，在浏览器输入 http://[yourIP] 就可以看到你的个人网站了！下一步我们通过设置域名来访问。</p>\n<h2 id=\"注册域名-域名解析\"><a href=\"#注册域名-域名解析\" class=\"headerlink\" title=\"注册域名/域名解析\"></a>注册域名/域名解析</h2><p>从Github学生包界面进入namecheap，挑选没有被别人占用的域名，确认后设置域名解析：<br><a href=\"https://blog.csdn.net/SweetTool/article/details/87900507\">Namecheap域名如何绑定IP</a></p>\n<p>等待几分钟后，就可以通过你的.me域名进入网站了！</p>\n","site":{"data":{}},"excerpt":"<p>强烈推荐<a href=\"https://education.github.com/pack\">Github学生包</a>，内含大量对学生的免费福利，包括不限于Jetbrain全家桶，AWS，Azure，DigitalOcean，Namecheap，name等。本文基于Github学生包里的DigitalOcean $50和Namecheap一年个人域名搭建个人博客。</p>\n<h2 id=\"技术架构\"><a href=\"#技术架构\" class=\"headerlink\" title=\"技术架构\"></a>技术架构</h2><ul>\n<li><p>VPS：<a href=\"https://cloud.digitalocean.com/\">DigitalOcean</a></p>\n</li>\n<li><p>域名注册：<a href=\"https://www.namecheap.com/\">NameCheap</a></p>\n</li>\n<li><p>博客框架：<a href=\"https://hexo.io/zh-cn/\">Hexo</a></p>\n</li>\n<li><p>自动部署：<a href=\"https://git-scm.com/\">githook</a></p>","more":"<h2 id=\"前期准备\"><a href=\"#前期准备\" class=\"headerlink\" title=\"前期准备\"></a>前期准备</h2></li>\n<li><p>学生，或者有一个.edu后缀的邮箱</p>\n</li>\n<li><p>可以给国外付款的visa卡，或者PayPal（可使用银联的借记卡）</p>\n</li>\n<li><p>$5用来激活账户</p>\n</li>\n</ul>\n<h2 id=\"Github学生认证\"><a href=\"#Github学生认证\" class=\"headerlink\" title=\"Github学生认证\"></a>Github学生认证</h2><p>这一步网络上有大量教程，本文不再赘述。</p>\n<p>参见：</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/qq_40176716/article/details/84679999\">注册Github并进行学生认证</a></li>\n<li><a href=\"https://blog.csdn.net/qq_36667170/article/details/79084166\">Github教程 学生认证</a></li>\n</ul>\n<h2 id=\"注册Paypal\"><a href=\"#注册Paypal\" class=\"headerlink\" title=\"注册Paypal\"></a>注册Paypal</h2><p>亲测中国银行借记卡(有union标志)可用。</p>\n<p>参见：</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/love_bb/article/details/76064080\">注册申请PayPal支付账户</a></li>\n<li><a href=\"https://blog.csdn.net/PecoVio/article/details/82708048\">PayPal注册绑卡使用教程</a></li>\n</ul>\n<h2 id=\"注册DigitalOcean\"><a href=\"#注册DigitalOcean\" class=\"headerlink\" title=\"注册DigitalOcean\"></a>注册DigitalOcean</h2><p>这一步也很简单，参见网上教程，只是最近注册完之后增加了一步verify，需要实名认证。processing的过程有点慢，我的认证13个小时才给我成功，如果卡在processing不用急，睡一觉就好了。</p>\n<p>参见：</p>\n<ul>\n<li><a href=\"https://blog.csdn.net/hunzhangzui9837/article/details/84974624\">在GitHub Students Developer Pack申请DigitalOcean的50刀优惠码</a></li>\n<li><a href=\"https://www.jianshu.com/p/c5e7721d886c?tdsourcetag=s_pctim_aiomsg\">从领取Github教育礼包到DigitalOcean购买服务器</a></li>\n</ul>\n<h2 id=\"申请一个服务器\"><a href=\"#申请一个服务器\" class=\"headerlink\" title=\"申请一个服务器\"></a>申请一个服务器</h2><p>新建一个实例（droplets）,系统选择ubuntu（centos也可），standard（标准型，我们用来搭建个人博客足够了）：</p>\n<p><img src=\"/asset/create-droplets.png\" alt=\"create-droplets\"></p>\n<p>价格选择最便宜的，这样可以用接近一年</p>\n<p><img src=\"/asset/prize.png\" alt=\"prize\"></p>\n<p>重点来了！！！</p>\n<p>服务器既然在国外，那速度肯定是首要考虑的。所以在选择服务器所在地区的时候，首先我们可以在官网测一下到达每个地方的速度，选择最快的地方搭建我们的服务器。在这里我选择了Frankfurt：</p>\n<p><img src=\"/asset/region.png\" alt=\"region\"></p>\n<p>下面是一些常规选项，IPV6看个人需求可要可不要，其他有些是付费的，没需求就不用改</p>\n<p><img src=\"/asset/others.png\" alt=\"others\"></p>\n<p>最后点击create droplet等待30s即创建好第一个服务器，随后服务器IP，Root，密码会发到你的邮箱里。</p>\n<p><img src=\"/asset/final.png\" alt=\"final\"></p>\n<h2 id=\"本地配置hexo\"><a href=\"#本地配置hexo\" class=\"headerlink\" title=\"本地配置hexo\"></a>本地配置hexo</h2><p>怎么安装，怎么使用，怎么用markdown写作，怎么部署到远程VPS服务器，在<a href=\"http://peizhengyijiaqin.me/2019/12/11/writing/\">下一篇博客</a>会讲。</p>\n<h2 id=\"部署Hexo到远程VPS服务器\"><a href=\"#部署Hexo到远程VPS服务器\" class=\"headerlink\" title=\"部署Hexo到远程VPS服务器\"></a>部署Hexo到远程VPS服务器</h2><p>putty输入服务器IP和密码远程连接，在第一次登录时由于DigitalOcean的安全策略会让你修改自己的登录密码。</p>\n<p>具体过程参见：<br><a href=\"https://www.jianshu.com/p/5cf20649f328\">简书-VPS部署Hexo网站</a></p>\n<p>现在，在浏览器输入 http://[yourIP] 就可以看到你的个人网站了！下一步我们通过设置域名来访问。</p>\n<h2 id=\"注册域名-域名解析\"><a href=\"#注册域名-域名解析\" class=\"headerlink\" title=\"注册域名/域名解析\"></a>注册域名/域名解析</h2><p>从Github学生包界面进入namecheap，挑选没有被别人占用的域名，确认后设置域名解析：<br><a href=\"https://blog.csdn.net/SweetTool/article/details/87900507\">Namecheap域名如何绑定IP</a></p>\n<p>等待几分钟后，就可以通过你的.me域名进入网站了！</p>"},{"title":"机器学习概览","date":"2019-12-14T04:07:39.000Z","mathjax":true,"toc":true,"_content":"## 机器学习是什么\n\n### 定义\n\n>[wiki](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)：机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。\n\n>Arthur samuel：机器学习是在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。\n\n一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。\n<!--more-->\n让我[引用](https://www.jianshu.com/p/25ef14c072ad)一张图片来说明：\n![机器学习过程](/asset/whatisml.png)\n注意：这里是用有监督模型举例(后面会对这个名词进一步解释)\n\n### 针对机器学习最重要的内容\n- 数据：经验只有转化为了计算机能够理解的数据，才能让计算机从中学习。谁的数据量大、质量高，谁就占据了机器学习和人工只能领域最有利的资本。\n- 模型：即算法，有了数据之后，可以设计模型，通过数据来训练这个模型。这个模型就是机器学习的核心，作为用来产生决策的中枢。\n\n### 数据的分类\n- 结构化数据---------储存在数据库中的数据\n- 非结构化数据-------语音信号、图像图形、自然语言\n\n\n## 机器学习能干什么\n\n* 语音识别、机器翻译(微软Cortana、苹果Siri、科大讯飞、谷歌翻译)\n* 人脸识别(微信、支付宝、宿舍门禁)\n* 量化交易(预测股市)\n* 房价预测\n* 推荐系统(淘宝、京东、抖音)\n* 医生/老中医\n* 解微分方程、不定积分(见：[AI拿下高数一血，求解微分方程、不定积分只需1秒，成绩远超Matlab](https://zhuanlan.zhihu.com/p/98174049?utm_source=zhihu&utm_medium=social&utm_oi=667848254054731776))\n* 寻找淹没在背景噪声中的小信号([Higgs Boson Machine Learning Challenge](https://www.kaggle.com/c/higgs-boson)、引力波信号、引力透镜参数预测)\n\n## 好用的机器学习库以及书籍推荐\n\nscikit-learn：最有名且易用好上手，广泛作为机器学习的入门库(Python)\n书籍：![scikit-learn机器学习](/asset/scikit.png)\n\n## 机器学习的分类\n\n按照模型的学习方式，我们可以分为如下几类：\n\n### 有监督学习\n\n对于数据集中的每一条数据，我们在把它交给算法前就有了相应的“正确答案”，我们的算法就是在基于这些我们人为给定的“正确答案”在做预测。\n\n有监督学习的任务一般是回归或者分类问题：\n\n- 回归：线性回归\n比如通过商品房的地段、高度、外形、面积、采光面积等参数来预测商品房价格。预测结果是一个连续的值。\n- 分类：支持向量机(SVM)、决策树、逻辑回归、朴素贝叶斯、KNN\n比如通过西瓜的颜色、大小、重量、花纹等等预测西瓜甜不甜。\n请注意，这里我们预测的输出只包括甜和不甜，是一个离散值，这是一个二分类的问题。\n反之，若是我们输出的是西瓜介于0到1之间的的甜度(0是一点都不甜，1是超级甜)，那这个分类问题就转化为了回归问题。\n\n### 无监督学习\n\n对于数据集中的所有数据，他们没有一个相应的“正确答案”。算法要做的是利用算法自动的将数据归类，也叫做**聚类**。\n\n- KMEAN\n\n### 半监督学习\n\n介于监督学习和无监督学习之间的一种方式。即一部分数据有标记，一部分数据没有标记。\n\n### 增强学习\n\n增强学习是一种有反馈的学习方式。\n\n例子：贪吃蛇问题  \n一个N×N的格子里，定义贪吃蛇每一步上下左右随机行走，碰到墙或者自身则得到负反馈，吃到果子得到正反馈，在训练很多轮以后，贪吃蛇就学会了如何躲避墙和自身去吃果子。\n\n## 机器学习的一般步骤\n\n### 数据采集和标记\n\n- 构建数据集：收集尽可能的多的特征，给出数据标记（人工或自动）\n\n预测房价：面积大小、房间数、地段、楼龄等；\n芝麻信用：海量的用户交易数据；\n\n### 数据清洗\n\n- 对数据中的单位进行统一\n- 去掉重复数据、噪声和数据缺失\n\n### 特征选择\n\n- 从哪些特征对进行机器学习是有用的；人工设计或自动选择\n\n### 模型选择\n\n- 根据问题选择模型，聚类还是分类，回归还是分类\n\n### 模型训练和测试\n\n- 把数据集合分为训练集和测试集，用训练集训练模型，训练完成后用测试集测试模型的精度。(测试集必须是模型没有见过的数据)\n\n### 模型性能评估与优化\n\n- 训练时长，训练数据是否足够，是否能满足需要\n\n### 模型使用\n\n- 训练好的模型进行存储，以备下次使用\n\n## 机器如何学习\n\n### 损失函数\n\n例如：\n用一维线性回归举例:\n\n $$ y = kx+b $$\n\nx就是我们的input，y就是我们的label，我们首先给算法一定的(x,y)数据，算法拟合出来一条直线方程，当我们再输入x数据时，模型能够预测出相应的y。这就是一个简单的有监督机器学习例子。但是，机器怎么知道哪个k和b是最好的呢？\n\n也就是说，我们需要用一个指标来衡量模型和数据的拟合程度，而模型的预测值和真实值的差，我们叫做**损失函数**。在这里，我们训练的一个线性回归模型，可以是让MSE(均方误差)最小，MSE在这里被称为这个回归模型的**损失函数**，它代表了预测值与真实值的偏离程度。而我们机器学习的过程就是通过改变k和b使得**损失函数**取得**最小值**。\n$$\nL_{MSE}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n$$\n这里的m表示数据个数。\n\n除此之外，对于**二分类**问题来说，常用的损失函数是二元交叉熵损失(Logistic损失):\n\n$$\nL_{\\text {logistic }}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}[-y_{i} \\log \\hat{y}_{i}-\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)]\n$$\n\n这里的 $\\hat{y}$ 代表预测y=1的概率。\n### 梯度下降\n\n我们知道了我们的目标是让k和b最小，那我们怎么实现呢？接下来我们来看一下它的解决方案——使用**梯度下降**算法来更新参数。\n\n![梯度下降](/asset/gradient.jpeg)\n\n这里 $\\theta_0$ 和 $\\theta_1$ 分别代表k和b。\n\n我们从一个随机的k和b出发，沿着向下最快的路径行走，直到达到最低点，即此时k和b收敛于一个定值，这个定值就是我们想要得到的使得损失函数最小的值。\n\n## 代码实现\n\n本文使用scikit-learn实现一个线性回归模型举例：\n```python\nfrom sklearn.linear_model import LinearRegression #从sklearn引入线性回归模型\nmodel=LinearRegression()                          #声明线性回归模型\nmodel.fit(X_train,Y_train)                        #用训练数据训练模型\nY_pred=model.predict(X_test)                      #用模型对测试数据做预测\nprint(Y_pred)                                     #输出预测结果\n```\n常用的API有：\n<table>\n  <tr>\n    <th>API</th>\n    <th>解释</th>\n  </tr>\n  <tr>\n    <td>fit(X_train,Y_train)</td>\n    <td>训练模型</td>\n  </tr>\n    <tr>\n    <td>predict(X_test)</td>\n    <td>预测测试数据的结果</td>\n  </tr>\n    <tr>\n    <td>score(X_test,Y_test)</td>\n    <td>测试预测数据的score(例如正确率)</td>\n  </tr>\n</table>\n\n这里只是展示了很少很少的API，还有很多非常非常实用的API及教程参见[官方文档](https://sklearn.apachecn.org/)\n\n## 总结\n\n- 1.获取数据\n- 2.处理数据(80%的时间)\n- 3.训练模型(20%的时间)\n- 4.进行预测\n- 5.观察结果，不满意则重复2.3.4.步，满意则保存模型\n","source":"_posts/ml-start.md","raw":"---\ntitle: 机器学习概览\ndate: 2019-12-14 12:07:39\ncategories: 机器学习\ntags: [机器学习]\nmathjax: true\ntoc: true\n---\n## 机器学习是什么\n\n### 定义\n\n>[wiki](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)：机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。\n\n>Arthur samuel：机器学习是在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。\n\n一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。\n<!--more-->\n让我[引用](https://www.jianshu.com/p/25ef14c072ad)一张图片来说明：\n![机器学习过程](/asset/whatisml.png)\n注意：这里是用有监督模型举例(后面会对这个名词进一步解释)\n\n### 针对机器学习最重要的内容\n- 数据：经验只有转化为了计算机能够理解的数据，才能让计算机从中学习。谁的数据量大、质量高，谁就占据了机器学习和人工只能领域最有利的资本。\n- 模型：即算法，有了数据之后，可以设计模型，通过数据来训练这个模型。这个模型就是机器学习的核心，作为用来产生决策的中枢。\n\n### 数据的分类\n- 结构化数据---------储存在数据库中的数据\n- 非结构化数据-------语音信号、图像图形、自然语言\n\n\n## 机器学习能干什么\n\n* 语音识别、机器翻译(微软Cortana、苹果Siri、科大讯飞、谷歌翻译)\n* 人脸识别(微信、支付宝、宿舍门禁)\n* 量化交易(预测股市)\n* 房价预测\n* 推荐系统(淘宝、京东、抖音)\n* 医生/老中医\n* 解微分方程、不定积分(见：[AI拿下高数一血，求解微分方程、不定积分只需1秒，成绩远超Matlab](https://zhuanlan.zhihu.com/p/98174049?utm_source=zhihu&utm_medium=social&utm_oi=667848254054731776))\n* 寻找淹没在背景噪声中的小信号([Higgs Boson Machine Learning Challenge](https://www.kaggle.com/c/higgs-boson)、引力波信号、引力透镜参数预测)\n\n## 好用的机器学习库以及书籍推荐\n\nscikit-learn：最有名且易用好上手，广泛作为机器学习的入门库(Python)\n书籍：![scikit-learn机器学习](/asset/scikit.png)\n\n## 机器学习的分类\n\n按照模型的学习方式，我们可以分为如下几类：\n\n### 有监督学习\n\n对于数据集中的每一条数据，我们在把它交给算法前就有了相应的“正确答案”，我们的算法就是在基于这些我们人为给定的“正确答案”在做预测。\n\n有监督学习的任务一般是回归或者分类问题：\n\n- 回归：线性回归\n比如通过商品房的地段、高度、外形、面积、采光面积等参数来预测商品房价格。预测结果是一个连续的值。\n- 分类：支持向量机(SVM)、决策树、逻辑回归、朴素贝叶斯、KNN\n比如通过西瓜的颜色、大小、重量、花纹等等预测西瓜甜不甜。\n请注意，这里我们预测的输出只包括甜和不甜，是一个离散值，这是一个二分类的问题。\n反之，若是我们输出的是西瓜介于0到1之间的的甜度(0是一点都不甜，1是超级甜)，那这个分类问题就转化为了回归问题。\n\n### 无监督学习\n\n对于数据集中的所有数据，他们没有一个相应的“正确答案”。算法要做的是利用算法自动的将数据归类，也叫做**聚类**。\n\n- KMEAN\n\n### 半监督学习\n\n介于监督学习和无监督学习之间的一种方式。即一部分数据有标记，一部分数据没有标记。\n\n### 增强学习\n\n增强学习是一种有反馈的学习方式。\n\n例子：贪吃蛇问题  \n一个N×N的格子里，定义贪吃蛇每一步上下左右随机行走，碰到墙或者自身则得到负反馈，吃到果子得到正反馈，在训练很多轮以后，贪吃蛇就学会了如何躲避墙和自身去吃果子。\n\n## 机器学习的一般步骤\n\n### 数据采集和标记\n\n- 构建数据集：收集尽可能的多的特征，给出数据标记（人工或自动）\n\n预测房价：面积大小、房间数、地段、楼龄等；\n芝麻信用：海量的用户交易数据；\n\n### 数据清洗\n\n- 对数据中的单位进行统一\n- 去掉重复数据、噪声和数据缺失\n\n### 特征选择\n\n- 从哪些特征对进行机器学习是有用的；人工设计或自动选择\n\n### 模型选择\n\n- 根据问题选择模型，聚类还是分类，回归还是分类\n\n### 模型训练和测试\n\n- 把数据集合分为训练集和测试集，用训练集训练模型，训练完成后用测试集测试模型的精度。(测试集必须是模型没有见过的数据)\n\n### 模型性能评估与优化\n\n- 训练时长，训练数据是否足够，是否能满足需要\n\n### 模型使用\n\n- 训练好的模型进行存储，以备下次使用\n\n## 机器如何学习\n\n### 损失函数\n\n例如：\n用一维线性回归举例:\n\n $$ y = kx+b $$\n\nx就是我们的input，y就是我们的label，我们首先给算法一定的(x,y)数据，算法拟合出来一条直线方程，当我们再输入x数据时，模型能够预测出相应的y。这就是一个简单的有监督机器学习例子。但是，机器怎么知道哪个k和b是最好的呢？\n\n也就是说，我们需要用一个指标来衡量模型和数据的拟合程度，而模型的预测值和真实值的差，我们叫做**损失函数**。在这里，我们训练的一个线性回归模型，可以是让MSE(均方误差)最小，MSE在这里被称为这个回归模型的**损失函数**，它代表了预测值与真实值的偏离程度。而我们机器学习的过程就是通过改变k和b使得**损失函数**取得**最小值**。\n$$\nL_{MSE}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n$$\n这里的m表示数据个数。\n\n除此之外，对于**二分类**问题来说，常用的损失函数是二元交叉熵损失(Logistic损失):\n\n$$\nL_{\\text {logistic }}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}[-y_{i} \\log \\hat{y}_{i}-\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)]\n$$\n\n这里的 $\\hat{y}$ 代表预测y=1的概率。\n### 梯度下降\n\n我们知道了我们的目标是让k和b最小，那我们怎么实现呢？接下来我们来看一下它的解决方案——使用**梯度下降**算法来更新参数。\n\n![梯度下降](/asset/gradient.jpeg)\n\n这里 $\\theta_0$ 和 $\\theta_1$ 分别代表k和b。\n\n我们从一个随机的k和b出发，沿着向下最快的路径行走，直到达到最低点，即此时k和b收敛于一个定值，这个定值就是我们想要得到的使得损失函数最小的值。\n\n## 代码实现\n\n本文使用scikit-learn实现一个线性回归模型举例：\n```python\nfrom sklearn.linear_model import LinearRegression #从sklearn引入线性回归模型\nmodel=LinearRegression()                          #声明线性回归模型\nmodel.fit(X_train,Y_train)                        #用训练数据训练模型\nY_pred=model.predict(X_test)                      #用模型对测试数据做预测\nprint(Y_pred)                                     #输出预测结果\n```\n常用的API有：\n<table>\n  <tr>\n    <th>API</th>\n    <th>解释</th>\n  </tr>\n  <tr>\n    <td>fit(X_train,Y_train)</td>\n    <td>训练模型</td>\n  </tr>\n    <tr>\n    <td>predict(X_test)</td>\n    <td>预测测试数据的结果</td>\n  </tr>\n    <tr>\n    <td>score(X_test,Y_test)</td>\n    <td>测试预测数据的score(例如正确率)</td>\n  </tr>\n</table>\n\n这里只是展示了很少很少的API，还有很多非常非常实用的API及教程参见[官方文档](https://sklearn.apachecn.org/)\n\n## 总结\n\n- 1.获取数据\n- 2.处理数据(80%的时间)\n- 3.训练模型(20%的时间)\n- 4.进行预测\n- 5.观察结果，不满意则重复2.3.4.步，满意则保存模型\n","slug":"ml-start","published":1,"updated":"2021-02-01T15:54:41.569Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9eh0009lsvw0ddt7xoq","content":"<h2 id=\"机器学习是什么\"><a href=\"#机器学习是什么\" class=\"headerlink\" title=\"机器学习是什么\"></a>机器学习是什么</h2><h3 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h3><blockquote>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\">wiki</a>：机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。</p>\n</blockquote>\n<blockquote>\n<p>Arthur samuel：机器学习是在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。</p>\n</blockquote>\n<p>一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。</p>\n<a id=\"more\"></a>\n<p>让我<a href=\"https://www.jianshu.com/p/25ef14c072ad\">引用</a>一张图片来说明：<br><img src=\"/asset/whatisml.png\" alt=\"机器学习过程\"><br>注意：这里是用有监督模型举例(后面会对这个名词进一步解释)</p>\n<h3 id=\"针对机器学习最重要的内容\"><a href=\"#针对机器学习最重要的内容\" class=\"headerlink\" title=\"针对机器学习最重要的内容\"></a>针对机器学习最重要的内容</h3><ul>\n<li>数据：经验只有转化为了计算机能够理解的数据，才能让计算机从中学习。谁的数据量大、质量高，谁就占据了机器学习和人工只能领域最有利的资本。</li>\n<li>模型：即算法，有了数据之后，可以设计模型，通过数据来训练这个模型。这个模型就是机器学习的核心，作为用来产生决策的中枢。</li>\n</ul>\n<h3 id=\"数据的分类\"><a href=\"#数据的分类\" class=\"headerlink\" title=\"数据的分类\"></a>数据的分类</h3><ul>\n<li>结构化数据———储存在数据库中的数据</li>\n<li>非结构化数据——-语音信号、图像图形、自然语言</li>\n</ul>\n<h2 id=\"机器学习能干什么\"><a href=\"#机器学习能干什么\" class=\"headerlink\" title=\"机器学习能干什么\"></a>机器学习能干什么</h2><ul>\n<li>语音识别、机器翻译(微软Cortana、苹果Siri、科大讯飞、谷歌翻译)</li>\n<li>人脸识别(微信、支付宝、宿舍门禁)</li>\n<li>量化交易(预测股市)</li>\n<li>房价预测</li>\n<li>推荐系统(淘宝、京东、抖音)</li>\n<li>医生/老中医</li>\n<li>解微分方程、不定积分(见：<a href=\"https://zhuanlan.zhihu.com/p/98174049?utm_source=zhihu&utm_medium=social&utm_oi=667848254054731776\">AI拿下高数一血，求解微分方程、不定积分只需1秒，成绩远超Matlab</a>)</li>\n<li>寻找淹没在背景噪声中的小信号(<a href=\"https://www.kaggle.com/c/higgs-boson\">Higgs Boson Machine Learning Challenge</a>、引力波信号、引力透镜参数预测)</li>\n</ul>\n<h2 id=\"好用的机器学习库以及书籍推荐\"><a href=\"#好用的机器学习库以及书籍推荐\" class=\"headerlink\" title=\"好用的机器学习库以及书籍推荐\"></a>好用的机器学习库以及书籍推荐</h2><p>scikit-learn：最有名且易用好上手，广泛作为机器学习的入门库(Python)<br>书籍：<img src=\"/asset/scikit.png\" alt=\"scikit-learn机器学习\"></p>\n<h2 id=\"机器学习的分类\"><a href=\"#机器学习的分类\" class=\"headerlink\" title=\"机器学习的分类\"></a>机器学习的分类</h2><p>按照模型的学习方式，我们可以分为如下几类：</p>\n<h3 id=\"有监督学习\"><a href=\"#有监督学习\" class=\"headerlink\" title=\"有监督学习\"></a>有监督学习</h3><p>对于数据集中的每一条数据，我们在把它交给算法前就有了相应的“正确答案”，我们的算法就是在基于这些我们人为给定的“正确答案”在做预测。</p>\n<p>有监督学习的任务一般是回归或者分类问题：</p>\n<ul>\n<li>回归：线性回归<br>比如通过商品房的地段、高度、外形、面积、采光面积等参数来预测商品房价格。预测结果是一个连续的值。</li>\n<li>分类：支持向量机(SVM)、决策树、逻辑回归、朴素贝叶斯、KNN<br>比如通过西瓜的颜色、大小、重量、花纹等等预测西瓜甜不甜。<br>请注意，这里我们预测的输出只包括甜和不甜，是一个离散值，这是一个二分类的问题。<br>反之，若是我们输出的是西瓜介于0到1之间的的甜度(0是一点都不甜，1是超级甜)，那这个分类问题就转化为了回归问题。</li>\n</ul>\n<h3 id=\"无监督学习\"><a href=\"#无监督学习\" class=\"headerlink\" title=\"无监督学习\"></a>无监督学习</h3><p>对于数据集中的所有数据，他们没有一个相应的“正确答案”。算法要做的是利用算法自动的将数据归类，也叫做<strong>聚类</strong>。</p>\n<ul>\n<li>KMEAN</li>\n</ul>\n<h3 id=\"半监督学习\"><a href=\"#半监督学习\" class=\"headerlink\" title=\"半监督学习\"></a>半监督学习</h3><p>介于监督学习和无监督学习之间的一种方式。即一部分数据有标记，一部分数据没有标记。</p>\n<h3 id=\"增强学习\"><a href=\"#增强学习\" class=\"headerlink\" title=\"增强学习\"></a>增强学习</h3><p>增强学习是一种有反馈的学习方式。</p>\n<p>例子：贪吃蛇问题<br>一个N×N的格子里，定义贪吃蛇每一步上下左右随机行走，碰到墙或者自身则得到负反馈，吃到果子得到正反馈，在训练很多轮以后，贪吃蛇就学会了如何躲避墙和自身去吃果子。</p>\n<h2 id=\"机器学习的一般步骤\"><a href=\"#机器学习的一般步骤\" class=\"headerlink\" title=\"机器学习的一般步骤\"></a>机器学习的一般步骤</h2><h3 id=\"数据采集和标记\"><a href=\"#数据采集和标记\" class=\"headerlink\" title=\"数据采集和标记\"></a>数据采集和标记</h3><ul>\n<li>构建数据集：收集尽可能的多的特征，给出数据标记（人工或自动）</li>\n</ul>\n<p>预测房价：面积大小、房间数、地段、楼龄等；<br>芝麻信用：海量的用户交易数据；</p>\n<h3 id=\"数据清洗\"><a href=\"#数据清洗\" class=\"headerlink\" title=\"数据清洗\"></a>数据清洗</h3><ul>\n<li>对数据中的单位进行统一</li>\n<li>去掉重复数据、噪声和数据缺失</li>\n</ul>\n<h3 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h3><ul>\n<li>从哪些特征对进行机器学习是有用的；人工设计或自动选择</li>\n</ul>\n<h3 id=\"模型选择\"><a href=\"#模型选择\" class=\"headerlink\" title=\"模型选择\"></a>模型选择</h3><ul>\n<li>根据问题选择模型，聚类还是分类，回归还是分类</li>\n</ul>\n<h3 id=\"模型训练和测试\"><a href=\"#模型训练和测试\" class=\"headerlink\" title=\"模型训练和测试\"></a>模型训练和测试</h3><ul>\n<li>把数据集合分为训练集和测试集，用训练集训练模型，训练完成后用测试集测试模型的精度。(测试集必须是模型没有见过的数据)</li>\n</ul>\n<h3 id=\"模型性能评估与优化\"><a href=\"#模型性能评估与优化\" class=\"headerlink\" title=\"模型性能评估与优化\"></a>模型性能评估与优化</h3><ul>\n<li>训练时长，训练数据是否足够，是否能满足需要</li>\n</ul>\n<h3 id=\"模型使用\"><a href=\"#模型使用\" class=\"headerlink\" title=\"模型使用\"></a>模型使用</h3><ul>\n<li>训练好的模型进行存储，以备下次使用</li>\n</ul>\n<h2 id=\"机器如何学习\"><a href=\"#机器如何学习\" class=\"headerlink\" title=\"机器如何学习\"></a>机器如何学习</h2><h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p>例如：<br>用一维线性回归举例:</p>\n<p> $$ y = kx+b $$</p>\n<p>x就是我们的input，y就是我们的label，我们首先给算法一定的(x,y)数据，算法拟合出来一条直线方程，当我们再输入x数据时，模型能够预测出相应的y。这就是一个简单的有监督机器学习例子。但是，机器怎么知道哪个k和b是最好的呢？</p>\n<p>也就是说，我们需要用一个指标来衡量模型和数据的拟合程度，而模型的预测值和真实值的差，我们叫做<strong>损失函数</strong>。在这里，我们训练的一个线性回归模型，可以是让MSE(均方误差)最小，MSE在这里被称为这个回归模型的<strong>损失函数</strong>，它代表了预测值与真实值的偏离程度。而我们机器学习的过程就是通过改变k和b使得<strong>损失函数</strong>取得<strong>最小值</strong>。<br>$$<br>L_{MSE}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}<br>$$<br>这里的m表示数据个数。</p>\n<p>除此之外，对于<strong>二分类</strong>问题来说，常用的损失函数是二元交叉熵损失(Logistic损失):</p>\n<p>$$<br>L_{\\text {logistic }}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}[-y_{i} \\log \\hat{y}<em>{i}-\\left(1-y</em>{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)]<br>$$</p>\n<p>这里的 $\\hat{y}$ 代表预测y=1的概率。</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>我们知道了我们的目标是让k和b最小，那我们怎么实现呢？接下来我们来看一下它的解决方案——使用<strong>梯度下降</strong>算法来更新参数。</p>\n<p><img src=\"/asset/gradient.jpeg\" alt=\"梯度下降\"></p>\n<p>这里 $\\theta_0$ 和 $\\theta_1$ 分别代表k和b。</p>\n<p>我们从一个随机的k和b出发，沿着向下最快的路径行走，直到达到最低点，即此时k和b收敛于一个定值，这个定值就是我们想要得到的使得损失函数最小的值。</p>\n<h2 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h2><p>本文使用scikit-learn实现一个线性回归模型举例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression <span class=\"comment\">#从sklearn引入线性回归模型</span></span><br><span class=\"line\">model=LinearRegression()                          <span class=\"comment\">#声明线性回归模型</span></span><br><span class=\"line\">model.fit(X_train,Y_train)                        <span class=\"comment\">#用训练数据训练模型</span></span><br><span class=\"line\">Y_pred=model.predict(X_test)                      <span class=\"comment\">#用模型对测试数据做预测</span></span><br><span class=\"line\">print(Y_pred)                                     <span class=\"comment\">#输出预测结果</span></span><br></pre></td></tr></table></figure>\n<p>常用的API有：</p>\n<table>\n  <tr>\n    <th>API</th>\n    <th>解释</th>\n  </tr>\n  <tr>\n    <td>fit(X_train,Y_train)</td>\n    <td>训练模型</td>\n  </tr>\n    <tr>\n    <td>predict(X_test)</td>\n    <td>预测测试数据的结果</td>\n  </tr>\n    <tr>\n    <td>score(X_test,Y_test)</td>\n    <td>测试预测数据的score(例如正确率)</td>\n  </tr>\n</table>\n\n<p>这里只是展示了很少很少的API，还有很多非常非常实用的API及教程参见<a href=\"https://sklearn.apachecn.org/\">官方文档</a></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>1.获取数据</li>\n<li>2.处理数据(80%的时间)</li>\n<li>3.训练模型(20%的时间)</li>\n<li>4.进行预测</li>\n<li>5.观察结果，不满意则重复2.3.4.步，满意则保存模型</li>\n</ul>\n","site":{"data":{}},"excerpt":"<h2 id=\"机器学习是什么\"><a href=\"#机器学习是什么\" class=\"headerlink\" title=\"机器学习是什么\"></a>机器学习是什么</h2><h3 id=\"定义\"><a href=\"#定义\" class=\"headerlink\" title=\"定义\"></a>定义</h3><blockquote>\n<p><a href=\"https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\">wiki</a>：机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。</p>\n</blockquote>\n<blockquote>\n<p>Arthur samuel：机器学习是在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域。</p>\n</blockquote>\n<p>一个计算机程序，针对某个特定任务，从历史数据学习，并且越做越好。</p>","more":"<p>让我<a href=\"https://www.jianshu.com/p/25ef14c072ad\">引用</a>一张图片来说明：<br><img src=\"/asset/whatisml.png\" alt=\"机器学习过程\"><br>注意：这里是用有监督模型举例(后面会对这个名词进一步解释)</p>\n<h3 id=\"针对机器学习最重要的内容\"><a href=\"#针对机器学习最重要的内容\" class=\"headerlink\" title=\"针对机器学习最重要的内容\"></a>针对机器学习最重要的内容</h3><ul>\n<li>数据：经验只有转化为了计算机能够理解的数据，才能让计算机从中学习。谁的数据量大、质量高，谁就占据了机器学习和人工只能领域最有利的资本。</li>\n<li>模型：即算法，有了数据之后，可以设计模型，通过数据来训练这个模型。这个模型就是机器学习的核心，作为用来产生决策的中枢。</li>\n</ul>\n<h3 id=\"数据的分类\"><a href=\"#数据的分类\" class=\"headerlink\" title=\"数据的分类\"></a>数据的分类</h3><ul>\n<li>结构化数据———储存在数据库中的数据</li>\n<li>非结构化数据——-语音信号、图像图形、自然语言</li>\n</ul>\n<h2 id=\"机器学习能干什么\"><a href=\"#机器学习能干什么\" class=\"headerlink\" title=\"机器学习能干什么\"></a>机器学习能干什么</h2><ul>\n<li>语音识别、机器翻译(微软Cortana、苹果Siri、科大讯飞、谷歌翻译)</li>\n<li>人脸识别(微信、支付宝、宿舍门禁)</li>\n<li>量化交易(预测股市)</li>\n<li>房价预测</li>\n<li>推荐系统(淘宝、京东、抖音)</li>\n<li>医生/老中医</li>\n<li>解微分方程、不定积分(见：<a href=\"https://zhuanlan.zhihu.com/p/98174049?utm_source=zhihu&utm_medium=social&utm_oi=667848254054731776\">AI拿下高数一血，求解微分方程、不定积分只需1秒，成绩远超Matlab</a>)</li>\n<li>寻找淹没在背景噪声中的小信号(<a href=\"https://www.kaggle.com/c/higgs-boson\">Higgs Boson Machine Learning Challenge</a>、引力波信号、引力透镜参数预测)</li>\n</ul>\n<h2 id=\"好用的机器学习库以及书籍推荐\"><a href=\"#好用的机器学习库以及书籍推荐\" class=\"headerlink\" title=\"好用的机器学习库以及书籍推荐\"></a>好用的机器学习库以及书籍推荐</h2><p>scikit-learn：最有名且易用好上手，广泛作为机器学习的入门库(Python)<br>书籍：<img src=\"/asset/scikit.png\" alt=\"scikit-learn机器学习\"></p>\n<h2 id=\"机器学习的分类\"><a href=\"#机器学习的分类\" class=\"headerlink\" title=\"机器学习的分类\"></a>机器学习的分类</h2><p>按照模型的学习方式，我们可以分为如下几类：</p>\n<h3 id=\"有监督学习\"><a href=\"#有监督学习\" class=\"headerlink\" title=\"有监督学习\"></a>有监督学习</h3><p>对于数据集中的每一条数据，我们在把它交给算法前就有了相应的“正确答案”，我们的算法就是在基于这些我们人为给定的“正确答案”在做预测。</p>\n<p>有监督学习的任务一般是回归或者分类问题：</p>\n<ul>\n<li>回归：线性回归<br>比如通过商品房的地段、高度、外形、面积、采光面积等参数来预测商品房价格。预测结果是一个连续的值。</li>\n<li>分类：支持向量机(SVM)、决策树、逻辑回归、朴素贝叶斯、KNN<br>比如通过西瓜的颜色、大小、重量、花纹等等预测西瓜甜不甜。<br>请注意，这里我们预测的输出只包括甜和不甜，是一个离散值，这是一个二分类的问题。<br>反之，若是我们输出的是西瓜介于0到1之间的的甜度(0是一点都不甜，1是超级甜)，那这个分类问题就转化为了回归问题。</li>\n</ul>\n<h3 id=\"无监督学习\"><a href=\"#无监督学习\" class=\"headerlink\" title=\"无监督学习\"></a>无监督学习</h3><p>对于数据集中的所有数据，他们没有一个相应的“正确答案”。算法要做的是利用算法自动的将数据归类，也叫做<strong>聚类</strong>。</p>\n<ul>\n<li>KMEAN</li>\n</ul>\n<h3 id=\"半监督学习\"><a href=\"#半监督学习\" class=\"headerlink\" title=\"半监督学习\"></a>半监督学习</h3><p>介于监督学习和无监督学习之间的一种方式。即一部分数据有标记，一部分数据没有标记。</p>\n<h3 id=\"增强学习\"><a href=\"#增强学习\" class=\"headerlink\" title=\"增强学习\"></a>增强学习</h3><p>增强学习是一种有反馈的学习方式。</p>\n<p>例子：贪吃蛇问题<br>一个N×N的格子里，定义贪吃蛇每一步上下左右随机行走，碰到墙或者自身则得到负反馈，吃到果子得到正反馈，在训练很多轮以后，贪吃蛇就学会了如何躲避墙和自身去吃果子。</p>\n<h2 id=\"机器学习的一般步骤\"><a href=\"#机器学习的一般步骤\" class=\"headerlink\" title=\"机器学习的一般步骤\"></a>机器学习的一般步骤</h2><h3 id=\"数据采集和标记\"><a href=\"#数据采集和标记\" class=\"headerlink\" title=\"数据采集和标记\"></a>数据采集和标记</h3><ul>\n<li>构建数据集：收集尽可能的多的特征，给出数据标记（人工或自动）</li>\n</ul>\n<p>预测房价：面积大小、房间数、地段、楼龄等；<br>芝麻信用：海量的用户交易数据；</p>\n<h3 id=\"数据清洗\"><a href=\"#数据清洗\" class=\"headerlink\" title=\"数据清洗\"></a>数据清洗</h3><ul>\n<li>对数据中的单位进行统一</li>\n<li>去掉重复数据、噪声和数据缺失</li>\n</ul>\n<h3 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h3><ul>\n<li>从哪些特征对进行机器学习是有用的；人工设计或自动选择</li>\n</ul>\n<h3 id=\"模型选择\"><a href=\"#模型选择\" class=\"headerlink\" title=\"模型选择\"></a>模型选择</h3><ul>\n<li>根据问题选择模型，聚类还是分类，回归还是分类</li>\n</ul>\n<h3 id=\"模型训练和测试\"><a href=\"#模型训练和测试\" class=\"headerlink\" title=\"模型训练和测试\"></a>模型训练和测试</h3><ul>\n<li>把数据集合分为训练集和测试集，用训练集训练模型，训练完成后用测试集测试模型的精度。(测试集必须是模型没有见过的数据)</li>\n</ul>\n<h3 id=\"模型性能评估与优化\"><a href=\"#模型性能评估与优化\" class=\"headerlink\" title=\"模型性能评估与优化\"></a>模型性能评估与优化</h3><ul>\n<li>训练时长，训练数据是否足够，是否能满足需要</li>\n</ul>\n<h3 id=\"模型使用\"><a href=\"#模型使用\" class=\"headerlink\" title=\"模型使用\"></a>模型使用</h3><ul>\n<li>训练好的模型进行存储，以备下次使用</li>\n</ul>\n<h2 id=\"机器如何学习\"><a href=\"#机器如何学习\" class=\"headerlink\" title=\"机器如何学习\"></a>机器如何学习</h2><h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p>例如：<br>用一维线性回归举例:</p>\n<p> $$ y = kx+b $$</p>\n<p>x就是我们的input，y就是我们的label，我们首先给算法一定的(x,y)数据，算法拟合出来一条直线方程，当我们再输入x数据时，模型能够预测出相应的y。这就是一个简单的有监督机器学习例子。但是，机器怎么知道哪个k和b是最好的呢？</p>\n<p>也就是说，我们需要用一个指标来衡量模型和数据的拟合程度，而模型的预测值和真实值的差，我们叫做<strong>损失函数</strong>。在这里，我们训练的一个线性回归模型，可以是让MSE(均方误差)最小，MSE在这里被称为这个回归模型的<strong>损失函数</strong>，它代表了预测值与真实值的偏离程度。而我们机器学习的过程就是通过改变k和b使得<strong>损失函数</strong>取得<strong>最小值</strong>。<br>$$<br>L_{MSE}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}<br>$$<br>这里的m表示数据个数。</p>\n<p>除此之外，对于<strong>二分类</strong>问题来说，常用的损失函数是二元交叉熵损失(Logistic损失):</p>\n<p>$$<br>L_{\\text {logistic }}(\\hat{y}, y)=\\frac{1}{m} \\sum_{i=1}^{m}[-y_{i} \\log \\hat{y}<em>{i}-\\left(1-y</em>{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)]<br>$$</p>\n<p>这里的 $\\hat{y}$ 代表预测y=1的概率。</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>我们知道了我们的目标是让k和b最小，那我们怎么实现呢？接下来我们来看一下它的解决方案——使用<strong>梯度下降</strong>算法来更新参数。</p>\n<p><img src=\"/asset/gradient.jpeg\" alt=\"梯度下降\"></p>\n<p>这里 $\\theta_0$ 和 $\\theta_1$ 分别代表k和b。</p>\n<p>我们从一个随机的k和b出发，沿着向下最快的路径行走，直到达到最低点，即此时k和b收敛于一个定值，这个定值就是我们想要得到的使得损失函数最小的值。</p>\n<h2 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h2><p>本文使用scikit-learn实现一个线性回归模型举例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression <span class=\"comment\">#从sklearn引入线性回归模型</span></span><br><span class=\"line\">model=LinearRegression()                          <span class=\"comment\">#声明线性回归模型</span></span><br><span class=\"line\">model.fit(X_train,Y_train)                        <span class=\"comment\">#用训练数据训练模型</span></span><br><span class=\"line\">Y_pred=model.predict(X_test)                      <span class=\"comment\">#用模型对测试数据做预测</span></span><br><span class=\"line\">print(Y_pred)                                     <span class=\"comment\">#输出预测结果</span></span><br></pre></td></tr></table></figure>\n<p>常用的API有：</p>\n<table>\n  <tr>\n    <th>API</th>\n    <th>解释</th>\n  </tr>\n  <tr>\n    <td>fit(X_train,Y_train)</td>\n    <td>训练模型</td>\n  </tr>\n    <tr>\n    <td>predict(X_test)</td>\n    <td>预测测试数据的结果</td>\n  </tr>\n    <tr>\n    <td>score(X_test,Y_test)</td>\n    <td>测试预测数据的score(例如正确率)</td>\n  </tr>\n</table>\n\n<p>这里只是展示了很少很少的API，还有很多非常非常实用的API及教程参见<a href=\"https://sklearn.apachecn.org/\">官方文档</a></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>1.获取数据</li>\n<li>2.处理数据(80%的时间)</li>\n<li>3.训练模型(20%的时间)</li>\n<li>4.进行预测</li>\n<li>5.观察结果，不满意则重复2.3.4.步，满意则保存模型</li>\n</ul>"},{"title":"First Poem","date":"2019-12-11T16:45:00.000Z","cover":"/asset/lost1.jpg","_content":"<font face=\"Times New Roman\" size=6>LOST</font>  \n<br>\n<font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;----Evie</font>\n<!--more-->\n<br>\n\n<font face=\"Ink Free\" size=5>\nChilled night, chilly light, <br>\nWind rolling sky, <br>\nCloud over head. <br>\nAshes tangled my hair, <br>\nMind lost from ear. <br>\n<br>\nRunning from sunlight, <br>\nThe music ignite a fire, <br>\nLit a face in the dusky theater air; <br>\nThe dance stepped the melody <br>\nInto my eyes gently. <br>\nTears shining <br> \nIn the reflected screen light, <br>\nHeart flipping <br> \nOver the woebegone rejoicing <br>\nWith the beauty of art. <br>\n<br>\nI've lost my blue <br>\nFrom their bloody mouth. <br>\nTaping up and down, <br>\nTheir tip of tongue fan a fame. <br>\nWhen there is no longer a pure face <br>\nBut tech race on the screen, <br>\nThey stop looking into themselves. <br>\n<br>\nI've shut my soul from heat of lies, <br>\nBut lost in my chilling heart. <br>\nLooking outside <br> \nFrom the window of fear, <br>\nEveryone seems to be tired. <br>\nWalking over again <br>\nLike nothing changed there, <br>\nI want my feeling back. <br>\n\n</font> \n","source":"_posts/poem1.md","raw":"---\ntitle: First Poem\ndate: 2019-12-12 00:45:00\ncategories: 诗集\ntags: poem\ncover: /asset/lost1.jpg\n---\n<font face=\"Times New Roman\" size=6>LOST</font>  \n<br>\n<font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;----Evie</font>\n<!--more-->\n<br>\n\n<font face=\"Ink Free\" size=5>\nChilled night, chilly light, <br>\nWind rolling sky, <br>\nCloud over head. <br>\nAshes tangled my hair, <br>\nMind lost from ear. <br>\n<br>\nRunning from sunlight, <br>\nThe music ignite a fire, <br>\nLit a face in the dusky theater air; <br>\nThe dance stepped the melody <br>\nInto my eyes gently. <br>\nTears shining <br> \nIn the reflected screen light, <br>\nHeart flipping <br> \nOver the woebegone rejoicing <br>\nWith the beauty of art. <br>\n<br>\nI've lost my blue <br>\nFrom their bloody mouth. <br>\nTaping up and down, <br>\nTheir tip of tongue fan a fame. <br>\nWhen there is no longer a pure face <br>\nBut tech race on the screen, <br>\nThey stop looking into themselves. <br>\n<br>\nI've shut my soul from heat of lies, <br>\nBut lost in my chilling heart. <br>\nLooking outside <br> \nFrom the window of fear, <br>\nEveryone seems to be tired. <br>\nWalking over again <br>\nLike nothing changed there, <br>\nI want my feeling back. <br>\n\n</font> \n","slug":"poem1","published":1,"updated":"2021-02-01T15:49:04.944Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9ei000blsvwg14zdntt","content":"<p><font face=\"Times New Roman\" size=6>LOST</font><br><br><br><font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie</font></p>\n<a id=\"more\"></a>\n<br>\n\n<font face=\"Ink Free\" size=5>\nChilled night, chilly light, <br>\nWind rolling sky, <br>\nCloud over head. <br>\nAshes tangled my hair, <br>\nMind lost from ear. <br>\n<br>\nRunning from sunlight, <br>\nThe music ignite a fire, <br>\nLit a face in the dusky theater air; <br>\nThe dance stepped the melody <br>\nInto my eyes gently. <br>\nTears shining <br> \nIn the reflected screen light, <br>\nHeart flipping <br> \nOver the woebegone rejoicing <br>\nWith the beauty of art. <br>\n<br>\nI've lost my blue <br>\nFrom their bloody mouth. <br>\nTaping up and down, <br>\nTheir tip of tongue fan a fame. <br>\nWhen there is no longer a pure face <br>\nBut tech race on the screen, <br>\nThey stop looking into themselves. <br>\n<br>\nI've shut my soul from heat of lies, <br>\nBut lost in my chilling heart. <br>\nLooking outside <br> \nFrom the window of fear, <br>\nEveryone seems to be tired. <br>\nWalking over again <br>\nLike nothing changed there, <br>\nI want my feeling back. <br>\n\n</font> \n","site":{"data":{}},"excerpt":"<p><font face=\"Times New Roman\" size=6>LOST</font><br><br><br><font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie</font></p>","more":"<br>\n\n<font face=\"Ink Free\" size=5>\nChilled night, chilly light, <br>\nWind rolling sky, <br>\nCloud over head. <br>\nAshes tangled my hair, <br>\nMind lost from ear. <br>\n<br>\nRunning from sunlight, <br>\nThe music ignite a fire, <br>\nLit a face in the dusky theater air; <br>\nThe dance stepped the melody <br>\nInto my eyes gently. <br>\nTears shining <br> \nIn the reflected screen light, <br>\nHeart flipping <br> \nOver the woebegone rejoicing <br>\nWith the beauty of art. <br>\n<br>\nI've lost my blue <br>\nFrom their bloody mouth. <br>\nTaping up and down, <br>\nTheir tip of tongue fan a fame. <br>\nWhen there is no longer a pure face <br>\nBut tech race on the screen, <br>\nThey stop looking into themselves. <br>\n<br>\nI've shut my soul from heat of lies, <br>\nBut lost in my chilling heart. <br>\nLooking outside <br> \nFrom the window of fear, <br>\nEveryone seems to be tired. <br>\nWalking over again <br>\nLike nothing changed there, <br>\nI want my feeling back. <br>\n\n</font>"},{"title":"价值投资思路","date":"2021-03-15T09:23:28.000Z","_content":"\n## 太长不想看版\n\n- 预期收益：长期年化10%以上\n- 持股时间：基本面不变化除非较为高估不卖\n- 买入：在合理的估值买入伟大的公司\n- 持有：顶住波动，坚定持有\n- 卖出：在较为高估时卖出\n\n<!--more-->\n\n## 买入篇\n\n### 找到一个好的企业\n\n什么是好？\n\n“好”这个概念很模糊，我们怎样去定义一个企业是好还是不好呢？从投资的角度来看，股价持续上涨的企业就是好企业，**而股票的价格又是围绕着企业的价值上下波动的**，虽然市场对于价格与价值背离的忍耐度可能有时候比较强，但没有业绩支撑的股票在爆炒之后总是一地鸡毛。如此，我们的问题就变成了怎么找到一个越来越有价值的企业，具体一点可以表现为利润越来越多的企业。\n\n企业利润的增长来源于以下几个方面，首先可以来源于**需求的增加**，如果该企业处于一个蓬勃发展的行业，市场需求不断扩大，即使该企业在与竞争对手的竞争中不占优势也能不断实现业绩的增长，就想俗话说：风口来了猪都能飞起来。在这种情况下，单纯的扩大产能就可以实现利润的增长，但一旦市场需求达到饱和，该种企业也就从风口掉了下来。利润增长的第二个来源是**价格的提升或者成本的降低**，价格的提升一般在垄断类型的企业中比较常见，而成本的降低往往来自于产能的增加、人员结构优化、原材料获取成本降低等。优秀的企业往往在市占率提高的同时还能提高自己的净利率。\n\n从利润的角度我们明白了什么是一个好的企业，但还有一个至关重要的问题就是**利润的增长能否持续**，茅台之所以神是因为他的高增长持续了近二十年而看不到减缓的迹象。我们接下来进行探究什么样的企业可以常年稳定持续增长。首先，企业处于**弱周期行业**，比如消费行业、医疗行业，不管这个世界发生了什么事情，我们总是要吃饭，总是要看病的。其次，**垄断地位**，比如茅台之于白酒、腾讯之于互联网。垄断意味着提价权，意味着更高的毛利，意味着很难被别的公司超越。\n\n### 买入的时机\n\n虽然，按照上述逻辑，我们很轻易的找到了很多公司，但如果在极为高估时顶着60倍的PE买入茅台，难道要拿住5年等待解套吗？因此，在一个合理的估值买入伟大的公司，这才是我们要做的事情。\n\n如何估值是一个很难的问题，最简单的方法是看公司的市盈率PE的高低在历史上处于什么样的水平，一般来说，熊市时整个市场的股票PE都偏低，而牛市整个市场的股票PE都偏高。所以最理想的情况就是在熊市低估时买进，忽略期间的巨额波动，在牛市高估时卖出。\n\n## 持有篇\n\n价值投资的最难之处在于持有，就算在一个较为低估的时候买入，一切逻辑都十分看好，但当很多股票走出翻倍行情时，你持有的股票震荡了两年，你的心态会有变化吗？你明知道拿住就可以翻倍，但每次打开账户，里面的钱越来越少，你的心态会有变化吗？好不容易等来一次拉升，赚了20个点，但由于还没有高估，你没有卖出，几天后又跌了回去，你的心态又会怎么变？\n\n所以，想做价值投资，最简单的方法就是少看盘，当在估值低位分批进场后，卸载软件吧！\n\n## 卖出篇\n\n如何衡量牛市到了哪个阶段，就看身边人的狂热情况，大量的韭菜冲入股市，新闻联播开始播报股市气象，大妈喊出股市10000点的口号之后，就可以卖了。虽然可能卖出之后股票每天都还在涨，千万不要追高买回来！学会空仓，等到股市崩盘，一地鸡毛，没有人再谈论股票时，你就可以继续开始你的下一次轮回！\n","source":"_posts/value-investment.md","raw":"---\ntitle: 价值投资思路\ndate: 2021-03-15 17:23:28\ntags: 投资\ncategories: 投资\n---\n\n## 太长不想看版\n\n- 预期收益：长期年化10%以上\n- 持股时间：基本面不变化除非较为高估不卖\n- 买入：在合理的估值买入伟大的公司\n- 持有：顶住波动，坚定持有\n- 卖出：在较为高估时卖出\n\n<!--more-->\n\n## 买入篇\n\n### 找到一个好的企业\n\n什么是好？\n\n“好”这个概念很模糊，我们怎样去定义一个企业是好还是不好呢？从投资的角度来看，股价持续上涨的企业就是好企业，**而股票的价格又是围绕着企业的价值上下波动的**，虽然市场对于价格与价值背离的忍耐度可能有时候比较强，但没有业绩支撑的股票在爆炒之后总是一地鸡毛。如此，我们的问题就变成了怎么找到一个越来越有价值的企业，具体一点可以表现为利润越来越多的企业。\n\n企业利润的增长来源于以下几个方面，首先可以来源于**需求的增加**，如果该企业处于一个蓬勃发展的行业，市场需求不断扩大，即使该企业在与竞争对手的竞争中不占优势也能不断实现业绩的增长，就想俗话说：风口来了猪都能飞起来。在这种情况下，单纯的扩大产能就可以实现利润的增长，但一旦市场需求达到饱和，该种企业也就从风口掉了下来。利润增长的第二个来源是**价格的提升或者成本的降低**，价格的提升一般在垄断类型的企业中比较常见，而成本的降低往往来自于产能的增加、人员结构优化、原材料获取成本降低等。优秀的企业往往在市占率提高的同时还能提高自己的净利率。\n\n从利润的角度我们明白了什么是一个好的企业，但还有一个至关重要的问题就是**利润的增长能否持续**，茅台之所以神是因为他的高增长持续了近二十年而看不到减缓的迹象。我们接下来进行探究什么样的企业可以常年稳定持续增长。首先，企业处于**弱周期行业**，比如消费行业、医疗行业，不管这个世界发生了什么事情，我们总是要吃饭，总是要看病的。其次，**垄断地位**，比如茅台之于白酒、腾讯之于互联网。垄断意味着提价权，意味着更高的毛利，意味着很难被别的公司超越。\n\n### 买入的时机\n\n虽然，按照上述逻辑，我们很轻易的找到了很多公司，但如果在极为高估时顶着60倍的PE买入茅台，难道要拿住5年等待解套吗？因此，在一个合理的估值买入伟大的公司，这才是我们要做的事情。\n\n如何估值是一个很难的问题，最简单的方法是看公司的市盈率PE的高低在历史上处于什么样的水平，一般来说，熊市时整个市场的股票PE都偏低，而牛市整个市场的股票PE都偏高。所以最理想的情况就是在熊市低估时买进，忽略期间的巨额波动，在牛市高估时卖出。\n\n## 持有篇\n\n价值投资的最难之处在于持有，就算在一个较为低估的时候买入，一切逻辑都十分看好，但当很多股票走出翻倍行情时，你持有的股票震荡了两年，你的心态会有变化吗？你明知道拿住就可以翻倍，但每次打开账户，里面的钱越来越少，你的心态会有变化吗？好不容易等来一次拉升，赚了20个点，但由于还没有高估，你没有卖出，几天后又跌了回去，你的心态又会怎么变？\n\n所以，想做价值投资，最简单的方法就是少看盘，当在估值低位分批进场后，卸载软件吧！\n\n## 卖出篇\n\n如何衡量牛市到了哪个阶段，就看身边人的狂热情况，大量的韭菜冲入股市，新闻联播开始播报股市气象，大妈喊出股市10000点的口号之后，就可以卖了。虽然可能卖出之后股票每天都还在涨，千万不要追高买回来！学会空仓，等到股市崩盘，一地鸡毛，没有人再谈论股票时，你就可以继续开始你的下一次轮回！\n","slug":"value-investment","published":1,"updated":"2021-03-15T09:28:27.514Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9ej000flsvw7lncfw2y","content":"<h2 id=\"太长不想看版\"><a href=\"#太长不想看版\" class=\"headerlink\" title=\"太长不想看版\"></a>太长不想看版</h2><ul>\n<li>预期收益：长期年化10%以上</li>\n<li>持股时间：基本面不变化除非较为高估不卖</li>\n<li>买入：在合理的估值买入伟大的公司</li>\n<li>持有：顶住波动，坚定持有</li>\n<li>卖出：在较为高估时卖出</li>\n</ul>\n<a id=\"more\"></a>\n\n<h2 id=\"买入篇\"><a href=\"#买入篇\" class=\"headerlink\" title=\"买入篇\"></a>买入篇</h2><h3 id=\"找到一个好的企业\"><a href=\"#找到一个好的企业\" class=\"headerlink\" title=\"找到一个好的企业\"></a>找到一个好的企业</h3><p>什么是好？</p>\n<p>“好”这个概念很模糊，我们怎样去定义一个企业是好还是不好呢？从投资的角度来看，股价持续上涨的企业就是好企业，<strong>而股票的价格又是围绕着企业的价值上下波动的</strong>，虽然市场对于价格与价值背离的忍耐度可能有时候比较强，但没有业绩支撑的股票在爆炒之后总是一地鸡毛。如此，我们的问题就变成了怎么找到一个越来越有价值的企业，具体一点可以表现为利润越来越多的企业。</p>\n<p>企业利润的增长来源于以下几个方面，首先可以来源于<strong>需求的增加</strong>，如果该企业处于一个蓬勃发展的行业，市场需求不断扩大，即使该企业在与竞争对手的竞争中不占优势也能不断实现业绩的增长，就想俗话说：风口来了猪都能飞起来。在这种情况下，单纯的扩大产能就可以实现利润的增长，但一旦市场需求达到饱和，该种企业也就从风口掉了下来。利润增长的第二个来源是<strong>价格的提升或者成本的降低</strong>，价格的提升一般在垄断类型的企业中比较常见，而成本的降低往往来自于产能的增加、人员结构优化、原材料获取成本降低等。优秀的企业往往在市占率提高的同时还能提高自己的净利率。</p>\n<p>从利润的角度我们明白了什么是一个好的企业，但还有一个至关重要的问题就是<strong>利润的增长能否持续</strong>，茅台之所以神是因为他的高增长持续了近二十年而看不到减缓的迹象。我们接下来进行探究什么样的企业可以常年稳定持续增长。首先，企业处于<strong>弱周期行业</strong>，比如消费行业、医疗行业，不管这个世界发生了什么事情，我们总是要吃饭，总是要看病的。其次，<strong>垄断地位</strong>，比如茅台之于白酒、腾讯之于互联网。垄断意味着提价权，意味着更高的毛利，意味着很难被别的公司超越。</p>\n<h3 id=\"买入的时机\"><a href=\"#买入的时机\" class=\"headerlink\" title=\"买入的时机\"></a>买入的时机</h3><p>虽然，按照上述逻辑，我们很轻易的找到了很多公司，但如果在极为高估时顶着60倍的PE买入茅台，难道要拿住5年等待解套吗？因此，在一个合理的估值买入伟大的公司，这才是我们要做的事情。</p>\n<p>如何估值是一个很难的问题，最简单的方法是看公司的市盈率PE的高低在历史上处于什么样的水平，一般来说，熊市时整个市场的股票PE都偏低，而牛市整个市场的股票PE都偏高。所以最理想的情况就是在熊市低估时买进，忽略期间的巨额波动，在牛市高估时卖出。</p>\n<h2 id=\"持有篇\"><a href=\"#持有篇\" class=\"headerlink\" title=\"持有篇\"></a>持有篇</h2><p>价值投资的最难之处在于持有，就算在一个较为低估的时候买入，一切逻辑都十分看好，但当很多股票走出翻倍行情时，你持有的股票震荡了两年，你的心态会有变化吗？你明知道拿住就可以翻倍，但每次打开账户，里面的钱越来越少，你的心态会有变化吗？好不容易等来一次拉升，赚了20个点，但由于还没有高估，你没有卖出，几天后又跌了回去，你的心态又会怎么变？</p>\n<p>所以，想做价值投资，最简单的方法就是少看盘，当在估值低位分批进场后，卸载软件吧！</p>\n<h2 id=\"卖出篇\"><a href=\"#卖出篇\" class=\"headerlink\" title=\"卖出篇\"></a>卖出篇</h2><p>如何衡量牛市到了哪个阶段，就看身边人的狂热情况，大量的韭菜冲入股市，新闻联播开始播报股市气象，大妈喊出股市10000点的口号之后，就可以卖了。虽然可能卖出之后股票每天都还在涨，千万不要追高买回来！学会空仓，等到股市崩盘，一地鸡毛，没有人再谈论股票时，你就可以继续开始你的下一次轮回！</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"太长不想看版\"><a href=\"#太长不想看版\" class=\"headerlink\" title=\"太长不想看版\"></a>太长不想看版</h2><ul>\n<li>预期收益：长期年化10%以上</li>\n<li>持股时间：基本面不变化除非较为高估不卖</li>\n<li>买入：在合理的估值买入伟大的公司</li>\n<li>持有：顶住波动，坚定持有</li>\n<li>卖出：在较为高估时卖出</li>\n</ul>","more":"<h2 id=\"买入篇\"><a href=\"#买入篇\" class=\"headerlink\" title=\"买入篇\"></a>买入篇</h2><h3 id=\"找到一个好的企业\"><a href=\"#找到一个好的企业\" class=\"headerlink\" title=\"找到一个好的企业\"></a>找到一个好的企业</h3><p>什么是好？</p>\n<p>“好”这个概念很模糊，我们怎样去定义一个企业是好还是不好呢？从投资的角度来看，股价持续上涨的企业就是好企业，<strong>而股票的价格又是围绕着企业的价值上下波动的</strong>，虽然市场对于价格与价值背离的忍耐度可能有时候比较强，但没有业绩支撑的股票在爆炒之后总是一地鸡毛。如此，我们的问题就变成了怎么找到一个越来越有价值的企业，具体一点可以表现为利润越来越多的企业。</p>\n<p>企业利润的增长来源于以下几个方面，首先可以来源于<strong>需求的增加</strong>，如果该企业处于一个蓬勃发展的行业，市场需求不断扩大，即使该企业在与竞争对手的竞争中不占优势也能不断实现业绩的增长，就想俗话说：风口来了猪都能飞起来。在这种情况下，单纯的扩大产能就可以实现利润的增长，但一旦市场需求达到饱和，该种企业也就从风口掉了下来。利润增长的第二个来源是<strong>价格的提升或者成本的降低</strong>，价格的提升一般在垄断类型的企业中比较常见，而成本的降低往往来自于产能的增加、人员结构优化、原材料获取成本降低等。优秀的企业往往在市占率提高的同时还能提高自己的净利率。</p>\n<p>从利润的角度我们明白了什么是一个好的企业，但还有一个至关重要的问题就是<strong>利润的增长能否持续</strong>，茅台之所以神是因为他的高增长持续了近二十年而看不到减缓的迹象。我们接下来进行探究什么样的企业可以常年稳定持续增长。首先，企业处于<strong>弱周期行业</strong>，比如消费行业、医疗行业，不管这个世界发生了什么事情，我们总是要吃饭，总是要看病的。其次，<strong>垄断地位</strong>，比如茅台之于白酒、腾讯之于互联网。垄断意味着提价权，意味着更高的毛利，意味着很难被别的公司超越。</p>\n<h3 id=\"买入的时机\"><a href=\"#买入的时机\" class=\"headerlink\" title=\"买入的时机\"></a>买入的时机</h3><p>虽然，按照上述逻辑，我们很轻易的找到了很多公司，但如果在极为高估时顶着60倍的PE买入茅台，难道要拿住5年等待解套吗？因此，在一个合理的估值买入伟大的公司，这才是我们要做的事情。</p>\n<p>如何估值是一个很难的问题，最简单的方法是看公司的市盈率PE的高低在历史上处于什么样的水平，一般来说，熊市时整个市场的股票PE都偏低，而牛市整个市场的股票PE都偏高。所以最理想的情况就是在熊市低估时买进，忽略期间的巨额波动，在牛市高估时卖出。</p>\n<h2 id=\"持有篇\"><a href=\"#持有篇\" class=\"headerlink\" title=\"持有篇\"></a>持有篇</h2><p>价值投资的最难之处在于持有，就算在一个较为低估的时候买入，一切逻辑都十分看好，但当很多股票走出翻倍行情时，你持有的股票震荡了两年，你的心态会有变化吗？你明知道拿住就可以翻倍，但每次打开账户，里面的钱越来越少，你的心态会有变化吗？好不容易等来一次拉升，赚了20个点，但由于还没有高估，你没有卖出，几天后又跌了回去，你的心态又会怎么变？</p>\n<p>所以，想做价值投资，最简单的方法就是少看盘，当在估值低位分批进场后，卸载软件吧！</p>\n<h2 id=\"卖出篇\"><a href=\"#卖出篇\" class=\"headerlink\" title=\"卖出篇\"></a>卖出篇</h2><p>如何衡量牛市到了哪个阶段，就看身边人的狂热情况，大量的韭菜冲入股市，新闻联播开始播报股市气象，大妈喊出股市10000点的口号之后，就可以卖了。虽然可能卖出之后股票每天都还在涨，千万不要追高买回来！学会空仓，等到股市崩盘，一地鸡毛，没有人再谈论股票时，你就可以继续开始你的下一次轮回！</p>"},{"title":"Second Poem","date":"2019-12-13T19:58:00.000Z","cover":"/asset/pounding.jpg","_content":"<font face=\"Times New Roman\" size=6>POUNDING</font>  \n<br>\n<font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;----Evie</font>\n<!--more-->\n<br>\n\n<font face=\"Ink Free\" size=5>\nPounding, pounding heart,<br>\nWhy you pump so hard?<br>\nFeeling a bunch of vessels<br>\nExtending to my roots,<br>\nWhere blood poured<br>\nAfter running across<br>\nWvery finger of my nerves.<br>\n<br>\nWho am I<br>\nBesides a cluster of cells,<br>\nTramping in the sounds...<br>\n<br>\n\n</font> \n","source":"_posts/poem2.md","raw":"---\ntitle: Second Poem\ndate: 2019-12-14 3:58:00\ncategories: 诗集\ntags: poem\ncover: /asset/pounding.jpg\n---\n<font face=\"Times New Roman\" size=6>POUNDING</font>  \n<br>\n<font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;----Evie</font>\n<!--more-->\n<br>\n\n<font face=\"Ink Free\" size=5>\nPounding, pounding heart,<br>\nWhy you pump so hard?<br>\nFeeling a bunch of vessels<br>\nExtending to my roots,<br>\nWhere blood poured<br>\nAfter running across<br>\nWvery finger of my nerves.<br>\n<br>\nWho am I<br>\nBesides a cluster of cells,<br>\nTramping in the sounds...<br>\n<br>\n\n</font> \n","slug":"poem2","published":1,"updated":"2021-02-01T15:49:11.330Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9el000glsvwffyq1t04","content":"<p><font face=\"Times New Roman\" size=6>POUNDING</font><br><br><br><font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie</font></p>\n<a id=\"more\"></a>\n<br>\n\n<font face=\"Ink Free\" size=5>\nPounding, pounding heart,<br>\nWhy you pump so hard?<br>\nFeeling a bunch of vessels<br>\nExtending to my roots,<br>\nWhere blood poured<br>\nAfter running across<br>\nWvery finger of my nerves.<br>\n<br>\nWho am I<br>\nBesides a cluster of cells,<br>\nTramping in the sounds...<br>\n<br>\n\n</font> \n","site":{"data":{}},"excerpt":"<p><font face=\"Times New Roman\" size=6>POUNDING</font><br><br><br><font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie</font></p>","more":"<br>\n\n<font face=\"Ink Free\" size=5>\nPounding, pounding heart,<br>\nWhy you pump so hard?<br>\nFeeling a bunch of vessels<br>\nExtending to my roots,<br>\nWhere blood poured<br>\nAfter running across<br>\nWvery finger of my nerves.<br>\n<br>\nWho am I<br>\nBesides a cluster of cells,<br>\nTramping in the sounds...<br>\n<br>\n\n</font>"},{"title":"2019大数据算法赛总结","date":"2019-12-09T02:32:45.000Z","cover":"/asset/Big-Data.jpg","toc":true,"_content":"之前一直对机器学习比较感兴趣，大三开学开始学习深度学习，正巧赶上全国高校计算机挑战赛开赛，就报名了我参加的第一个大数据方向的竞赛。这个比赛入门门槛很低，随便找个模型调用一下sklearn库就能跑出结果，但想争夺一个好的名次还是不太容易。\n\n## 概览\n\n报名费用：¥150（每队）\n\n参加人数： 290队\n\n最终排名： 62\n\n最终得分（AUC）：0.733\n\n使用模型：XGBOOST\n\n<!--more-->\n\n## 数据预处理\n\n### 数据概况\n\ntrain.csv文件中包括60000条记录，每一条记录包括Data字段和如下几列，每一列的含义如下:\n\n![train.csv](/asset/train.png)\n\n### 缺失数据\n\n我们用Python读取数据首先查看有无缺失数据的情况，发现每一列都有60000条数据没有缺失值，有可能数据已经进行了缺失值的填充，后面再进行判断。\n\n![missing_value](/asset/missing_value.png)\n\n### 粗略观察数据和label的关系\n\n我们对每一列和label的关系进行绘图，观察他们的相关程度和是否有线性或者非线性的关系，例如下图：\n\n<img src=\"/asset/A2.png\" width=\"60%\"><img src=\"/asset/E2.png\" width=\"60%\">\n<img src=\"/asset/E3.png\" width=\"60%\"><img src=\"/asset/E4.png\" width=\"60%\">\n\n扔掉基本没有差别的列：['E3','E5', 'E8', 'E9','E19','E26''E29']\n\nC2也要被丢掉，因为C2中绝大部分值都是一个相同的值，占比82.5%而其他的值的占比最多的才0.1%。\n\n## CTR（Click-Through-Rate）问题长尾效应\n\nCTR问题有的特点就是海量离散特征，且存在长尾效应（Long Tail Effect）：\n\n即80%的效益来自于20%的特征，也就是说一个特征可能有成千上万种取值，但只有取值的频率最高的那些是最有用的，如果我们不对此进行处理，长尾的现象可能会降低我们模型的表现。我们由此对类别特征做了label-encoder，按照出现频率对频率较高的特征进行映射，并将出现频率很低的那些都映射为一个相同的值。\n\n![Long_tail](/asset/Long_tail.png)\n\n## 分箱处理\n\n有时候我们对有连续意思的离散特征做分箱处理会使模型的表现提高。\n\n### 举个例子\n\n早上7点26分和早上7点27分对于是否会点击一个广告基本没有任何区别，但早上和晚上可能会有所不同。但如果直接把没有分箱的数据送给模型的话，模型会认为早上7点26分和早上7点27分就是两个完全不同的时间，因此我们可以简单的把时间信息按照早上、中午、下午、晚上划分，或者更细致一点的考虑的话，晚上人们的时间规划可能很不一样因此也可以划分的更细致一点，比如晚上按小时划分。\n\n### 分箱方法\n\n虽然目前有很多做分箱的理论，但因为我们需要做分箱的特征只有一列，且我们需要分箱的数据在密度图下可以明显看出区别，故我们人工估计待分箱的箱数以及分箱的位置，我们认为该种方式比聚类方法分箱有直接、暴力、可解释性强等优点。\n\n经过尝试，我们发现对C3进行分箱是一个很好的选择，它能显著提高我们模型的表现：\n\n![C3](/asset/C3box.png)\n\n## 不平衡数据/Ensemble\n\n通过观察给出的数据我们发现这是一个不平衡数据集。有83%的人不会点击广告，而只有17%的人会点击广告。就算我们训练一个只会输出不会点击广告的模型，那我们也会有83%的正确率，我们试过欠采样和SMOTE但效果都不佳，因此我们采用Ensemble方式，抽取全部点击广告的人且对不会点击广告的数据进行欠采样使得抽出的数据集平衡，有放回的抽取N次，训练N个模型取平均得到最后的结果，模型的表现会有很大的提升。\n\n## 模型选择\n\n我们尝试了LR，RandomForest，RandomForest+LR，GBDT，GBDT+LR，XGBOOST，XGBOOST+LR,MLR，DeepFM等模型\n\n### 第一版模型\n\n无脑One-hot，发现随着特征列数的增加，所有的模型都趋于一个相同的值，AUC：0.710\n\n### 第二版模型\n\n进行长尾数据的处理，缩减特征的维度再做One-hot，模型效果有所改善，但改善不大忘记最后结果了\n\n### 第三版模型\n\n细致处理长尾数据，不要One-hot，XGBOOST细致调参，得分AUC上了0.720\n\n### 第四版模型\n\n用DeepFM做相同的事情，重新调参，AUC：0.720\n\n### 第五版模型\n\n细致处理长尾数据，不要One-hot，做Ensemble，XGBOOST的AUC上了0.728，DeepFM的AUC上了0.721\n\n### 第六版模型\n\n将XGBOOST和DeepFM做模型融合（结果取平均），成功将AUC提到0.731\n\n### 最终模型\n\n细致选取使用的特征，细致处理长尾数据，不要One-hot，做Ensemble，对C3进行分箱，训练50个单独的XGBOOST做平均，AUC：0.733\n","source":"_posts/summury-of-big-data-competition.md","raw":"---\ntitle: 2019大数据算法赛总结\ndate: 2019-12-09 10:32:45\ncategories: 机器学习\ntags: [bigdata,总结,机器学习]\ncover: /asset/Big-Data.jpg\ntoc: true\n---\n之前一直对机器学习比较感兴趣，大三开学开始学习深度学习，正巧赶上全国高校计算机挑战赛开赛，就报名了我参加的第一个大数据方向的竞赛。这个比赛入门门槛很低，随便找个模型调用一下sklearn库就能跑出结果，但想争夺一个好的名次还是不太容易。\n\n## 概览\n\n报名费用：¥150（每队）\n\n参加人数： 290队\n\n最终排名： 62\n\n最终得分（AUC）：0.733\n\n使用模型：XGBOOST\n\n<!--more-->\n\n## 数据预处理\n\n### 数据概况\n\ntrain.csv文件中包括60000条记录，每一条记录包括Data字段和如下几列，每一列的含义如下:\n\n![train.csv](/asset/train.png)\n\n### 缺失数据\n\n我们用Python读取数据首先查看有无缺失数据的情况，发现每一列都有60000条数据没有缺失值，有可能数据已经进行了缺失值的填充，后面再进行判断。\n\n![missing_value](/asset/missing_value.png)\n\n### 粗略观察数据和label的关系\n\n我们对每一列和label的关系进行绘图，观察他们的相关程度和是否有线性或者非线性的关系，例如下图：\n\n<img src=\"/asset/A2.png\" width=\"60%\"><img src=\"/asset/E2.png\" width=\"60%\">\n<img src=\"/asset/E3.png\" width=\"60%\"><img src=\"/asset/E4.png\" width=\"60%\">\n\n扔掉基本没有差别的列：['E3','E5', 'E8', 'E9','E19','E26''E29']\n\nC2也要被丢掉，因为C2中绝大部分值都是一个相同的值，占比82.5%而其他的值的占比最多的才0.1%。\n\n## CTR（Click-Through-Rate）问题长尾效应\n\nCTR问题有的特点就是海量离散特征，且存在长尾效应（Long Tail Effect）：\n\n即80%的效益来自于20%的特征，也就是说一个特征可能有成千上万种取值，但只有取值的频率最高的那些是最有用的，如果我们不对此进行处理，长尾的现象可能会降低我们模型的表现。我们由此对类别特征做了label-encoder，按照出现频率对频率较高的特征进行映射，并将出现频率很低的那些都映射为一个相同的值。\n\n![Long_tail](/asset/Long_tail.png)\n\n## 分箱处理\n\n有时候我们对有连续意思的离散特征做分箱处理会使模型的表现提高。\n\n### 举个例子\n\n早上7点26分和早上7点27分对于是否会点击一个广告基本没有任何区别，但早上和晚上可能会有所不同。但如果直接把没有分箱的数据送给模型的话，模型会认为早上7点26分和早上7点27分就是两个完全不同的时间，因此我们可以简单的把时间信息按照早上、中午、下午、晚上划分，或者更细致一点的考虑的话，晚上人们的时间规划可能很不一样因此也可以划分的更细致一点，比如晚上按小时划分。\n\n### 分箱方法\n\n虽然目前有很多做分箱的理论，但因为我们需要做分箱的特征只有一列，且我们需要分箱的数据在密度图下可以明显看出区别，故我们人工估计待分箱的箱数以及分箱的位置，我们认为该种方式比聚类方法分箱有直接、暴力、可解释性强等优点。\n\n经过尝试，我们发现对C3进行分箱是一个很好的选择，它能显著提高我们模型的表现：\n\n![C3](/asset/C3box.png)\n\n## 不平衡数据/Ensemble\n\n通过观察给出的数据我们发现这是一个不平衡数据集。有83%的人不会点击广告，而只有17%的人会点击广告。就算我们训练一个只会输出不会点击广告的模型，那我们也会有83%的正确率，我们试过欠采样和SMOTE但效果都不佳，因此我们采用Ensemble方式，抽取全部点击广告的人且对不会点击广告的数据进行欠采样使得抽出的数据集平衡，有放回的抽取N次，训练N个模型取平均得到最后的结果，模型的表现会有很大的提升。\n\n## 模型选择\n\n我们尝试了LR，RandomForest，RandomForest+LR，GBDT，GBDT+LR，XGBOOST，XGBOOST+LR,MLR，DeepFM等模型\n\n### 第一版模型\n\n无脑One-hot，发现随着特征列数的增加，所有的模型都趋于一个相同的值，AUC：0.710\n\n### 第二版模型\n\n进行长尾数据的处理，缩减特征的维度再做One-hot，模型效果有所改善，但改善不大忘记最后结果了\n\n### 第三版模型\n\n细致处理长尾数据，不要One-hot，XGBOOST细致调参，得分AUC上了0.720\n\n### 第四版模型\n\n用DeepFM做相同的事情，重新调参，AUC：0.720\n\n### 第五版模型\n\n细致处理长尾数据，不要One-hot，做Ensemble，XGBOOST的AUC上了0.728，DeepFM的AUC上了0.721\n\n### 第六版模型\n\n将XGBOOST和DeepFM做模型融合（结果取平均），成功将AUC提到0.731\n\n### 最终模型\n\n细致选取使用的特征，细致处理长尾数据，不要One-hot，做Ensemble，对C3进行分箱，训练50个单独的XGBOOST做平均，AUC：0.733\n","slug":"summury-of-big-data-competition","published":1,"updated":"2021-02-01T15:09:47.061Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9em000llsvw2y3t17i7","content":"<p>之前一直对机器学习比较感兴趣，大三开学开始学习深度学习，正巧赶上全国高校计算机挑战赛开赛，就报名了我参加的第一个大数据方向的竞赛。这个比赛入门门槛很低，随便找个模型调用一下sklearn库就能跑出结果，但想争夺一个好的名次还是不太容易。</p>\n<h2 id=\"概览\"><a href=\"#概览\" class=\"headerlink\" title=\"概览\"></a>概览</h2><p>报名费用：¥150（每队）</p>\n<p>参加人数： 290队</p>\n<p>最终排名： 62</p>\n<p>最终得分（AUC）：0.733</p>\n<p>使用模型：XGBOOST</p>\n<a id=\"more\"></a>\n\n<h2 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h2><h3 id=\"数据概况\"><a href=\"#数据概况\" class=\"headerlink\" title=\"数据概况\"></a>数据概况</h3><p>train.csv文件中包括60000条记录，每一条记录包括Data字段和如下几列，每一列的含义如下:</p>\n<p><img src=\"/asset/train.png\" alt=\"train.csv\"></p>\n<h3 id=\"缺失数据\"><a href=\"#缺失数据\" class=\"headerlink\" title=\"缺失数据\"></a>缺失数据</h3><p>我们用Python读取数据首先查看有无缺失数据的情况，发现每一列都有60000条数据没有缺失值，有可能数据已经进行了缺失值的填充，后面再进行判断。</p>\n<p><img src=\"/asset/missing_value.png\" alt=\"missing_value\"></p>\n<h3 id=\"粗略观察数据和label的关系\"><a href=\"#粗略观察数据和label的关系\" class=\"headerlink\" title=\"粗略观察数据和label的关系\"></a>粗略观察数据和label的关系</h3><p>我们对每一列和label的关系进行绘图，观察他们的相关程度和是否有线性或者非线性的关系，例如下图：</p>\n<p><img src=\"/asset/A2.png\" width=\"60%\"><img src=\"/asset/E2.png\" width=\"60%\"><br><img src=\"/asset/E3.png\" width=\"60%\"><img src=\"/asset/E4.png\" width=\"60%\"></p>\n<p>扔掉基本没有差别的列：[‘E3’,’E5’, ‘E8’, ‘E9’,’E19’,’E26’’E29’]</p>\n<p>C2也要被丢掉，因为C2中绝大部分值都是一个相同的值，占比82.5%而其他的值的占比最多的才0.1%。</p>\n<h2 id=\"CTR（Click-Through-Rate）问题长尾效应\"><a href=\"#CTR（Click-Through-Rate）问题长尾效应\" class=\"headerlink\" title=\"CTR（Click-Through-Rate）问题长尾效应\"></a>CTR（Click-Through-Rate）问题长尾效应</h2><p>CTR问题有的特点就是海量离散特征，且存在长尾效应（Long Tail Effect）：</p>\n<p>即80%的效益来自于20%的特征，也就是说一个特征可能有成千上万种取值，但只有取值的频率最高的那些是最有用的，如果我们不对此进行处理，长尾的现象可能会降低我们模型的表现。我们由此对类别特征做了label-encoder，按照出现频率对频率较高的特征进行映射，并将出现频率很低的那些都映射为一个相同的值。</p>\n<p><img src=\"/asset/Long_tail.png\" alt=\"Long_tail\"></p>\n<h2 id=\"分箱处理\"><a href=\"#分箱处理\" class=\"headerlink\" title=\"分箱处理\"></a>分箱处理</h2><p>有时候我们对有连续意思的离散特征做分箱处理会使模型的表现提高。</p>\n<h3 id=\"举个例子\"><a href=\"#举个例子\" class=\"headerlink\" title=\"举个例子\"></a>举个例子</h3><p>早上7点26分和早上7点27分对于是否会点击一个广告基本没有任何区别，但早上和晚上可能会有所不同。但如果直接把没有分箱的数据送给模型的话，模型会认为早上7点26分和早上7点27分就是两个完全不同的时间，因此我们可以简单的把时间信息按照早上、中午、下午、晚上划分，或者更细致一点的考虑的话，晚上人们的时间规划可能很不一样因此也可以划分的更细致一点，比如晚上按小时划分。</p>\n<h3 id=\"分箱方法\"><a href=\"#分箱方法\" class=\"headerlink\" title=\"分箱方法\"></a>分箱方法</h3><p>虽然目前有很多做分箱的理论，但因为我们需要做分箱的特征只有一列，且我们需要分箱的数据在密度图下可以明显看出区别，故我们人工估计待分箱的箱数以及分箱的位置，我们认为该种方式比聚类方法分箱有直接、暴力、可解释性强等优点。</p>\n<p>经过尝试，我们发现对C3进行分箱是一个很好的选择，它能显著提高我们模型的表现：</p>\n<p><img src=\"/asset/C3box.png\" alt=\"C3\"></p>\n<h2 id=\"不平衡数据-Ensemble\"><a href=\"#不平衡数据-Ensemble\" class=\"headerlink\" title=\"不平衡数据/Ensemble\"></a>不平衡数据/Ensemble</h2><p>通过观察给出的数据我们发现这是一个不平衡数据集。有83%的人不会点击广告，而只有17%的人会点击广告。就算我们训练一个只会输出不会点击广告的模型，那我们也会有83%的正确率，我们试过欠采样和SMOTE但效果都不佳，因此我们采用Ensemble方式，抽取全部点击广告的人且对不会点击广告的数据进行欠采样使得抽出的数据集平衡，有放回的抽取N次，训练N个模型取平均得到最后的结果，模型的表现会有很大的提升。</p>\n<h2 id=\"模型选择\"><a href=\"#模型选择\" class=\"headerlink\" title=\"模型选择\"></a>模型选择</h2><p>我们尝试了LR，RandomForest，RandomForest+LR，GBDT，GBDT+LR，XGBOOST，XGBOOST+LR,MLR，DeepFM等模型</p>\n<h3 id=\"第一版模型\"><a href=\"#第一版模型\" class=\"headerlink\" title=\"第一版模型\"></a>第一版模型</h3><p>无脑One-hot，发现随着特征列数的增加，所有的模型都趋于一个相同的值，AUC：0.710</p>\n<h3 id=\"第二版模型\"><a href=\"#第二版模型\" class=\"headerlink\" title=\"第二版模型\"></a>第二版模型</h3><p>进行长尾数据的处理，缩减特征的维度再做One-hot，模型效果有所改善，但改善不大忘记最后结果了</p>\n<h3 id=\"第三版模型\"><a href=\"#第三版模型\" class=\"headerlink\" title=\"第三版模型\"></a>第三版模型</h3><p>细致处理长尾数据，不要One-hot，XGBOOST细致调参，得分AUC上了0.720</p>\n<h3 id=\"第四版模型\"><a href=\"#第四版模型\" class=\"headerlink\" title=\"第四版模型\"></a>第四版模型</h3><p>用DeepFM做相同的事情，重新调参，AUC：0.720</p>\n<h3 id=\"第五版模型\"><a href=\"#第五版模型\" class=\"headerlink\" title=\"第五版模型\"></a>第五版模型</h3><p>细致处理长尾数据，不要One-hot，做Ensemble，XGBOOST的AUC上了0.728，DeepFM的AUC上了0.721</p>\n<h3 id=\"第六版模型\"><a href=\"#第六版模型\" class=\"headerlink\" title=\"第六版模型\"></a>第六版模型</h3><p>将XGBOOST和DeepFM做模型融合（结果取平均），成功将AUC提到0.731</p>\n<h3 id=\"最终模型\"><a href=\"#最终模型\" class=\"headerlink\" title=\"最终模型\"></a>最终模型</h3><p>细致选取使用的特征，细致处理长尾数据，不要One-hot，做Ensemble，对C3进行分箱，训练50个单独的XGBOOST做平均，AUC：0.733</p>\n","site":{"data":{}},"excerpt":"<p>之前一直对机器学习比较感兴趣，大三开学开始学习深度学习，正巧赶上全国高校计算机挑战赛开赛，就报名了我参加的第一个大数据方向的竞赛。这个比赛入门门槛很低，随便找个模型调用一下sklearn库就能跑出结果，但想争夺一个好的名次还是不太容易。</p>\n<h2 id=\"概览\"><a href=\"#概览\" class=\"headerlink\" title=\"概览\"></a>概览</h2><p>报名费用：¥150（每队）</p>\n<p>参加人数： 290队</p>\n<p>最终排名： 62</p>\n<p>最终得分（AUC）：0.733</p>\n<p>使用模型：XGBOOST</p>","more":"<h2 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h2><h3 id=\"数据概况\"><a href=\"#数据概况\" class=\"headerlink\" title=\"数据概况\"></a>数据概况</h3><p>train.csv文件中包括60000条记录，每一条记录包括Data字段和如下几列，每一列的含义如下:</p>\n<p><img src=\"/asset/train.png\" alt=\"train.csv\"></p>\n<h3 id=\"缺失数据\"><a href=\"#缺失数据\" class=\"headerlink\" title=\"缺失数据\"></a>缺失数据</h3><p>我们用Python读取数据首先查看有无缺失数据的情况，发现每一列都有60000条数据没有缺失值，有可能数据已经进行了缺失值的填充，后面再进行判断。</p>\n<p><img src=\"/asset/missing_value.png\" alt=\"missing_value\"></p>\n<h3 id=\"粗略观察数据和label的关系\"><a href=\"#粗略观察数据和label的关系\" class=\"headerlink\" title=\"粗略观察数据和label的关系\"></a>粗略观察数据和label的关系</h3><p>我们对每一列和label的关系进行绘图，观察他们的相关程度和是否有线性或者非线性的关系，例如下图：</p>\n<p><img src=\"/asset/A2.png\" width=\"60%\"><img src=\"/asset/E2.png\" width=\"60%\"><br><img src=\"/asset/E3.png\" width=\"60%\"><img src=\"/asset/E4.png\" width=\"60%\"></p>\n<p>扔掉基本没有差别的列：[‘E3’,’E5’, ‘E8’, ‘E9’,’E19’,’E26’’E29’]</p>\n<p>C2也要被丢掉，因为C2中绝大部分值都是一个相同的值，占比82.5%而其他的值的占比最多的才0.1%。</p>\n<h2 id=\"CTR（Click-Through-Rate）问题长尾效应\"><a href=\"#CTR（Click-Through-Rate）问题长尾效应\" class=\"headerlink\" title=\"CTR（Click-Through-Rate）问题长尾效应\"></a>CTR（Click-Through-Rate）问题长尾效应</h2><p>CTR问题有的特点就是海量离散特征，且存在长尾效应（Long Tail Effect）：</p>\n<p>即80%的效益来自于20%的特征，也就是说一个特征可能有成千上万种取值，但只有取值的频率最高的那些是最有用的，如果我们不对此进行处理，长尾的现象可能会降低我们模型的表现。我们由此对类别特征做了label-encoder，按照出现频率对频率较高的特征进行映射，并将出现频率很低的那些都映射为一个相同的值。</p>\n<p><img src=\"/asset/Long_tail.png\" alt=\"Long_tail\"></p>\n<h2 id=\"分箱处理\"><a href=\"#分箱处理\" class=\"headerlink\" title=\"分箱处理\"></a>分箱处理</h2><p>有时候我们对有连续意思的离散特征做分箱处理会使模型的表现提高。</p>\n<h3 id=\"举个例子\"><a href=\"#举个例子\" class=\"headerlink\" title=\"举个例子\"></a>举个例子</h3><p>早上7点26分和早上7点27分对于是否会点击一个广告基本没有任何区别，但早上和晚上可能会有所不同。但如果直接把没有分箱的数据送给模型的话，模型会认为早上7点26分和早上7点27分就是两个完全不同的时间，因此我们可以简单的把时间信息按照早上、中午、下午、晚上划分，或者更细致一点的考虑的话，晚上人们的时间规划可能很不一样因此也可以划分的更细致一点，比如晚上按小时划分。</p>\n<h3 id=\"分箱方法\"><a href=\"#分箱方法\" class=\"headerlink\" title=\"分箱方法\"></a>分箱方法</h3><p>虽然目前有很多做分箱的理论，但因为我们需要做分箱的特征只有一列，且我们需要分箱的数据在密度图下可以明显看出区别，故我们人工估计待分箱的箱数以及分箱的位置，我们认为该种方式比聚类方法分箱有直接、暴力、可解释性强等优点。</p>\n<p>经过尝试，我们发现对C3进行分箱是一个很好的选择，它能显著提高我们模型的表现：</p>\n<p><img src=\"/asset/C3box.png\" alt=\"C3\"></p>\n<h2 id=\"不平衡数据-Ensemble\"><a href=\"#不平衡数据-Ensemble\" class=\"headerlink\" title=\"不平衡数据/Ensemble\"></a>不平衡数据/Ensemble</h2><p>通过观察给出的数据我们发现这是一个不平衡数据集。有83%的人不会点击广告，而只有17%的人会点击广告。就算我们训练一个只会输出不会点击广告的模型，那我们也会有83%的正确率，我们试过欠采样和SMOTE但效果都不佳，因此我们采用Ensemble方式，抽取全部点击广告的人且对不会点击广告的数据进行欠采样使得抽出的数据集平衡，有放回的抽取N次，训练N个模型取平均得到最后的结果，模型的表现会有很大的提升。</p>\n<h2 id=\"模型选择\"><a href=\"#模型选择\" class=\"headerlink\" title=\"模型选择\"></a>模型选择</h2><p>我们尝试了LR，RandomForest，RandomForest+LR，GBDT，GBDT+LR，XGBOOST，XGBOOST+LR,MLR，DeepFM等模型</p>\n<h3 id=\"第一版模型\"><a href=\"#第一版模型\" class=\"headerlink\" title=\"第一版模型\"></a>第一版模型</h3><p>无脑One-hot，发现随着特征列数的增加，所有的模型都趋于一个相同的值，AUC：0.710</p>\n<h3 id=\"第二版模型\"><a href=\"#第二版模型\" class=\"headerlink\" title=\"第二版模型\"></a>第二版模型</h3><p>进行长尾数据的处理，缩减特征的维度再做One-hot，模型效果有所改善，但改善不大忘记最后结果了</p>\n<h3 id=\"第三版模型\"><a href=\"#第三版模型\" class=\"headerlink\" title=\"第三版模型\"></a>第三版模型</h3><p>细致处理长尾数据，不要One-hot，XGBOOST细致调参，得分AUC上了0.720</p>\n<h3 id=\"第四版模型\"><a href=\"#第四版模型\" class=\"headerlink\" title=\"第四版模型\"></a>第四版模型</h3><p>用DeepFM做相同的事情，重新调参，AUC：0.720</p>\n<h3 id=\"第五版模型\"><a href=\"#第五版模型\" class=\"headerlink\" title=\"第五版模型\"></a>第五版模型</h3><p>细致处理长尾数据，不要One-hot，做Ensemble，XGBOOST的AUC上了0.728，DeepFM的AUC上了0.721</p>\n<h3 id=\"第六版模型\"><a href=\"#第六版模型\" class=\"headerlink\" title=\"第六版模型\"></a>第六版模型</h3><p>将XGBOOST和DeepFM做模型融合（结果取平均），成功将AUC提到0.731</p>\n<h3 id=\"最终模型\"><a href=\"#最终模型\" class=\"headerlink\" title=\"最终模型\"></a>最终模型</h3><p>细致选取使用的特征，细致处理长尾数据，不要One-hot，做Ensemble，对C3进行分箱，训练50个单独的XGBOOST做平均，AUC：0.733</p>"},{"title":"Third Poem","date":"2019-12-17T14:16:00.000Z","cover":"/asset/hiding.jpg","_content":"<font face=\"Times New Roman\" size=6>HIDING</font>  \n<br>\n<font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;----Evie</font>\n<!--more-->\n<br>\n\n<font face=\"Ink Free\" size=5>\nBooming, booming bus.<br>\nRains dropped on the windows.<br>\nOutside is the dusky sky<br>\nWith dark clouds hiding somewhere.<br>\nInside is a complacent heart<br>\nWithout being complimenting.<br>\n<br>\nHiding the disappointment <br>\nFrom a beautiful fragile mind,<br>\nWhere intellect should always stand.<br>\nI show my courage<br>\nto my cold little heart,<br>\nwhich might suffer <br>\nfrom wild heat of tricking,<br>\nbut would not go into hiding<br>\nAny more.<br>\n<br>\n\n</font> \n","source":"_posts/poem3.md","raw":"---\ntitle: Third Poem\ndate: 2019-12-17 22:16:00\ncategories: 诗集\ntags: poem\ncover: /asset/hiding.jpg\n---\n<font face=\"Times New Roman\" size=6>HIDING</font>  \n<br>\n<font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;----Evie</font>\n<!--more-->\n<br>\n\n<font face=\"Ink Free\" size=5>\nBooming, booming bus.<br>\nRains dropped on the windows.<br>\nOutside is the dusky sky<br>\nWith dark clouds hiding somewhere.<br>\nInside is a complacent heart<br>\nWithout being complimenting.<br>\n<br>\nHiding the disappointment <br>\nFrom a beautiful fragile mind,<br>\nWhere intellect should always stand.<br>\nI show my courage<br>\nto my cold little heart,<br>\nwhich might suffer <br>\nfrom wild heat of tricking,<br>\nbut would not go into hiding<br>\nAny more.<br>\n<br>\n\n</font> \n","slug":"poem3","published":1,"updated":"2021-02-01T15:49:15.421Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9ez001flsvw6ylbauix","content":"<p><font face=\"Times New Roman\" size=6>HIDING</font><br><br><br><font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie</font></p>\n<a id=\"more\"></a>\n<br>\n\n<font face=\"Ink Free\" size=5>\nBooming, booming bus.<br>\nRains dropped on the windows.<br>\nOutside is the dusky sky<br>\nWith dark clouds hiding somewhere.<br>\nInside is a complacent heart<br>\nWithout being complimenting.<br>\n<br>\nHiding the disappointment <br>\nFrom a beautiful fragile mind,<br>\nWhere intellect should always stand.<br>\nI show my courage<br>\nto my cold little heart,<br>\nwhich might suffer <br>\nfrom wild heat of tricking,<br>\nbut would not go into hiding<br>\nAny more.<br>\n<br>\n\n</font> \n","site":{"data":{}},"excerpt":"<p><font face=\"Times New Roman\" size=6>HIDING</font><br><br><br><font face=\"Monotype Corsiva\" size=6>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—-Evie</font></p>","more":"<br>\n\n<font face=\"Ink Free\" size=5>\nBooming, booming bus.<br>\nRains dropped on the windows.<br>\nOutside is the dusky sky<br>\nWith dark clouds hiding somewhere.<br>\nInside is a complacent heart<br>\nWithout being complimenting.<br>\n<br>\nHiding the disappointment <br>\nFrom a beautiful fragile mind,<br>\nWhere intellect should always stand.<br>\nI show my courage<br>\nto my cold little heart,<br>\nwhich might suffer <br>\nfrom wild heat of tricking,<br>\nbut would not go into hiding<br>\nAny more.<br>\n<br>\n\n</font>"},{"title":"Hexo+markdown开始博客写作","date":"2019-12-11T14:17:30.000Z","cover":"/asset/write.jpeg","toc":true,"_content":"## 目录\n\n- Hexo安装\n- Hexo使用\n- Markdown写作\n<!--more-->\n\n## Hexo安装\n\n引用[Hexo官方文档](https://hexo.io/zh-cn/docs/)：\n\n> **什么是 Hexo?**\nHexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。\n\n> **安装** \n安装 Hexo 只需几分钟时间，若您在安装过程中遇到问题或无法找到解决方式，请提交问题，我们会尽力解决您的问题。\n\n> **安装前提**\n安装 Hexo 相当简单，只需要先安装下列应用程序即可：\nNode.js (Node.js 版本需不低于 8.10，建议使用 Node.js 10.0 及以上版本)\nGit\n>\n>如果您的电脑中已经安装上述必备程序，那么恭喜您！你可以直接前往 安装 Hexo 步骤。\n如果您的电脑中尚未安装所需要的程序，请根据以下安装指示完成安装。\n\n> **安装 Git**\n>- Windows：下载并安装 git.\n>- Mac：使用 Homebrew, MacPorts 或者下载 安装程序。\n>- Linux (Ubuntu, Debian)：sudo apt-get install git-core\n>- Linux (Fedora, Red Hat, CentOS)：sudo yum install git-core\n\n> **安装 Node.js**\nNode.js 为大多数平台提供了官方的 [安装程序](https://nodejs.org/en/download/)。对于中国大陆地区用户，可以前往 [淘宝 Node.js 镜像](https://npm.taobao.org/mirrors/node) 下载。\n>\n>其它的安装方法：\n>\n>- Windows：通过 nvs（推荐）或者nvm 安装。\n>- Mac：使用 Homebrew 或 MacPorts 安装。\n>- Linux（DEB/RPM-based）：从 NodeSource 安装。\n>\n>其它：使用相应的软件包管理器进行安装，可以参考由 Node.js 提供的 指导\n对于 Mac 和 Linux 同样建议使用 nvs 或者 nvm，以避免可能会出现的权限问题。\n\n这里需要用到刚才安装的git，在git bush里进行如下操作：\n> **安装 Hexo**\n所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。\n>\n>$ npm install -g hexo-cli\n\n这样，Hexo就成功安装好了，下一步就可以着手在本地搭建自己的博客了。\n\n## Hexo使用\n\n**如果你已经有了一个建好的网站**，并且已经上传到了github，可将网站代码克隆到本地。\n\n在选定的文件夹下：\n\n```\n$ git clone git@github.com:heros979/My_blog.git\n```\nclone后面是你的github代码库的地址，示例是我的，你们要改成你们自己的。\n\n**如果你是第一次建立自己的网站**，在你想要的（随意哪个都行）文件夹里右键进入git bush，输入：\n```\n$ hexo init <folder>\n$ cd <folder>\n$ npm install\n```\n其中folder是你存放代码的文件夹名字\n\n好了，我们的文件夹里有了很多新的文件,如下：\n```\n.\n├── _config.yml\n├── package.json\n├── scaffolds\n├── source\n|   ├── _drafts\n|   └── _posts\n└── themes\n```\n\n其中 _config.yml 是网站的[配置](https://hexo.io/zh-cn/docs/configuration)信息，您可以在此配置大部分的参数。  \n如果是第一次建立的话，要在deploy处加入自己的github地址。例如我的：\n\n![deploy](/asset/deploy.png)\n\n我们要写的博客就放在 _post 文件夹里。在根目录可以使用如下方式新建post\n```\nhexo new <post-name>\n```\n\n我们在写完post后在根目录输入：\n```\nhexo clean\nhexo g\nhexo d\n```\n\n即可将我们的网站部署到VPS服务器和我们的github上。\n\n## Markdown写作\n\n打开我们刚才建立的post-name.md文件，我们要用markdown的语法来写这个页面，markdown是一个很简单的写作工具，也很好看，很实用。\n\n例子1：\n```\n*开辟鸿蒙，谁为情种？*                  #这是斜体\n**都只为风月情浓。**                    #这是加粗\n***奈何天，伤怀日，***                  #这是斜体加粗\n<font size=4>寂寥时，试遣愚衷。</font>  #自定义字体大小\n~~因此上，演出这~~                      #这是删除线\n> 怀金悼玉的《红楼梦》。                 # 这是引用\n```\n\n*开辟鸿蒙，谁为情种？*\n**都只为风月情浓。**\n***奈何天，伤怀日，***\n<font size=4>寂寥时，试遣愚衷。</font> \n~~因此上，演出这~~\n> 怀金悼玉的《红楼梦》。\n\n例子2：\n\n```\n## CALLED BACK  #这是标题，几个#就是几级标题\n\n<font face=\"微软雅黑\" size=6 color=#FF0000 >Just lost when I was saved!</font>  #修改字体字号颜色\n<table><tr><td bgcolor=orange>Just felt the world go by!</td></tr></table>     #修改背景色\nJust girt me for the onset with eternity,\nWhen breath blew back,\nAnd on the other side\nI heard recede the disappointed tide!\n\nTherefore, as one returned, I feel,\nOdd secrets of the line to tell!\nSome sailor, skirting foreign shores,\nSome pale reporter from the awful doors \nBefore the seal!\n\nNext time, to stay!\nNext time, the things to see\nBy ear unheard,\nUnscrutinized by eye.\n\nNext time, to tarry,\nWhile the ages steal,—\nSlow tramp the centuries,\nAnd the cycles wheel.\n```\n\n# CALLED BACK\n\n<font face=\"微软雅黑\" size=6 color=#FF0000 >Just lost when I was saved!</font>\n<table><tr><td bgcolor=orange>Just felt the world go by!</td></tr></table>\nJust girt me for the onset with eternity,\nWhen breath blew back,\nAnd on the other side\nI heard recede the disappointed tide!\n\nTherefore, as one returned, I feel,\nOdd secrets of the line to tell!\nSome sailor, skirting foreign shores,\nSome pale reporter from the awful doors \nBefore the seal!\n\nNext time, to stay!\nNext time, the things to see\nBy ear unheard,\nUnscrutinized by eye.\n\nNext time, to tarry,\nWhile the ages steal,—\nSlow tramp the centuries,\nAnd the cycles wheel.\n\n更多语法，比如标题、换行、注释、分割线、引用、代码块、列表、链接、表格、图片、流程图、LaTeX。\n\n![write](http://n.sinaimg.cn/translate/20171114/WsmA-fynstfh7830699.gif)\n\n参见：[Markdown快速入门](https://sspai.com/post/45816)，[Markdown语法(字体,样式,公式,背景,图表等)](https://blog.csdn.net/woswod/article/details/82753451)","source":"_posts/writing.md","raw":"---\ntitle: Hexo+markdown开始博客写作\ndate: 2019-12-11 22:17:30\ncategories: 博客\ntags: [hexo,markdown,写作,网站]\ncover: /asset/write.jpeg\ntoc: true\n---\n## 目录\n\n- Hexo安装\n- Hexo使用\n- Markdown写作\n<!--more-->\n\n## Hexo安装\n\n引用[Hexo官方文档](https://hexo.io/zh-cn/docs/)：\n\n> **什么是 Hexo?**\nHexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。\n\n> **安装** \n安装 Hexo 只需几分钟时间，若您在安装过程中遇到问题或无法找到解决方式，请提交问题，我们会尽力解决您的问题。\n\n> **安装前提**\n安装 Hexo 相当简单，只需要先安装下列应用程序即可：\nNode.js (Node.js 版本需不低于 8.10，建议使用 Node.js 10.0 及以上版本)\nGit\n>\n>如果您的电脑中已经安装上述必备程序，那么恭喜您！你可以直接前往 安装 Hexo 步骤。\n如果您的电脑中尚未安装所需要的程序，请根据以下安装指示完成安装。\n\n> **安装 Git**\n>- Windows：下载并安装 git.\n>- Mac：使用 Homebrew, MacPorts 或者下载 安装程序。\n>- Linux (Ubuntu, Debian)：sudo apt-get install git-core\n>- Linux (Fedora, Red Hat, CentOS)：sudo yum install git-core\n\n> **安装 Node.js**\nNode.js 为大多数平台提供了官方的 [安装程序](https://nodejs.org/en/download/)。对于中国大陆地区用户，可以前往 [淘宝 Node.js 镜像](https://npm.taobao.org/mirrors/node) 下载。\n>\n>其它的安装方法：\n>\n>- Windows：通过 nvs（推荐）或者nvm 安装。\n>- Mac：使用 Homebrew 或 MacPorts 安装。\n>- Linux（DEB/RPM-based）：从 NodeSource 安装。\n>\n>其它：使用相应的软件包管理器进行安装，可以参考由 Node.js 提供的 指导\n对于 Mac 和 Linux 同样建议使用 nvs 或者 nvm，以避免可能会出现的权限问题。\n\n这里需要用到刚才安装的git，在git bush里进行如下操作：\n> **安装 Hexo**\n所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。\n>\n>$ npm install -g hexo-cli\n\n这样，Hexo就成功安装好了，下一步就可以着手在本地搭建自己的博客了。\n\n## Hexo使用\n\n**如果你已经有了一个建好的网站**，并且已经上传到了github，可将网站代码克隆到本地。\n\n在选定的文件夹下：\n\n```\n$ git clone git@github.com:heros979/My_blog.git\n```\nclone后面是你的github代码库的地址，示例是我的，你们要改成你们自己的。\n\n**如果你是第一次建立自己的网站**，在你想要的（随意哪个都行）文件夹里右键进入git bush，输入：\n```\n$ hexo init <folder>\n$ cd <folder>\n$ npm install\n```\n其中folder是你存放代码的文件夹名字\n\n好了，我们的文件夹里有了很多新的文件,如下：\n```\n.\n├── _config.yml\n├── package.json\n├── scaffolds\n├── source\n|   ├── _drafts\n|   └── _posts\n└── themes\n```\n\n其中 _config.yml 是网站的[配置](https://hexo.io/zh-cn/docs/configuration)信息，您可以在此配置大部分的参数。  \n如果是第一次建立的话，要在deploy处加入自己的github地址。例如我的：\n\n![deploy](/asset/deploy.png)\n\n我们要写的博客就放在 _post 文件夹里。在根目录可以使用如下方式新建post\n```\nhexo new <post-name>\n```\n\n我们在写完post后在根目录输入：\n```\nhexo clean\nhexo g\nhexo d\n```\n\n即可将我们的网站部署到VPS服务器和我们的github上。\n\n## Markdown写作\n\n打开我们刚才建立的post-name.md文件，我们要用markdown的语法来写这个页面，markdown是一个很简单的写作工具，也很好看，很实用。\n\n例子1：\n```\n*开辟鸿蒙，谁为情种？*                  #这是斜体\n**都只为风月情浓。**                    #这是加粗\n***奈何天，伤怀日，***                  #这是斜体加粗\n<font size=4>寂寥时，试遣愚衷。</font>  #自定义字体大小\n~~因此上，演出这~~                      #这是删除线\n> 怀金悼玉的《红楼梦》。                 # 这是引用\n```\n\n*开辟鸿蒙，谁为情种？*\n**都只为风月情浓。**\n***奈何天，伤怀日，***\n<font size=4>寂寥时，试遣愚衷。</font> \n~~因此上，演出这~~\n> 怀金悼玉的《红楼梦》。\n\n例子2：\n\n```\n## CALLED BACK  #这是标题，几个#就是几级标题\n\n<font face=\"微软雅黑\" size=6 color=#FF0000 >Just lost when I was saved!</font>  #修改字体字号颜色\n<table><tr><td bgcolor=orange>Just felt the world go by!</td></tr></table>     #修改背景色\nJust girt me for the onset with eternity,\nWhen breath blew back,\nAnd on the other side\nI heard recede the disappointed tide!\n\nTherefore, as one returned, I feel,\nOdd secrets of the line to tell!\nSome sailor, skirting foreign shores,\nSome pale reporter from the awful doors \nBefore the seal!\n\nNext time, to stay!\nNext time, the things to see\nBy ear unheard,\nUnscrutinized by eye.\n\nNext time, to tarry,\nWhile the ages steal,—\nSlow tramp the centuries,\nAnd the cycles wheel.\n```\n\n# CALLED BACK\n\n<font face=\"微软雅黑\" size=6 color=#FF0000 >Just lost when I was saved!</font>\n<table><tr><td bgcolor=orange>Just felt the world go by!</td></tr></table>\nJust girt me for the onset with eternity,\nWhen breath blew back,\nAnd on the other side\nI heard recede the disappointed tide!\n\nTherefore, as one returned, I feel,\nOdd secrets of the line to tell!\nSome sailor, skirting foreign shores,\nSome pale reporter from the awful doors \nBefore the seal!\n\nNext time, to stay!\nNext time, the things to see\nBy ear unheard,\nUnscrutinized by eye.\n\nNext time, to tarry,\nWhile the ages steal,—\nSlow tramp the centuries,\nAnd the cycles wheel.\n\n更多语法，比如标题、换行、注释、分割线、引用、代码块、列表、链接、表格、图片、流程图、LaTeX。\n\n![write](http://n.sinaimg.cn/translate/20171114/WsmA-fynstfh7830699.gif)\n\n参见：[Markdown快速入门](https://sspai.com/post/45816)，[Markdown语法(字体,样式,公式,背景,图表等)](https://blog.csdn.net/woswod/article/details/82753451)","slug":"writing","published":1,"updated":"2021-02-01T15:09:50.448Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9ez001glsvwcf6z416b","content":"<h2 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h2><ul>\n<li>Hexo安装</li>\n<li>Hexo使用</li>\n<li>Markdown写作<a id=\"more\"></a>\n\n</li>\n</ul>\n<h2 id=\"Hexo安装\"><a href=\"#Hexo安装\" class=\"headerlink\" title=\"Hexo安装\"></a>Hexo安装</h2><p>引用<a href=\"https://hexo.io/zh-cn/docs/\">Hexo官方文档</a>：</p>\n<blockquote>\n<p><strong>什么是 Hexo?</strong><br>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p>\n</blockquote>\n<blockquote>\n<p><strong>安装</strong><br>安装 Hexo 只需几分钟时间，若您在安装过程中遇到问题或无法找到解决方式，请提交问题，我们会尽力解决您的问题。</p>\n</blockquote>\n<blockquote>\n<p><strong>安装前提</strong><br>安装 Hexo 相当简单，只需要先安装下列应用程序即可：<br>Node.js (Node.js 版本需不低于 8.10，建议使用 Node.js 10.0 及以上版本)<br>Git</p>\n<p>如果您的电脑中已经安装上述必备程序，那么恭喜您！你可以直接前往 安装 Hexo 步骤。<br>如果您的电脑中尚未安装所需要的程序，请根据以下安装指示完成安装。</p>\n</blockquote>\n<blockquote>\n<p><strong>安装 Git</strong></p>\n<ul>\n<li>Windows：下载并安装 git.</li>\n<li>Mac：使用 Homebrew, MacPorts 或者下载 安装程序。</li>\n<li>Linux (Ubuntu, Debian)：sudo apt-get install git-core</li>\n<li>Linux (Fedora, Red Hat, CentOS)：sudo yum install git-core</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>安装 Node.js</strong><br>Node.js 为大多数平台提供了官方的 <a href=\"https://nodejs.org/en/download/\">安装程序</a>。对于中国大陆地区用户，可以前往 <a href=\"https://npm.taobao.org/mirrors/node\">淘宝 Node.js 镜像</a> 下载。</p>\n<p>其它的安装方法：</p>\n<ul>\n<li>Windows：通过 nvs（推荐）或者nvm 安装。</li>\n<li>Mac：使用 Homebrew 或 MacPorts 安装。</li>\n<li>Linux（DEB/RPM-based）：从 NodeSource 安装。</li>\n</ul>\n<p>其它：使用相应的软件包管理器进行安装，可以参考由 Node.js 提供的 指导<br>对于 Mac 和 Linux 同样建议使用 nvs 或者 nvm，以避免可能会出现的权限问题。</p>\n</blockquote>\n<p>这里需要用到刚才安装的git，在git bush里进行如下操作：</p>\n<blockquote>\n<p><strong>安装 Hexo</strong><br>所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。</p>\n<p>$ npm install -g hexo-cli</p>\n</blockquote>\n<p>这样，Hexo就成功安装好了，下一步就可以着手在本地搭建自己的博客了。</p>\n<h2 id=\"Hexo使用\"><a href=\"#Hexo使用\" class=\"headerlink\" title=\"Hexo使用\"></a>Hexo使用</h2><p><strong>如果你已经有了一个建好的网站</strong>，并且已经上传到了github，可将网站代码克隆到本地。</p>\n<p>在选定的文件夹下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone git@github.com:heros979&#x2F;My_blog.git</span><br></pre></td></tr></table></figure>\n<p>clone后面是你的github代码库的地址，示例是我的，你们要改成你们自己的。</p>\n<p><strong>如果你是第一次建立自己的网站</strong>，在你想要的（随意哪个都行）文件夹里右键进入git bush，输入：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo init &lt;folder&gt;</span><br><span class=\"line\">$ cd &lt;folder&gt;</span><br><span class=\"line\">$ npm install</span><br></pre></td></tr></table></figure>\n<p>其中folder是你存放代码的文件夹名字</p>\n<p>好了，我们的文件夹里有了很多新的文件,如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.</span><br><span class=\"line\">├── _config.yml</span><br><span class=\"line\">├── package.json</span><br><span class=\"line\">├── scaffolds</span><br><span class=\"line\">├── source</span><br><span class=\"line\">|   ├── _drafts</span><br><span class=\"line\">|   └── _posts</span><br><span class=\"line\">└── themes</span><br></pre></td></tr></table></figure>\n<p>其中 _config.yml 是网站的<a href=\"https://hexo.io/zh-cn/docs/configuration\">配置</a>信息，您可以在此配置大部分的参数。<br>如果是第一次建立的话，要在deploy处加入自己的github地址。例如我的：</p>\n<p><img src=\"/asset/deploy.png\" alt=\"deploy\"></p>\n<p>我们要写的博客就放在 _post 文件夹里。在根目录可以使用如下方式新建post</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new &lt;post-name&gt;</span><br></pre></td></tr></table></figure>\n<p>我们在写完post后在根目录输入：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo clean</span><br><span class=\"line\">hexo g</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>即可将我们的网站部署到VPS服务器和我们的github上。</p>\n<h2 id=\"Markdown写作\"><a href=\"#Markdown写作\" class=\"headerlink\" title=\"Markdown写作\"></a>Markdown写作</h2><p>打开我们刚才建立的post-name.md文件，我们要用markdown的语法来写这个页面，markdown是一个很简单的写作工具，也很好看，很实用。</p>\n<p>例子1：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*开辟鸿蒙，谁为情种？*                  #这是斜体</span><br><span class=\"line\">**都只为风月情浓。**                    #这是加粗</span><br><span class=\"line\">***奈何天，伤怀日，***                  #这是斜体加粗</span><br><span class=\"line\">&lt;font size&#x3D;4&gt;寂寥时，试遣愚衷。&lt;&#x2F;font&gt;  #自定义字体大小</span><br><span class=\"line\">~~因此上，演出这~~                      #这是删除线</span><br><span class=\"line\">&gt; 怀金悼玉的《红楼梦》。                 # 这是引用</span><br></pre></td></tr></table></figure>\n<p><em>开辟鸿蒙，谁为情种？</em><br><strong>都只为风月情浓。</strong><br><strong><em>奈何天，伤怀日，</em></strong><br><font size=4>寂寥时，试遣愚衷。</font><br><del>因此上，演出这</del></p>\n<blockquote>\n<p>怀金悼玉的《红楼梦》。</p>\n</blockquote>\n<p>例子2：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">## CALLED BACK  #这是标题，几个#就是几级标题</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;font face&#x3D;&quot;微软雅黑&quot; size&#x3D;6 color&#x3D;#FF0000 &gt;Just lost when I was saved!&lt;&#x2F;font&gt;  #修改字体字号颜色</span><br><span class=\"line\">&lt;table&gt;&lt;tr&gt;&lt;td bgcolor&#x3D;orange&gt;Just felt the world go by!&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;table&gt;     #修改背景色</span><br><span class=\"line\">Just girt me for the onset with eternity,</span><br><span class=\"line\">When breath blew back,</span><br><span class=\"line\">And on the other side</span><br><span class=\"line\">I heard recede the disappointed tide!</span><br><span class=\"line\"></span><br><span class=\"line\">Therefore, as one returned, I feel,</span><br><span class=\"line\">Odd secrets of the line to tell!</span><br><span class=\"line\">Some sailor, skirting foreign shores,</span><br><span class=\"line\">Some pale reporter from the awful doors </span><br><span class=\"line\">Before the seal!</span><br><span class=\"line\"></span><br><span class=\"line\">Next time, to stay!</span><br><span class=\"line\">Next time, the things to see</span><br><span class=\"line\">By ear unheard,</span><br><span class=\"line\">Unscrutinized by eye.</span><br><span class=\"line\"></span><br><span class=\"line\">Next time, to tarry,</span><br><span class=\"line\">While the ages steal,—</span><br><span class=\"line\">Slow tramp the centuries,</span><br><span class=\"line\">And the cycles wheel.</span><br></pre></td></tr></table></figure>\n<h1 id=\"CALLED-BACK\"><a href=\"#CALLED-BACK\" class=\"headerlink\" title=\"CALLED BACK\"></a>CALLED BACK</h1><p><font face=\"微软雅黑\" size=6 color=#FF0000 >Just lost when I was saved!</font></p>\n<table><tr><td bgcolor=orange>Just felt the world go by!</td></tr></table>\nJust girt me for the onset with eternity,\nWhen breath blew back,\nAnd on the other side\nI heard recede the disappointed tide!\n\n<p>Therefore, as one returned, I feel,<br>Odd secrets of the line to tell!<br>Some sailor, skirting foreign shores,<br>Some pale reporter from the awful doors<br>Before the seal!</p>\n<p>Next time, to stay!<br>Next time, the things to see<br>By ear unheard,<br>Unscrutinized by eye.</p>\n<p>Next time, to tarry,<br>While the ages steal,—<br>Slow tramp the centuries,<br>And the cycles wheel.</p>\n<p>更多语法，比如标题、换行、注释、分割线、引用、代码块、列表、链接、表格、图片、流程图、LaTeX。</p>\n<p><img src=\"http://n.sinaimg.cn/translate/20171114/WsmA-fynstfh7830699.gif\" alt=\"write\"></p>\n<p>参见：<a href=\"https://sspai.com/post/45816\">Markdown快速入门</a>，<a href=\"https://blog.csdn.net/woswod/article/details/82753451\">Markdown语法(字体,样式,公式,背景,图表等)</a></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h2><ul>\n<li>Hexo安装</li>\n<li>Hexo使用</li>\n<li>Markdown写作","more":"</li>\n</ul>\n<h2 id=\"Hexo安装\"><a href=\"#Hexo安装\" class=\"headerlink\" title=\"Hexo安装\"></a>Hexo安装</h2><p>引用<a href=\"https://hexo.io/zh-cn/docs/\">Hexo官方文档</a>：</p>\n<blockquote>\n<p><strong>什么是 Hexo?</strong><br>Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。</p>\n</blockquote>\n<blockquote>\n<p><strong>安装</strong><br>安装 Hexo 只需几分钟时间，若您在安装过程中遇到问题或无法找到解决方式，请提交问题，我们会尽力解决您的问题。</p>\n</blockquote>\n<blockquote>\n<p><strong>安装前提</strong><br>安装 Hexo 相当简单，只需要先安装下列应用程序即可：<br>Node.js (Node.js 版本需不低于 8.10，建议使用 Node.js 10.0 及以上版本)<br>Git</p>\n<p>如果您的电脑中已经安装上述必备程序，那么恭喜您！你可以直接前往 安装 Hexo 步骤。<br>如果您的电脑中尚未安装所需要的程序，请根据以下安装指示完成安装。</p>\n</blockquote>\n<blockquote>\n<p><strong>安装 Git</strong></p>\n<ul>\n<li>Windows：下载并安装 git.</li>\n<li>Mac：使用 Homebrew, MacPorts 或者下载 安装程序。</li>\n<li>Linux (Ubuntu, Debian)：sudo apt-get install git-core</li>\n<li>Linux (Fedora, Red Hat, CentOS)：sudo yum install git-core</li>\n</ul>\n</blockquote>\n<blockquote>\n<p><strong>安装 Node.js</strong><br>Node.js 为大多数平台提供了官方的 <a href=\"https://nodejs.org/en/download/\">安装程序</a>。对于中国大陆地区用户，可以前往 <a href=\"https://npm.taobao.org/mirrors/node\">淘宝 Node.js 镜像</a> 下载。</p>\n<p>其它的安装方法：</p>\n<ul>\n<li>Windows：通过 nvs（推荐）或者nvm 安装。</li>\n<li>Mac：使用 Homebrew 或 MacPorts 安装。</li>\n<li>Linux（DEB/RPM-based）：从 NodeSource 安装。</li>\n</ul>\n<p>其它：使用相应的软件包管理器进行安装，可以参考由 Node.js 提供的 指导<br>对于 Mac 和 Linux 同样建议使用 nvs 或者 nvm，以避免可能会出现的权限问题。</p>\n</blockquote>\n<p>这里需要用到刚才安装的git，在git bush里进行如下操作：</p>\n<blockquote>\n<p><strong>安装 Hexo</strong><br>所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。</p>\n<p>$ npm install -g hexo-cli</p>\n</blockquote>\n<p>这样，Hexo就成功安装好了，下一步就可以着手在本地搭建自己的博客了。</p>\n<h2 id=\"Hexo使用\"><a href=\"#Hexo使用\" class=\"headerlink\" title=\"Hexo使用\"></a>Hexo使用</h2><p><strong>如果你已经有了一个建好的网站</strong>，并且已经上传到了github，可将网站代码克隆到本地。</p>\n<p>在选定的文件夹下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ git clone git@github.com:heros979&#x2F;My_blog.git</span><br></pre></td></tr></table></figure>\n<p>clone后面是你的github代码库的地址，示例是我的，你们要改成你们自己的。</p>\n<p><strong>如果你是第一次建立自己的网站</strong>，在你想要的（随意哪个都行）文件夹里右键进入git bush，输入：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo init &lt;folder&gt;</span><br><span class=\"line\">$ cd &lt;folder&gt;</span><br><span class=\"line\">$ npm install</span><br></pre></td></tr></table></figure>\n<p>其中folder是你存放代码的文件夹名字</p>\n<p>好了，我们的文件夹里有了很多新的文件,如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">.</span><br><span class=\"line\">├── _config.yml</span><br><span class=\"line\">├── package.json</span><br><span class=\"line\">├── scaffolds</span><br><span class=\"line\">├── source</span><br><span class=\"line\">|   ├── _drafts</span><br><span class=\"line\">|   └── _posts</span><br><span class=\"line\">└── themes</span><br></pre></td></tr></table></figure>\n<p>其中 _config.yml 是网站的<a href=\"https://hexo.io/zh-cn/docs/configuration\">配置</a>信息，您可以在此配置大部分的参数。<br>如果是第一次建立的话，要在deploy处加入自己的github地址。例如我的：</p>\n<p><img src=\"/asset/deploy.png\" alt=\"deploy\"></p>\n<p>我们要写的博客就放在 _post 文件夹里。在根目录可以使用如下方式新建post</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new &lt;post-name&gt;</span><br></pre></td></tr></table></figure>\n<p>我们在写完post后在根目录输入：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo clean</span><br><span class=\"line\">hexo g</span><br><span class=\"line\">hexo d</span><br></pre></td></tr></table></figure>\n<p>即可将我们的网站部署到VPS服务器和我们的github上。</p>\n<h2 id=\"Markdown写作\"><a href=\"#Markdown写作\" class=\"headerlink\" title=\"Markdown写作\"></a>Markdown写作</h2><p>打开我们刚才建立的post-name.md文件，我们要用markdown的语法来写这个页面，markdown是一个很简单的写作工具，也很好看，很实用。</p>\n<p>例子1：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">*开辟鸿蒙，谁为情种？*                  #这是斜体</span><br><span class=\"line\">**都只为风月情浓。**                    #这是加粗</span><br><span class=\"line\">***奈何天，伤怀日，***                  #这是斜体加粗</span><br><span class=\"line\">&lt;font size&#x3D;4&gt;寂寥时，试遣愚衷。&lt;&#x2F;font&gt;  #自定义字体大小</span><br><span class=\"line\">~~因此上，演出这~~                      #这是删除线</span><br><span class=\"line\">&gt; 怀金悼玉的《红楼梦》。                 # 这是引用</span><br></pre></td></tr></table></figure>\n<p><em>开辟鸿蒙，谁为情种？</em><br><strong>都只为风月情浓。</strong><br><strong><em>奈何天，伤怀日，</em></strong><br><font size=4>寂寥时，试遣愚衷。</font><br><del>因此上，演出这</del></p>\n<blockquote>\n<p>怀金悼玉的《红楼梦》。</p>\n</blockquote>\n<p>例子2：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">## CALLED BACK  #这是标题，几个#就是几级标题</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;font face&#x3D;&quot;微软雅黑&quot; size&#x3D;6 color&#x3D;#FF0000 &gt;Just lost when I was saved!&lt;&#x2F;font&gt;  #修改字体字号颜色</span><br><span class=\"line\">&lt;table&gt;&lt;tr&gt;&lt;td bgcolor&#x3D;orange&gt;Just felt the world go by!&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;table&gt;     #修改背景色</span><br><span class=\"line\">Just girt me for the onset with eternity,</span><br><span class=\"line\">When breath blew back,</span><br><span class=\"line\">And on the other side</span><br><span class=\"line\">I heard recede the disappointed tide!</span><br><span class=\"line\"></span><br><span class=\"line\">Therefore, as one returned, I feel,</span><br><span class=\"line\">Odd secrets of the line to tell!</span><br><span class=\"line\">Some sailor, skirting foreign shores,</span><br><span class=\"line\">Some pale reporter from the awful doors </span><br><span class=\"line\">Before the seal!</span><br><span class=\"line\"></span><br><span class=\"line\">Next time, to stay!</span><br><span class=\"line\">Next time, the things to see</span><br><span class=\"line\">By ear unheard,</span><br><span class=\"line\">Unscrutinized by eye.</span><br><span class=\"line\"></span><br><span class=\"line\">Next time, to tarry,</span><br><span class=\"line\">While the ages steal,—</span><br><span class=\"line\">Slow tramp the centuries,</span><br><span class=\"line\">And the cycles wheel.</span><br></pre></td></tr></table></figure>\n<h1 id=\"CALLED-BACK\"><a href=\"#CALLED-BACK\" class=\"headerlink\" title=\"CALLED BACK\"></a>CALLED BACK</h1><p><font face=\"微软雅黑\" size=6 color=#FF0000 >Just lost when I was saved!</font></p>\n<table><tr><td bgcolor=orange>Just felt the world go by!</td></tr></table>\nJust girt me for the onset with eternity,\nWhen breath blew back,\nAnd on the other side\nI heard recede the disappointed tide!\n\n<p>Therefore, as one returned, I feel,<br>Odd secrets of the line to tell!<br>Some sailor, skirting foreign shores,<br>Some pale reporter from the awful doors<br>Before the seal!</p>\n<p>Next time, to stay!<br>Next time, the things to see<br>By ear unheard,<br>Unscrutinized by eye.</p>\n<p>Next time, to tarry,<br>While the ages steal,—<br>Slow tramp the centuries,<br>And the cycles wheel.</p>\n<p>更多语法，比如标题、换行、注释、分割线、引用、代码块、列表、链接、表格、图片、流程图、LaTeX。</p>\n<p><img src=\"http://n.sinaimg.cn/translate/20171114/WsmA-fynstfh7830699.gif\" alt=\"write\"></p>\n<p>参见：<a href=\"https://sspai.com/post/45816\">Markdown快速入门</a>，<a href=\"https://blog.csdn.net/woswod/article/details/82753451\">Markdown语法(字体,样式,公式,背景,图表等)</a></p>"},{"title":"(转)以最小作用量原理为第一性原理的经典演绎","date":"2020-01-15T23:49:54.000Z","author":"钟昊均","mathjax":true,"_content":"<center>钟昊均</center>\n\n## <center>摘要</center>\n&emsp;&emsp;朗道力学行文之简洁有力的基础，我认为是抓住了经典力学的本质——最小作用量原理，并以其为第一性原理，通过变分法的数学演绎来建立整个经典力学体系。\n\n<!--more-->\n## 拉格朗日分析\n&emsp;&emsp;很多人认为朗道的书很难，但我认为难的其实并不是朗道书里面花哨的积分或者各种推导技巧，难的是朗道在书中体现的对理论体系的物理本质的思考。比如在拉格朗日函数等于T-V的推导时，其实是合理地从积分泛函的物理本质结合数学结构的基础上合理猜出了动能在笛卡尔坐标中的一般形式，顺便还得出了笛卡尔系下质量的定义——拉格朗日函数对伽利略变换时全导数项的乘数因子。在建立三大守恒律与三大对称性的桥梁时更是体现了朗道对对称性的深刻理解，通过不变量的构造展现了诺特定理的内涵，不过没有在书中完整地展现和演绎诺特定理是有点遗憾的。\n\n&emsp;&emsp;这样的演绎方法最大的好处就是理论的结构和体系非常完整和优美，在具体力学过程的演绎上也是从拉格朗日函数上出发利用拉氏方程来解决具体问题。我以为正是广义坐标的引入体现了力学体系的一般性，这使得拉格朗日力学具有普适性，通过在逻辑上先不考虑约束的存在，而后再通过约束减少方程个数，这也使得拉格朗日力学相比利用几何约束来构造牛顿方程的牛顿力学更加自然，这些共同形成了拉格朗日力学对牛顿力学的先进性——本质上完成了力学空间从欧氏空间到位形空间的拓展。\n## 哈密顿分析\n&emsp;&emsp;但是这对于经典力学的研究并没有画上一个句号——力学的几何特征并没有在拉格朗日力学中得到最大的显现。对于定义在位形空间的拉氏力学而言，一个可积系统在宏观上很有可能是杂乱无章的，比如对于统计物理中的各种模型，使用拉氏量来描述体系是不够能体现体系的某些共同特征的；同时在处理坐标和速度的地位上也是有一点缺憾的——位置和速度可以分别同时给定，这就隐含了位置与速度在描述运动的地位上是不是可以相等的问题。这时，利用勒让德变换就能够将拉格朗日力学过渡到更加先进的哈密顿力学上来。\n\n&emsp;&emsp;勒让德变换在变换拉格朗日函数的时候很像分部积分，通过对微分拉氏量的数学变换，可以构造出一个以广义动量和广义坐标为变量的描述力学的量——哈密顿量。哈密顿力学的威力就在于更高的对称性，通过参数空间的拓展，由哈密顿量导出的正则方程拥有了拉氏方程不具有的高对称性，同时方程阶数由二阶降到一阶、方程个数则翻了倍，我们由此可以从正则方程中得到更多对系统的描述，在数值计算时甚至可以提高效率和可靠性。这种对称性使得定义在“相空间”的哈密顿力学有能力开始展现更多的力学系统的几何特征，力学系统在相空间中可以自然形成流形，同时刘维尔定理告诉我们相空间体积元对正则变换不变，这给了我们研究力学系统在相空间中流形上的性质时最好的工具，正如朗道在“正则变换”一节中说的那样：“对这种可能变换类型的扩大是力学的哈密顿方法的重要优点之一”。\n\n&emsp;&emsp;当然，哈密顿力学中的重要概念——泊松括号，这种算符的存在使得运动积分的构造变得程序化，毕竟两个运动积分的泊松括号也是运动积分，这对于力学的不变量理论是极其重要的。但是泊松括号的意义不仅仅在于此，其在量子力学中的映射——对易子算符对量子力学具有重要意义。\n## 结束语\n&emsp;&emsp;从朗道力学的目录来看，全书非常精炼，为了完成力学体系的构造，没有一节是冗余的，信息量非常大，在研读哈密顿力学之时有很多推导的细节都非常强调亲手推导，这种高屋建瓴的视角正是理论工作者所需要的。\n","source":"_posts/zhj-1.md","raw":"---\ntitle: (转)以最小作用量原理为第一性原理的经典演绎\ndate: 2020-01-16 07:49:54\ntags: [读后感]\ncategories: [物理]\nauthor: 钟昊均\nmathjax: true\n---\n<center>钟昊均</center>\n\n## <center>摘要</center>\n&emsp;&emsp;朗道力学行文之简洁有力的基础，我认为是抓住了经典力学的本质——最小作用量原理，并以其为第一性原理，通过变分法的数学演绎来建立整个经典力学体系。\n\n<!--more-->\n## 拉格朗日分析\n&emsp;&emsp;很多人认为朗道的书很难，但我认为难的其实并不是朗道书里面花哨的积分或者各种推导技巧，难的是朗道在书中体现的对理论体系的物理本质的思考。比如在拉格朗日函数等于T-V的推导时，其实是合理地从积分泛函的物理本质结合数学结构的基础上合理猜出了动能在笛卡尔坐标中的一般形式，顺便还得出了笛卡尔系下质量的定义——拉格朗日函数对伽利略变换时全导数项的乘数因子。在建立三大守恒律与三大对称性的桥梁时更是体现了朗道对对称性的深刻理解，通过不变量的构造展现了诺特定理的内涵，不过没有在书中完整地展现和演绎诺特定理是有点遗憾的。\n\n&emsp;&emsp;这样的演绎方法最大的好处就是理论的结构和体系非常完整和优美，在具体力学过程的演绎上也是从拉格朗日函数上出发利用拉氏方程来解决具体问题。我以为正是广义坐标的引入体现了力学体系的一般性，这使得拉格朗日力学具有普适性，通过在逻辑上先不考虑约束的存在，而后再通过约束减少方程个数，这也使得拉格朗日力学相比利用几何约束来构造牛顿方程的牛顿力学更加自然，这些共同形成了拉格朗日力学对牛顿力学的先进性——本质上完成了力学空间从欧氏空间到位形空间的拓展。\n## 哈密顿分析\n&emsp;&emsp;但是这对于经典力学的研究并没有画上一个句号——力学的几何特征并没有在拉格朗日力学中得到最大的显现。对于定义在位形空间的拉氏力学而言，一个可积系统在宏观上很有可能是杂乱无章的，比如对于统计物理中的各种模型，使用拉氏量来描述体系是不够能体现体系的某些共同特征的；同时在处理坐标和速度的地位上也是有一点缺憾的——位置和速度可以分别同时给定，这就隐含了位置与速度在描述运动的地位上是不是可以相等的问题。这时，利用勒让德变换就能够将拉格朗日力学过渡到更加先进的哈密顿力学上来。\n\n&emsp;&emsp;勒让德变换在变换拉格朗日函数的时候很像分部积分，通过对微分拉氏量的数学变换，可以构造出一个以广义动量和广义坐标为变量的描述力学的量——哈密顿量。哈密顿力学的威力就在于更高的对称性，通过参数空间的拓展，由哈密顿量导出的正则方程拥有了拉氏方程不具有的高对称性，同时方程阶数由二阶降到一阶、方程个数则翻了倍，我们由此可以从正则方程中得到更多对系统的描述，在数值计算时甚至可以提高效率和可靠性。这种对称性使得定义在“相空间”的哈密顿力学有能力开始展现更多的力学系统的几何特征，力学系统在相空间中可以自然形成流形，同时刘维尔定理告诉我们相空间体积元对正则变换不变，这给了我们研究力学系统在相空间中流形上的性质时最好的工具，正如朗道在“正则变换”一节中说的那样：“对这种可能变换类型的扩大是力学的哈密顿方法的重要优点之一”。\n\n&emsp;&emsp;当然，哈密顿力学中的重要概念——泊松括号，这种算符的存在使得运动积分的构造变得程序化，毕竟两个运动积分的泊松括号也是运动积分，这对于力学的不变量理论是极其重要的。但是泊松括号的意义不仅仅在于此，其在量子力学中的映射——对易子算符对量子力学具有重要意义。\n## 结束语\n&emsp;&emsp;从朗道力学的目录来看，全书非常精炼，为了完成力学体系的构造，没有一节是冗余的，信息量非常大，在研读哈密顿力学之时有很多推导的细节都非常强调亲手推导，这种高屋建瓴的视角正是理论工作者所需要的。\n","slug":"zhj-1","published":1,"updated":"2021-02-01T15:53:22.097Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9f0001ilsvwc7w1eqbr","content":"<center>钟昊均</center>\n\n<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a><center>摘要</center></h2><p>&emsp;&emsp;朗道力学行文之简洁有力的基础，我认为是抓住了经典力学的本质——最小作用量原理，并以其为第一性原理，通过变分法的数学演绎来建立整个经典力学体系。</p>\n<a id=\"more\"></a>\n<h2 id=\"拉格朗日分析\"><a href=\"#拉格朗日分析\" class=\"headerlink\" title=\"拉格朗日分析\"></a>拉格朗日分析</h2><p>&emsp;&emsp;很多人认为朗道的书很难，但我认为难的其实并不是朗道书里面花哨的积分或者各种推导技巧，难的是朗道在书中体现的对理论体系的物理本质的思考。比如在拉格朗日函数等于T-V的推导时，其实是合理地从积分泛函的物理本质结合数学结构的基础上合理猜出了动能在笛卡尔坐标中的一般形式，顺便还得出了笛卡尔系下质量的定义——拉格朗日函数对伽利略变换时全导数项的乘数因子。在建立三大守恒律与三大对称性的桥梁时更是体现了朗道对对称性的深刻理解，通过不变量的构造展现了诺特定理的内涵，不过没有在书中完整地展现和演绎诺特定理是有点遗憾的。</p>\n<p>&emsp;&emsp;这样的演绎方法最大的好处就是理论的结构和体系非常完整和优美，在具体力学过程的演绎上也是从拉格朗日函数上出发利用拉氏方程来解决具体问题。我以为正是广义坐标的引入体现了力学体系的一般性，这使得拉格朗日力学具有普适性，通过在逻辑上先不考虑约束的存在，而后再通过约束减少方程个数，这也使得拉格朗日力学相比利用几何约束来构造牛顿方程的牛顿力学更加自然，这些共同形成了拉格朗日力学对牛顿力学的先进性——本质上完成了力学空间从欧氏空间到位形空间的拓展。</p>\n<h2 id=\"哈密顿分析\"><a href=\"#哈密顿分析\" class=\"headerlink\" title=\"哈密顿分析\"></a>哈密顿分析</h2><p>&emsp;&emsp;但是这对于经典力学的研究并没有画上一个句号——力学的几何特征并没有在拉格朗日力学中得到最大的显现。对于定义在位形空间的拉氏力学而言，一个可积系统在宏观上很有可能是杂乱无章的，比如对于统计物理中的各种模型，使用拉氏量来描述体系是不够能体现体系的某些共同特征的；同时在处理坐标和速度的地位上也是有一点缺憾的——位置和速度可以分别同时给定，这就隐含了位置与速度在描述运动的地位上是不是可以相等的问题。这时，利用勒让德变换就能够将拉格朗日力学过渡到更加先进的哈密顿力学上来。</p>\n<p>&emsp;&emsp;勒让德变换在变换拉格朗日函数的时候很像分部积分，通过对微分拉氏量的数学变换，可以构造出一个以广义动量和广义坐标为变量的描述力学的量——哈密顿量。哈密顿力学的威力就在于更高的对称性，通过参数空间的拓展，由哈密顿量导出的正则方程拥有了拉氏方程不具有的高对称性，同时方程阶数由二阶降到一阶、方程个数则翻了倍，我们由此可以从正则方程中得到更多对系统的描述，在数值计算时甚至可以提高效率和可靠性。这种对称性使得定义在“相空间”的哈密顿力学有能力开始展现更多的力学系统的几何特征，力学系统在相空间中可以自然形成流形，同时刘维尔定理告诉我们相空间体积元对正则变换不变，这给了我们研究力学系统在相空间中流形上的性质时最好的工具，正如朗道在“正则变换”一节中说的那样：“对这种可能变换类型的扩大是力学的哈密顿方法的重要优点之一”。</p>\n<p>&emsp;&emsp;当然，哈密顿力学中的重要概念——泊松括号，这种算符的存在使得运动积分的构造变得程序化，毕竟两个运动积分的泊松括号也是运动积分，这对于力学的不变量理论是极其重要的。但是泊松括号的意义不仅仅在于此，其在量子力学中的映射——对易子算符对量子力学具有重要意义。</p>\n<h2 id=\"结束语\"><a href=\"#结束语\" class=\"headerlink\" title=\"结束语\"></a>结束语</h2><p>&emsp;&emsp;从朗道力学的目录来看，全书非常精炼，为了完成力学体系的构造，没有一节是冗余的，信息量非常大，在研读哈密顿力学之时有很多推导的细节都非常强调亲手推导，这种高屋建瓴的视角正是理论工作者所需要的。</p>\n","site":{"data":{}},"excerpt":"<center>钟昊均</center>\n\n<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a><center>摘要</center></h2><p>&emsp;&emsp;朗道力学行文之简洁有力的基础，我认为是抓住了经典力学的本质——最小作用量原理，并以其为第一性原理，通过变分法的数学演绎来建立整个经典力学体系。</p>","more":"<h2 id=\"拉格朗日分析\"><a href=\"#拉格朗日分析\" class=\"headerlink\" title=\"拉格朗日分析\"></a>拉格朗日分析</h2><p>&emsp;&emsp;很多人认为朗道的书很难，但我认为难的其实并不是朗道书里面花哨的积分或者各种推导技巧，难的是朗道在书中体现的对理论体系的物理本质的思考。比如在拉格朗日函数等于T-V的推导时，其实是合理地从积分泛函的物理本质结合数学结构的基础上合理猜出了动能在笛卡尔坐标中的一般形式，顺便还得出了笛卡尔系下质量的定义——拉格朗日函数对伽利略变换时全导数项的乘数因子。在建立三大守恒律与三大对称性的桥梁时更是体现了朗道对对称性的深刻理解，通过不变量的构造展现了诺特定理的内涵，不过没有在书中完整地展现和演绎诺特定理是有点遗憾的。</p>\n<p>&emsp;&emsp;这样的演绎方法最大的好处就是理论的结构和体系非常完整和优美，在具体力学过程的演绎上也是从拉格朗日函数上出发利用拉氏方程来解决具体问题。我以为正是广义坐标的引入体现了力学体系的一般性，这使得拉格朗日力学具有普适性，通过在逻辑上先不考虑约束的存在，而后再通过约束减少方程个数，这也使得拉格朗日力学相比利用几何约束来构造牛顿方程的牛顿力学更加自然，这些共同形成了拉格朗日力学对牛顿力学的先进性——本质上完成了力学空间从欧氏空间到位形空间的拓展。</p>\n<h2 id=\"哈密顿分析\"><a href=\"#哈密顿分析\" class=\"headerlink\" title=\"哈密顿分析\"></a>哈密顿分析</h2><p>&emsp;&emsp;但是这对于经典力学的研究并没有画上一个句号——力学的几何特征并没有在拉格朗日力学中得到最大的显现。对于定义在位形空间的拉氏力学而言，一个可积系统在宏观上很有可能是杂乱无章的，比如对于统计物理中的各种模型，使用拉氏量来描述体系是不够能体现体系的某些共同特征的；同时在处理坐标和速度的地位上也是有一点缺憾的——位置和速度可以分别同时给定，这就隐含了位置与速度在描述运动的地位上是不是可以相等的问题。这时，利用勒让德变换就能够将拉格朗日力学过渡到更加先进的哈密顿力学上来。</p>\n<p>&emsp;&emsp;勒让德变换在变换拉格朗日函数的时候很像分部积分，通过对微分拉氏量的数学变换，可以构造出一个以广义动量和广义坐标为变量的描述力学的量——哈密顿量。哈密顿力学的威力就在于更高的对称性，通过参数空间的拓展，由哈密顿量导出的正则方程拥有了拉氏方程不具有的高对称性，同时方程阶数由二阶降到一阶、方程个数则翻了倍，我们由此可以从正则方程中得到更多对系统的描述，在数值计算时甚至可以提高效率和可靠性。这种对称性使得定义在“相空间”的哈密顿力学有能力开始展现更多的力学系统的几何特征，力学系统在相空间中可以自然形成流形，同时刘维尔定理告诉我们相空间体积元对正则变换不变，这给了我们研究力学系统在相空间中流形上的性质时最好的工具，正如朗道在“正则变换”一节中说的那样：“对这种可能变换类型的扩大是力学的哈密顿方法的重要优点之一”。</p>\n<p>&emsp;&emsp;当然，哈密顿力学中的重要概念——泊松括号，这种算符的存在使得运动积分的构造变得程序化，毕竟两个运动积分的泊松括号也是运动积分，这对于力学的不变量理论是极其重要的。但是泊松括号的意义不仅仅在于此，其在量子力学中的映射——对易子算符对量子力学具有重要意义。</p>\n<h2 id=\"结束语\"><a href=\"#结束语\" class=\"headerlink\" title=\"结束语\"></a>结束语</h2><p>&emsp;&emsp;从朗道力学的目录来看，全书非常精炼，为了完成力学体系的构造，没有一节是冗余的，信息量非常大，在研读哈密顿力学之时有很多推导的细节都非常强调亲手推导，这种高屋建瓴的视角正是理论工作者所需要的。</p>"},{"title":"(转)果壳中的经典统计","date":"2020-01-15T23:50:02.000Z","author":"钟昊均","_content":"<center>钟昊均</center>\n\n## <center>摘要</center>\n&emsp;&emsp;一直想写一个有关经典统计的note，结合凝聚态场论学习中的一些经验，趁此机会写点我自己对经典统计的理解。\n<!--more-->\n## 统计方法的引入\n&emsp;&emsp;在经典力学的框架下，我们可以通过构造系统的拉格朗日量或哈密顿量来求出描述系统演化的拉格朗日方程或者正则方程，进而求解系统的各项性质。这种从最多的自由度出发精确求解系统相轨道的方法在数学上是困难的，从微分方程理论的角度来说，系统自由度的增加会极大地增加求解方程的困难，对于某些相互作用还会导致系统不可积，而且一旦方程是非线性的，这个系统就很有可能是极端初值依赖的，这使得对系统进行长时间的预测变为不可能，这些性质决定了运动方程在刻画多体系统上的不完备性。\n\n&emsp;&emsp;但是，对于多体系统，我们真的有必要精确求解这个系统吗？统计方法告诉我们，如果我们将尺度放到整个多体系统或者某个整体的多体子系统的层次并且给定一个简单的假设，我们就能从巨大的自由度中得到很多非平凡的结论，这就是经典统计方法带来的优势。微正则系综处理的是不满足KAM定理的保守不可积系统，对于不可积系统是不存在力学解析解的，而不满足KAM定理的要求会带来相轨道的遍历性（满足KAM定理的系统即使存在微扰也会保KAM环面的拓扑，哈密顿相流是必然不遍历的，因此从力学出发是不可能导出统计方法的），对于能量约束下形成的相空间上的球面来说，哈密顿相流的遍历性启发我们用系综平均来取代瞬时的体系参量以描述统计系统，同时如果给定一个更强的等概率原理（其实可以理解成相空间中态上的随机行走，这一点是比遍历性要求更高的），我们就能够给出微正则系综的数学描述，但是接下来就牵涉到系综平均和时间平均是否等价的问题。\n\n&emsp;&emsp;KAM定理的要求是在哈密顿量的可积部分上加上一项微扰项，微扰失效的情况下才会给系统引入遍历性，因此遍历性是统计方法的基本假设，在遍历性假设下时间平均才等于系综平均（哈密顿相流在能量球面上的随机行走轨迹能完全填充球面），但是实际上我们处理的很多系统的相互作用能不能打破KAM环面的拓扑是存疑的，在很多非线性系统中存在的吸引子也能够引发遍历性破缺。在我们的统计物理中，环境的噪声就被当成遍历性假设成立的解释之一，毕竟对于初值敏感的系统，噪声带来的微扰是影响巨大的。当然对于经典系统，能量很自然地假设是连续变化的，这个也是经典统计的一大基本假设。\n## 经典统计的局限性\n&emsp;&emsp;经典统计的局限性在很多微观体系中其实已经浮现，比如在固体比热和多原子分子气体乃至黑体辐射的分析中就已经一窥踪影。其原因无外乎三点：1、能级的不连续性被极小的宏观平均能级差隐藏；2、自旋乃至更高的自由度没有被考虑；3、能量表象下简并度被忽略。\n\n&emsp;&emsp;在我看来，第三点是经典统计最容易引入的修正，因为这个简并度在从微正则系综到正则系综的映射中就已经隐含了。微正则系综作为孤立系，在相空间中的自由度是最大的，需要考虑孤立系中全部子系统的轨迹，但是一旦我们考虑一个温度恒定的热源，通过积分积去环境带来的自由度，这样就完成了从微正则系综到正则系综的跃变。在这个过程中，很明显，被隐藏的自由度会带来更高的对称性，以至于在任意表象下都会引入非平庸的分布函数，但是目前我们刻意地只保留了指数的部分而忽略了具体表象下存在的其余对称性，比如k空间中谐振子波模就是存在简并度的，这一点在黑体辐射的经典分析中已经得到体现。\n\n&emsp;&emsp;对于凝聚态体系，自旋都是极其重要的，自旋自由度就是磁性系统的基础，但是对于自旋，我们也可以在完全应用量子场论之前做一点经典的统计，比如著名的伊辛模型，这个经典的自旋体系（区别于海森堡模型的自旋）可以做平均场，当然对于一维的伊辛模型的平均场得出的有限温相变点是很离谱的错误，但这不妨碍做高维的伊辛模型平均场（精确很多），这个模型往里挖还能引入很多新的思想，比如重整化，这里就不继续讨论了。\n\n&emsp;&emsp;能级不连续作为两朵大乌云中的一朵，在黑体辐射的研究被揭露了出来，无相互作用玻色子系统的能隙是产生玻色-爱因斯坦凝聚的关键，实验上在多原子分子的平均能量中我们也能看到阶梯状的随温度变化的情形，经典统计是得不出这样奇妙的结果的，毕竟经典系统是无能隙的。\n## 结束语\n&emsp;&emsp;经典统计是研究多体系统的重要工具，也是进入量子统计的基础，在经典统计中很多方法在量子统计中依然有用武之地，比如平均场和团簇展开都是非常常见的技术，其思想更是在物理学中占有极重的地位，因此经典统计对于我们的同学实属应该学好、必须学好的一种理论，这对今后的学习与研究都是有大益处的。\n\n","source":"_posts/zhj-2.md","raw":"---\ntitle: (转)果壳中的经典统计\ndate: 2020-01-16 07:50:02\nauthor: 钟昊均\ntags: [读后感]\ncategories: [物理]\n---\n<center>钟昊均</center>\n\n## <center>摘要</center>\n&emsp;&emsp;一直想写一个有关经典统计的note，结合凝聚态场论学习中的一些经验，趁此机会写点我自己对经典统计的理解。\n<!--more-->\n## 统计方法的引入\n&emsp;&emsp;在经典力学的框架下，我们可以通过构造系统的拉格朗日量或哈密顿量来求出描述系统演化的拉格朗日方程或者正则方程，进而求解系统的各项性质。这种从最多的自由度出发精确求解系统相轨道的方法在数学上是困难的，从微分方程理论的角度来说，系统自由度的增加会极大地增加求解方程的困难，对于某些相互作用还会导致系统不可积，而且一旦方程是非线性的，这个系统就很有可能是极端初值依赖的，这使得对系统进行长时间的预测变为不可能，这些性质决定了运动方程在刻画多体系统上的不完备性。\n\n&emsp;&emsp;但是，对于多体系统，我们真的有必要精确求解这个系统吗？统计方法告诉我们，如果我们将尺度放到整个多体系统或者某个整体的多体子系统的层次并且给定一个简单的假设，我们就能从巨大的自由度中得到很多非平凡的结论，这就是经典统计方法带来的优势。微正则系综处理的是不满足KAM定理的保守不可积系统，对于不可积系统是不存在力学解析解的，而不满足KAM定理的要求会带来相轨道的遍历性（满足KAM定理的系统即使存在微扰也会保KAM环面的拓扑，哈密顿相流是必然不遍历的，因此从力学出发是不可能导出统计方法的），对于能量约束下形成的相空间上的球面来说，哈密顿相流的遍历性启发我们用系综平均来取代瞬时的体系参量以描述统计系统，同时如果给定一个更强的等概率原理（其实可以理解成相空间中态上的随机行走，这一点是比遍历性要求更高的），我们就能够给出微正则系综的数学描述，但是接下来就牵涉到系综平均和时间平均是否等价的问题。\n\n&emsp;&emsp;KAM定理的要求是在哈密顿量的可积部分上加上一项微扰项，微扰失效的情况下才会给系统引入遍历性，因此遍历性是统计方法的基本假设，在遍历性假设下时间平均才等于系综平均（哈密顿相流在能量球面上的随机行走轨迹能完全填充球面），但是实际上我们处理的很多系统的相互作用能不能打破KAM环面的拓扑是存疑的，在很多非线性系统中存在的吸引子也能够引发遍历性破缺。在我们的统计物理中，环境的噪声就被当成遍历性假设成立的解释之一，毕竟对于初值敏感的系统，噪声带来的微扰是影响巨大的。当然对于经典系统，能量很自然地假设是连续变化的，这个也是经典统计的一大基本假设。\n## 经典统计的局限性\n&emsp;&emsp;经典统计的局限性在很多微观体系中其实已经浮现，比如在固体比热和多原子分子气体乃至黑体辐射的分析中就已经一窥踪影。其原因无外乎三点：1、能级的不连续性被极小的宏观平均能级差隐藏；2、自旋乃至更高的自由度没有被考虑；3、能量表象下简并度被忽略。\n\n&emsp;&emsp;在我看来，第三点是经典统计最容易引入的修正，因为这个简并度在从微正则系综到正则系综的映射中就已经隐含了。微正则系综作为孤立系，在相空间中的自由度是最大的，需要考虑孤立系中全部子系统的轨迹，但是一旦我们考虑一个温度恒定的热源，通过积分积去环境带来的自由度，这样就完成了从微正则系综到正则系综的跃变。在这个过程中，很明显，被隐藏的自由度会带来更高的对称性，以至于在任意表象下都会引入非平庸的分布函数，但是目前我们刻意地只保留了指数的部分而忽略了具体表象下存在的其余对称性，比如k空间中谐振子波模就是存在简并度的，这一点在黑体辐射的经典分析中已经得到体现。\n\n&emsp;&emsp;对于凝聚态体系，自旋都是极其重要的，自旋自由度就是磁性系统的基础，但是对于自旋，我们也可以在完全应用量子场论之前做一点经典的统计，比如著名的伊辛模型，这个经典的自旋体系（区别于海森堡模型的自旋）可以做平均场，当然对于一维的伊辛模型的平均场得出的有限温相变点是很离谱的错误，但这不妨碍做高维的伊辛模型平均场（精确很多），这个模型往里挖还能引入很多新的思想，比如重整化，这里就不继续讨论了。\n\n&emsp;&emsp;能级不连续作为两朵大乌云中的一朵，在黑体辐射的研究被揭露了出来，无相互作用玻色子系统的能隙是产生玻色-爱因斯坦凝聚的关键，实验上在多原子分子的平均能量中我们也能看到阶梯状的随温度变化的情形，经典统计是得不出这样奇妙的结果的，毕竟经典系统是无能隙的。\n## 结束语\n&emsp;&emsp;经典统计是研究多体系统的重要工具，也是进入量子统计的基础，在经典统计中很多方法在量子统计中依然有用武之地，比如平均场和团簇展开都是非常常见的技术，其思想更是在物理学中占有极重的地位，因此经典统计对于我们的同学实属应该学好、必须学好的一种理论，这对今后的学习与研究都是有大益处的。\n\n","slug":"zhj-2","published":1,"updated":"2021-02-01T15:53:38.018Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckotps9f1001klsvw9mdwhu93","content":"<center>钟昊均</center>\n\n<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a><center>摘要</center></h2><p>&emsp;&emsp;一直想写一个有关经典统计的note，结合凝聚态场论学习中的一些经验，趁此机会写点我自己对经典统计的理解。</p>\n<a id=\"more\"></a>\n<h2 id=\"统计方法的引入\"><a href=\"#统计方法的引入\" class=\"headerlink\" title=\"统计方法的引入\"></a>统计方法的引入</h2><p>&emsp;&emsp;在经典力学的框架下，我们可以通过构造系统的拉格朗日量或哈密顿量来求出描述系统演化的拉格朗日方程或者正则方程，进而求解系统的各项性质。这种从最多的自由度出发精确求解系统相轨道的方法在数学上是困难的，从微分方程理论的角度来说，系统自由度的增加会极大地增加求解方程的困难，对于某些相互作用还会导致系统不可积，而且一旦方程是非线性的，这个系统就很有可能是极端初值依赖的，这使得对系统进行长时间的预测变为不可能，这些性质决定了运动方程在刻画多体系统上的不完备性。</p>\n<p>&emsp;&emsp;但是，对于多体系统，我们真的有必要精确求解这个系统吗？统计方法告诉我们，如果我们将尺度放到整个多体系统或者某个整体的多体子系统的层次并且给定一个简单的假设，我们就能从巨大的自由度中得到很多非平凡的结论，这就是经典统计方法带来的优势。微正则系综处理的是不满足KAM定理的保守不可积系统，对于不可积系统是不存在力学解析解的，而不满足KAM定理的要求会带来相轨道的遍历性（满足KAM定理的系统即使存在微扰也会保KAM环面的拓扑，哈密顿相流是必然不遍历的，因此从力学出发是不可能导出统计方法的），对于能量约束下形成的相空间上的球面来说，哈密顿相流的遍历性启发我们用系综平均来取代瞬时的体系参量以描述统计系统，同时如果给定一个更强的等概率原理（其实可以理解成相空间中态上的随机行走，这一点是比遍历性要求更高的），我们就能够给出微正则系综的数学描述，但是接下来就牵涉到系综平均和时间平均是否等价的问题。</p>\n<p>&emsp;&emsp;KAM定理的要求是在哈密顿量的可积部分上加上一项微扰项，微扰失效的情况下才会给系统引入遍历性，因此遍历性是统计方法的基本假设，在遍历性假设下时间平均才等于系综平均（哈密顿相流在能量球面上的随机行走轨迹能完全填充球面），但是实际上我们处理的很多系统的相互作用能不能打破KAM环面的拓扑是存疑的，在很多非线性系统中存在的吸引子也能够引发遍历性破缺。在我们的统计物理中，环境的噪声就被当成遍历性假设成立的解释之一，毕竟对于初值敏感的系统，噪声带来的微扰是影响巨大的。当然对于经典系统，能量很自然地假设是连续变化的，这个也是经典统计的一大基本假设。</p>\n<h2 id=\"经典统计的局限性\"><a href=\"#经典统计的局限性\" class=\"headerlink\" title=\"经典统计的局限性\"></a>经典统计的局限性</h2><p>&emsp;&emsp;经典统计的局限性在很多微观体系中其实已经浮现，比如在固体比热和多原子分子气体乃至黑体辐射的分析中就已经一窥踪影。其原因无外乎三点：1、能级的不连续性被极小的宏观平均能级差隐藏；2、自旋乃至更高的自由度没有被考虑；3、能量表象下简并度被忽略。</p>\n<p>&emsp;&emsp;在我看来，第三点是经典统计最容易引入的修正，因为这个简并度在从微正则系综到正则系综的映射中就已经隐含了。微正则系综作为孤立系，在相空间中的自由度是最大的，需要考虑孤立系中全部子系统的轨迹，但是一旦我们考虑一个温度恒定的热源，通过积分积去环境带来的自由度，这样就完成了从微正则系综到正则系综的跃变。在这个过程中，很明显，被隐藏的自由度会带来更高的对称性，以至于在任意表象下都会引入非平庸的分布函数，但是目前我们刻意地只保留了指数的部分而忽略了具体表象下存在的其余对称性，比如k空间中谐振子波模就是存在简并度的，这一点在黑体辐射的经典分析中已经得到体现。</p>\n<p>&emsp;&emsp;对于凝聚态体系，自旋都是极其重要的，自旋自由度就是磁性系统的基础，但是对于自旋，我们也可以在完全应用量子场论之前做一点经典的统计，比如著名的伊辛模型，这个经典的自旋体系（区别于海森堡模型的自旋）可以做平均场，当然对于一维的伊辛模型的平均场得出的有限温相变点是很离谱的错误，但这不妨碍做高维的伊辛模型平均场（精确很多），这个模型往里挖还能引入很多新的思想，比如重整化，这里就不继续讨论了。</p>\n<p>&emsp;&emsp;能级不连续作为两朵大乌云中的一朵，在黑体辐射的研究被揭露了出来，无相互作用玻色子系统的能隙是产生玻色-爱因斯坦凝聚的关键，实验上在多原子分子的平均能量中我们也能看到阶梯状的随温度变化的情形，经典统计是得不出这样奇妙的结果的，毕竟经典系统是无能隙的。</p>\n<h2 id=\"结束语\"><a href=\"#结束语\" class=\"headerlink\" title=\"结束语\"></a>结束语</h2><p>&emsp;&emsp;经典统计是研究多体系统的重要工具，也是进入量子统计的基础，在经典统计中很多方法在量子统计中依然有用武之地，比如平均场和团簇展开都是非常常见的技术，其思想更是在物理学中占有极重的地位，因此经典统计对于我们的同学实属应该学好、必须学好的一种理论，这对今后的学习与研究都是有大益处的。</p>\n","site":{"data":{}},"excerpt":"<center>钟昊均</center>\n\n<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a><center>摘要</center></h2><p>&emsp;&emsp;一直想写一个有关经典统计的note，结合凝聚态场论学习中的一些经验，趁此机会写点我自己对经典统计的理解。</p>","more":"<h2 id=\"统计方法的引入\"><a href=\"#统计方法的引入\" class=\"headerlink\" title=\"统计方法的引入\"></a>统计方法的引入</h2><p>&emsp;&emsp;在经典力学的框架下，我们可以通过构造系统的拉格朗日量或哈密顿量来求出描述系统演化的拉格朗日方程或者正则方程，进而求解系统的各项性质。这种从最多的自由度出发精确求解系统相轨道的方法在数学上是困难的，从微分方程理论的角度来说，系统自由度的增加会极大地增加求解方程的困难，对于某些相互作用还会导致系统不可积，而且一旦方程是非线性的，这个系统就很有可能是极端初值依赖的，这使得对系统进行长时间的预测变为不可能，这些性质决定了运动方程在刻画多体系统上的不完备性。</p>\n<p>&emsp;&emsp;但是，对于多体系统，我们真的有必要精确求解这个系统吗？统计方法告诉我们，如果我们将尺度放到整个多体系统或者某个整体的多体子系统的层次并且给定一个简单的假设，我们就能从巨大的自由度中得到很多非平凡的结论，这就是经典统计方法带来的优势。微正则系综处理的是不满足KAM定理的保守不可积系统，对于不可积系统是不存在力学解析解的，而不满足KAM定理的要求会带来相轨道的遍历性（满足KAM定理的系统即使存在微扰也会保KAM环面的拓扑，哈密顿相流是必然不遍历的，因此从力学出发是不可能导出统计方法的），对于能量约束下形成的相空间上的球面来说，哈密顿相流的遍历性启发我们用系综平均来取代瞬时的体系参量以描述统计系统，同时如果给定一个更强的等概率原理（其实可以理解成相空间中态上的随机行走，这一点是比遍历性要求更高的），我们就能够给出微正则系综的数学描述，但是接下来就牵涉到系综平均和时间平均是否等价的问题。</p>\n<p>&emsp;&emsp;KAM定理的要求是在哈密顿量的可积部分上加上一项微扰项，微扰失效的情况下才会给系统引入遍历性，因此遍历性是统计方法的基本假设，在遍历性假设下时间平均才等于系综平均（哈密顿相流在能量球面上的随机行走轨迹能完全填充球面），但是实际上我们处理的很多系统的相互作用能不能打破KAM环面的拓扑是存疑的，在很多非线性系统中存在的吸引子也能够引发遍历性破缺。在我们的统计物理中，环境的噪声就被当成遍历性假设成立的解释之一，毕竟对于初值敏感的系统，噪声带来的微扰是影响巨大的。当然对于经典系统，能量很自然地假设是连续变化的，这个也是经典统计的一大基本假设。</p>\n<h2 id=\"经典统计的局限性\"><a href=\"#经典统计的局限性\" class=\"headerlink\" title=\"经典统计的局限性\"></a>经典统计的局限性</h2><p>&emsp;&emsp;经典统计的局限性在很多微观体系中其实已经浮现，比如在固体比热和多原子分子气体乃至黑体辐射的分析中就已经一窥踪影。其原因无外乎三点：1、能级的不连续性被极小的宏观平均能级差隐藏；2、自旋乃至更高的自由度没有被考虑；3、能量表象下简并度被忽略。</p>\n<p>&emsp;&emsp;在我看来，第三点是经典统计最容易引入的修正，因为这个简并度在从微正则系综到正则系综的映射中就已经隐含了。微正则系综作为孤立系，在相空间中的自由度是最大的，需要考虑孤立系中全部子系统的轨迹，但是一旦我们考虑一个温度恒定的热源，通过积分积去环境带来的自由度，这样就完成了从微正则系综到正则系综的跃变。在这个过程中，很明显，被隐藏的自由度会带来更高的对称性，以至于在任意表象下都会引入非平庸的分布函数，但是目前我们刻意地只保留了指数的部分而忽略了具体表象下存在的其余对称性，比如k空间中谐振子波模就是存在简并度的，这一点在黑体辐射的经典分析中已经得到体现。</p>\n<p>&emsp;&emsp;对于凝聚态体系，自旋都是极其重要的，自旋自由度就是磁性系统的基础，但是对于自旋，我们也可以在完全应用量子场论之前做一点经典的统计，比如著名的伊辛模型，这个经典的自旋体系（区别于海森堡模型的自旋）可以做平均场，当然对于一维的伊辛模型的平均场得出的有限温相变点是很离谱的错误，但这不妨碍做高维的伊辛模型平均场（精确很多），这个模型往里挖还能引入很多新的思想，比如重整化，这里就不继续讨论了。</p>\n<p>&emsp;&emsp;能级不连续作为两朵大乌云中的一朵，在黑体辐射的研究被揭露了出来，无相互作用玻色子系统的能隙是产生玻色-爱因斯坦凝聚的关键，实验上在多原子分子的平均能量中我们也能看到阶梯状的随温度变化的情形，经典统计是得不出这样奇妙的结果的，毕竟经典系统是无能隙的。</p>\n<h2 id=\"结束语\"><a href=\"#结束语\" class=\"headerlink\" title=\"结束语\"></a>结束语</h2><p>&emsp;&emsp;经典统计是研究多体系统的重要工具，也是进入量子统计的基础，在经典统计中很多方法在量子统计中依然有用武之地，比如平均场和团簇展开都是非常常见的技术，其思想更是在物理学中占有极重的地位，因此经典统计对于我们的同学实属应该学好、必须学好的一种理论，这对今后的学习与研究都是有大益处的。</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"ckotps9e70001lsvwbabueop5","category_id":"ckotps9ed0004lsvw5q9443hm","_id":"ckotps9el000hlsvwglvra9rq"},{"post_id":"ckotps9ej000flsvw7lncfw2y","category_id":"ckotps9ed0004lsvw5q9443hm","_id":"ckotps9en000nlsvwgfex63ke"},{"post_id":"ckotps9ec0003lsvwhqrldwlc","category_id":"ckotps9ej000clsvwejuc915c","_id":"ckotps9es000qlsvw3e7a7m8t"},{"post_id":"ckotps9em000llsvw2y3t17i7","category_id":"ckotps9ej000clsvwejuc915c","_id":"ckotps9et000rlsvw6nsve8gf"},{"post_id":"ckotps9eg0007lsvwee0z67cr","category_id":"ckotps9em000ilsvwa8ob7fgj","_id":"ckotps9et000ulsvw8u0fe4d5"},{"post_id":"ckotps9eh0009lsvw0ddt7xoq","category_id":"ckotps9ej000clsvwejuc915c","_id":"ckotps9eu000vlsvw2l3j7fu3"},{"post_id":"ckotps9ei000blsvwg14zdntt","category_id":"ckotps9et000slsvwa5hcb8ng","_id":"ckotps9ev000ylsvwclb2ha1r"},{"post_id":"ckotps9el000glsvwffyq1t04","category_id":"ckotps9et000slsvwa5hcb8ng","_id":"ckotps9ev0011lsvw4bz0ffr9"},{"post_id":"ckotps9ez001flsvw6ylbauix","category_id":"ckotps9et000slsvwa5hcb8ng","_id":"ckotps9f2001llsvwh2ar8o8g"},{"post_id":"ckotps9ez001glsvwcf6z416b","category_id":"ckotps9em000ilsvwa8ob7fgj","_id":"ckotps9f2001nlsvw0e3p6kik"},{"post_id":"ckotps9f0001ilsvwc7w1eqbr","category_id":"ckotps9f2001mlsvw5e7o64rp","_id":"ckotps9f4001rlsvw5pjw071s"},{"post_id":"ckotps9f1001klsvw9mdwhu93","category_id":"ckotps9f2001mlsvw5e7o64rp","_id":"ckotps9f5001vlsvwb5z1h2zt"}],"PostTag":[{"post_id":"ckotps9e70001lsvwbabueop5","tag_id":"ckotps9ef0005lsvwdf5l9z2d","_id":"ckotps9ej000elsvw8l5i6mti"},{"post_id":"ckotps9ej000flsvw7lncfw2y","tag_id":"ckotps9ef0005lsvwdf5l9z2d","_id":"ckotps9em000klsvw40228lk9"},{"post_id":"ckotps9ec0003lsvwhqrldwlc","tag_id":"ckotps9ej000dlsvw791c2jjx","_id":"ckotps9en000mlsvwdjncbbp5"},{"post_id":"ckotps9eg0007lsvwee0z67cr","tag_id":"ckotps9em000jlsvw8olc6y7b","_id":"ckotps9ev0010lsvw84md7xnp"},{"post_id":"ckotps9eg0007lsvwee0z67cr","tag_id":"ckotps9en000plsvwgxaka8fn","_id":"ckotps9ew0012lsvw8zshgpw8"},{"post_id":"ckotps9eg0007lsvwee0z67cr","tag_id":"ckotps9et000tlsvw1ujz2m7q","_id":"ckotps9ew0014lsvwgu0132go"},{"post_id":"ckotps9eg0007lsvwee0z67cr","tag_id":"ckotps9eu000xlsvwda0zhhbe","_id":"ckotps9ew0015lsvwapbu72fx"},{"post_id":"ckotps9eh0009lsvw0ddt7xoq","tag_id":"ckotps9ej000dlsvw791c2jjx","_id":"ckotps9ex0017lsvw04tecwgj"},{"post_id":"ckotps9ei000blsvwg14zdntt","tag_id":"ckotps9ew0013lsvw4la60tma","_id":"ckotps9ex0018lsvw3gop7m6z"},{"post_id":"ckotps9el000glsvwffyq1t04","tag_id":"ckotps9ew0013lsvw4la60tma","_id":"ckotps9ex001alsvw27f48wvc"},{"post_id":"ckotps9em000llsvw2y3t17i7","tag_id":"ckotps9ex0019lsvwasnv5s97","_id":"ckotps9ey001clsvw84tp2xx6"},{"post_id":"ckotps9em000llsvw2y3t17i7","tag_id":"ckotps9ex001blsvwerrw18db","_id":"ckotps9ey001dlsvw9y9p0y7k"},{"post_id":"ckotps9em000llsvw2y3t17i7","tag_id":"ckotps9ej000dlsvw791c2jjx","_id":"ckotps9ey001elsvw2a5ffkdl"},{"post_id":"ckotps9ez001flsvw6ylbauix","tag_id":"ckotps9ew0013lsvw4la60tma","_id":"ckotps9f0001hlsvw7u985ka4"},{"post_id":"ckotps9ez001glsvwcf6z416b","tag_id":"ckotps9en000plsvwgxaka8fn","_id":"ckotps9f4001slsvwdpl5fzwe"},{"post_id":"ckotps9ez001glsvwcf6z416b","tag_id":"ckotps9f1001jlsvweepc756x","_id":"ckotps9f4001tlsvw33gc9svi"},{"post_id":"ckotps9ez001glsvwcf6z416b","tag_id":"ckotps9f3001olsvwh2lr8woh","_id":"ckotps9f5001wlsvw816r4q3z"},{"post_id":"ckotps9ez001glsvwcf6z416b","tag_id":"ckotps9eu000xlsvwda0zhhbe","_id":"ckotps9f6001xlsvw1a719amr"},{"post_id":"ckotps9f0001ilsvwc7w1eqbr","tag_id":"ckotps9f3001qlsvw4zgh7bs9","_id":"ckotps9f6001ylsvw94wy74vp"},{"post_id":"ckotps9f1001klsvw9mdwhu93","tag_id":"ckotps9f3001qlsvw4zgh7bs9","_id":"ckotps9f6001zlsvw0o1mf2mi"}],"Tag":[{"name":"投资","_id":"ckotps9ef0005lsvwdf5l9z2d"},{"name":"机器学习","_id":"ckotps9ej000dlsvw791c2jjx"},{"name":"VPS","_id":"ckotps9em000jlsvw8olc6y7b"},{"name":"hexo","_id":"ckotps9en000plsvwgxaka8fn"},{"name":"hginx","_id":"ckotps9et000tlsvw1ujz2m7q"},{"name":"网站","_id":"ckotps9eu000xlsvwda0zhhbe"},{"name":"poem","_id":"ckotps9ew0013lsvw4la60tma"},{"name":"bigdata","_id":"ckotps9ex0019lsvwasnv5s97"},{"name":"总结","_id":"ckotps9ex001blsvwerrw18db"},{"name":"markdown","_id":"ckotps9f1001jlsvweepc756x"},{"name":"写作","_id":"ckotps9f3001olsvwh2lr8woh"},{"name":"读后感","_id":"ckotps9f3001qlsvw4zgh7bs9"}]}}